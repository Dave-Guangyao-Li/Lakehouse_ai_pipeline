#!/bin/bash
# Start FULL infrastructure including Spark (using Apache official image)

echo "ğŸš€ Starting FULL Lakehouse AI Pipeline Infrastructure..."
echo "   This includes: Kafka + MinIO + Spark"
echo ""

# Check if Docker is running
if ! docker info > /dev/null 2>&1; then
    echo "âŒ Docker is not running. Please start Docker Desktop first."
    exit 1
fi

echo "âœ… Docker is running"
echo ""

# Check if .env file exists
if [ ! -f "config/.env" ]; then
    echo "âš ï¸  config/.env not found. Creating from template..."
    cp config/env.example config/.env
    echo "âš ï¸  Please edit config/.env with your API keys"
    echo ""
fi

# Stop any existing containers from minimal config
echo "ğŸ›‘ Stopping minimal infrastructure (if running)..."
docker-compose -f docker-compose-minimal.yml down 2>/dev/null
echo ""

# Start full Docker Compose services
echo "ğŸ³ Starting FULL Docker services..."
echo "   - Zookeeper"
echo "   - Kafka"
echo "   - MinIO (S3-compatible storage)"
echo "   - Spark Master"
echo "   - Spark Worker"
echo ""

docker-compose -f docker-compose-full.yml up -d

# Wait for services to be healthy
echo ""
echo "â³ Waiting for services to be ready (30 seconds)..."
sleep 30

# Check service health
echo ""
echo "ğŸ” Checking service status..."
echo ""

# Check Kafka
if docker exec kafka kafka-broker-api-versions --bootstrap-server localhost:9092 > /dev/null 2>&1; then
    echo "âœ… Kafka is ready"
else
    echo "âš ï¸  Kafka is not ready yet"
fi

# Check MinIO
if curl -f http://localhost:9000/minio/health/live > /dev/null 2>&1; then
    echo "âœ… MinIO is ready"
else
    echo "âš ï¸  MinIO is not ready yet"
fi

# Check Spark Master
if curl -f http://localhost:8080 > /dev/null 2>&1; then
    echo "âœ… Spark Master is ready"
else
    echo "âš ï¸  Spark Master is not ready yet"
fi

# Check Spark Worker
if curl -f http://localhost:8081 > /dev/null 2>&1; then
    echo "âœ… Spark Worker is ready"
else
    echo "âš ï¸  Spark Worker is not ready yet"
fi

echo ""
echo "ğŸ“Š Full infrastructure started! Access points:"
echo ""
echo "   ğŸŒ Kafka:         localhost:9092"
echo "   ğŸŒ MinIO Console: http://localhost:9001 (admin/minioadmin)"
echo "   ğŸŒ MinIO API:     http://localhost:9000"
echo "   ğŸŒ Spark Master:  http://localhost:8080"
echo "   ğŸŒ Spark Worker:  http://localhost:8081"
echo ""
echo "ğŸ“ Next steps:"
echo "   1. Visit Spark Master UI: http://localhost:8080"
echo "   2. Verify worker is connected (should show 1 worker)"
echo "   3. Run Spark streaming job: ./scripts/start_spark_streaming.sh"
echo "   4. Check data in MinIO: http://localhost:9001"
echo ""
echo "ğŸ›‘ To stop all services: docker-compose -f docker-compose-full.yml down"
echo "ğŸ“Š To view logs: docker-compose -f docker-compose-full.yml logs -f"
echo ""
