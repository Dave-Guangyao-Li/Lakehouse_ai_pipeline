"""
Spark Streaming Processor - å†™å…¥MinIOç‰ˆæœ¬
è¯»å–Kafkaæ•°æ®å¹¶å†™å…¥MinIO (Bronzeå±‚)
"""
from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, from_json, to_timestamp, current_timestamp,
    to_date, lit
)
from pyspark.sql.types import StructType, StructField, StringType, MapType


# Kafkaæ¶ˆæ¯Schema
KAFKA_MESSAGE_SCHEMA = StructType([
    StructField("source", StringType(), True),
    StructField("timestamp", StringType(), True),
    StructField("data", MapType(StringType(), StringType()), True)
])


def create_spark_session():
    """åˆ›å»ºå¸¦MinIOé…ç½®çš„Spark Session"""
    return (
        SparkSession.builder
        .appName("AI_Trend_Monitor_MinIO")
        .master("spark://spark-master:7077")
        .config("spark.jars.packages",
                "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,"
                "org.apache.hadoop:hadoop-aws:3.3.4")
        .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000")
        .config("spark.hadoop.fs.s3a.access.key", "minioadmin")
        .config("spark.hadoop.fs.s3a.secret.key", "minioadmin")
        .config("spark.hadoop.fs.s3a.path.style.access", "true")
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
        .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false")
        .getOrCreate()
    )


def main():
    """ä¸»å¤„ç†æµç¨‹"""
    print("ğŸš€ Starting Spark Streaming processor (MinIOç‰ˆæœ¬)...")

    # åˆ›å»ºSpark Session
    spark = create_spark_session()
    spark.sparkContext.setLogLevel("WARN")
    print("âœ… Spark session created with MinIO configuration")

    # ä»Kafkaè¯»å–
    print("ğŸ“Š Connecting to Kafka...")
    kafka_df = (
        spark.readStream
        .format("kafka")
        .option("kafka.bootstrap.servers", "kafka:29092")
        .option("subscribe", "ai-social-raw")
        .option("startingOffsets", "earliest")
        .option("failOnDataLoss", "false")
        .load()
    )
    print("âœ… Connected to Kafka topic: ai-social-raw")

    # è§£æJSON
    parsed_df = (
        kafka_df
        .selectExpr("CAST(value AS STRING) as json_value")
        .select(from_json(col("json_value"), KAFKA_MESSAGE_SCHEMA).alias("data"))
        .select(
            col("data.source").alias("source"),
            col("data.timestamp").alias("event_timestamp"),
            col("data.data").alias("raw_data"),
            current_timestamp().alias("processed_at")
        )
        .withColumn("partition_date", to_date(col("processed_at")))
    )

    # å†™å…¥MinIO Bronzeå±‚ï¼ˆParquetæ ¼å¼ï¼‰
    print("ğŸ’¾ Starting to write data to MinIO...")
    print("   Path: s3a://lakehouse/bronze/social_media/")

    query = (
        parsed_df
        .writeStream
        .format("parquet")
        .outputMode("append")
        .option("checkpointLocation", "s3a://lakehouse/checkpoints/bronze")
        .option("path", "s3a://lakehouse/bronze/social_media")
        .partitionBy("partition_date", "source")  # æŒ‰æ—¥æœŸå’Œæ¥æºåˆ†åŒº
        .trigger(processingTime='30 seconds')  # æ¯30ç§’æ‰¹æ¬¡
        .start()
    )

    print("âœ… Streaming started! Writing to MinIO...")
    print("ğŸ“Š Data location: s3a://lakehouse/bronze/social_media/")
    print("ğŸ“‚ Partitioned by: partition_date, source")
    print("")
    print("ğŸ” éªŒè¯æ•°æ®:")
    print("   1. è®¿é—® http://localhost:9001")
    print("   2. ç™»å½• (minioadmin/minioadmin)")
    print("   3. è¿›å…¥ 'lakehouse' bucket")
    print("   4. æŸ¥çœ‹ bronze/social_media/ ç›®å½•")
    print("")
    print("â¹ï¸  Press Ctrl+C to stop")
    print("")

    # ç­‰å¾…ç»ˆæ­¢
    query.awaitTermination()


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nâ¹ï¸  Streaming stopped by user")
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        raise
