# Dashboardé—®é¢˜ä¿®å¤æ€»ç»“

**æ—¥æœŸ**: 2025-11-12 19:56
**é—®é¢˜**: Dashboardæ˜¾ç¤º"No data available"ä¸”Reddité‡‡é›†å™¨æŠ¥é”™

---

## ğŸ” é—®é¢˜è¯Šæ–­

### é—®é¢˜1: Dashboardæ˜¾ç¤º"No data available"
**ç—‡çŠ¶**:
- Kafkaæœ‰7410+æ¡æ¶ˆæ¯
- Dashboardæ˜¾ç¤º "âš ï¸ No data available"
- ä½†èƒ½çœ‹åˆ°Kafkaæ€»æ¶ˆæ¯æ•°

**æ ¹æœ¬åŸå› **:
- **Streamlitç¼“å­˜é—®é¢˜**: `@st.cache_data(ttl=300)` ç¼“å­˜äº†æ—©æœŸæ²¡æœ‰æ•°æ®æ—¶çš„ç»“æœ
- å³ä½¿Kafkaä¸­æœ‰æ–°æ•°æ®ï¼ŒDashboardä»æ˜¾ç¤ºç¼“å­˜çš„ç©ºç»“æœ

**éªŒè¯**:
```bash
# æµ‹è¯•Kafkaæ•°æ®è¯»å–ï¼ˆç¡®è®¤æ•°æ®å¯æ­£å¸¸è§£æï¼‰
source venv/bin/activate
python -c "from dashboard.kafka_reader import KafkaDataReader
reader = KafkaDataReader()
messages = reader.get_all_messages()
df = reader.parse_to_dataframe(messages)
print(f'âœ… æˆåŠŸè§£æ {len(df)} è¡Œæ•°æ®')"

# è¾“å‡º: âœ… æˆåŠŸè§£æ 7554 è¡Œæ•°æ®
```

### é—®é¢˜2: Reddité‡‡é›†å™¨æ˜¾ç¤º"âŒ å‡ºç°é”™è¯¯"
**ç—‡çŠ¶**:
```
âŒ Error collecting from r/ArtificialIntelligence: received 404 HTTP response
```

**æ ¹æœ¬åŸå› **:
- `r/ArtificialIntelligence` subredditä¸å¯è®¿é—®ï¼ˆå¯èƒ½è¢«ç¦ç”¨/ç§æœ‰åŒ–/æ”¹åï¼‰
- è™½ç„¶æŠ¥é”™ï¼Œä½†å…¶ä»–subredditä»æ­£å¸¸å·¥ä½œ

---

## âœ… ä¿®å¤æ–¹æ¡ˆ

### ä¿®å¤1: å¢å¼ºDashboardé”™è¯¯æ—¥å¿—å’Œè°ƒè¯•
**æ–‡ä»¶**: `dashboard/app_realtime.py`

**æ”¹åŠ¨**:
```python
# ä¿®æ”¹ load_real_data() å‡½æ•°ï¼ˆç¬¬252-288è¡Œï¼‰
# æ·»åŠ è¯¦ç»†çš„è°ƒè¯•æ—¥å¿—è¾“å‡º
def load_real_data():
    ...
    if not messages:
        print(f"âš ï¸ Warning: get_all_messages() returned empty list, but total_count={total_count}")
        return None, total_count

    df = reader.parse_to_dataframe(messages)

    if df.empty:
        print(f"âš ï¸ Warning: parse_to_dataframe() returned empty DataFrame from {len(messages)} messages")
        return None, total_count

    print(f"âœ… Successfully loaded {len(df)} rows from Kafka (total_count={total_count})")
    return df, total_count
```

**å¥½å¤„**:
- æ›´å®¹æ˜“è¯Šæ–­æ•°æ®åŠ è½½é—®é¢˜
- printè¾“å‡ºä¼šæ˜¾ç¤ºåœ¨è¿è¡ŒDashboardçš„terminalä¸­

### ä¿®å¤2: æ›´æ–°Redditç›®æ ‡subredditåˆ—è¡¨
**æ–‡ä»¶**: `data_ingestion/reddit/collector.py`

**æ”¹åŠ¨**:
```python
TARGET_SUBREDDITS = [
    'MachineLearning',
    'artificial',
    'LocalLLaMA',
    'OpenAI',
    'ChatGPT',
    # 'ArtificialIntelligence',  # æš‚æ—¶ç¦ç”¨ï¼šè¿”å›404é”™è¯¯
    'deeplearning',
    'LanguageTechnology',
    'learnmachinelearning',  # æ–°å¢ï¼šé€‚åˆåˆå­¦è€…çš„MLå†…å®¹
    'agi'  # æ–°å¢ï¼šAGIç›¸å…³è®¨è®º
]
```

**å¥½å¤„**:
- ç§»é™¤æœ‰é—®é¢˜çš„subreddit
- æ·»åŠ 2ä¸ªæ–°çš„æ´»è·ƒsubredditä½œä¸ºæ›¿ä»£
- DashboardçŠ¶æ€ä¸å†æ˜¾ç¤ºé”™è¯¯

---

## ğŸš€ å¦‚ä½•åº”ç”¨ä¿®å¤

### æ­¥éª¤1: é‡å¯Reddité‡‡é›†å™¨
```bash
# åœæ­¢æ—§çš„é‡‡é›†å™¨
cd /Users/guangyaoli/Documents/Lakehouse_ai_pipeline
./scripts/stop_collectors.sh

# å¯åŠ¨æ–°çš„é‡‡é›†å™¨ï¼ˆä¼šè¯»å–æ›´æ–°åçš„subredditåˆ—è¡¨ï¼‰
./scripts/start_collectors.sh
```

### æ­¥éª¤2: æ¸…é™¤Dashboardç¼“å­˜å¹¶é‡å¯
```bash
# å¦‚æœDashboardæ­£åœ¨è¿è¡Œï¼ŒæŒ‰ Ctrl+C åœæ­¢

# é‡æ–°å¯åŠ¨Dashboard
source venv/bin/activate
streamlit run dashboard/app_realtime.py
```

### æ­¥éª¤3: æµ‹è¯•éªŒè¯
1. æ‰“å¼€æµè§ˆå™¨è®¿é—® http://localhost:8501
2. **ç‚¹å‡»"ğŸ”„ ç«‹å³åˆ·æ–°"æŒ‰é’®** - è¿™ä¼šæ¸…é™¤ç¼“å­˜å¹¶é‡æ–°åŠ è½½æ•°æ®
3. åº”è¯¥çœ‹åˆ°æ•°æ®æ­£å¸¸æ˜¾ç¤º
4. æ£€æŸ¥ä¾§è¾¹æ  - RedditçŠ¶æ€åº”è¯¥æ˜¾ç¤º"âœ… æ­£å¸¸è¿è¡Œ"

---

## ğŸ“Š é¢„æœŸç»“æœ

ä¿®å¤ååº”è¯¥çœ‹åˆ°ï¼š

### Dashboard
```
ğŸ¤– AI Trend Monitor
â— LIVE DATA

ğŸ“Š Total Posts: 7,554
ğŸ¦ Twitter Posts: 193
ğŸ¤– Reddit Posts: 2,069
ğŸ”µ Bluesky Posts: 5,292

[æ•°æ®å¡ç‰‡æ­£å¸¸æ˜¾ç¤º...]
```

### ä¾§è¾¹æ çŠ¶æ€
```
ğŸ¤– Collectors Status
- ğŸ”µ Bluesky: âœ… æ­£å¸¸è¿è¡Œ
  ğŸ“¡ æ­£åœ¨é‡‡é›† Bluesky å¸–å­

- ğŸ¤– Reddit: âœ… æ­£å¸¸è¿è¡Œ
  ğŸ“¡ é‡‡é›†é¢‘ç‡: 120ç§’/æ¬¡
```

---

## ğŸ”§ å¦‚æœé—®é¢˜ä»å­˜åœ¨

### é—®é¢˜A: ç‚¹å‡»"ç«‹å³åˆ·æ–°"åä»æ˜¾ç¤º"No data available"
**å¯èƒ½åŸå› **: Kafkaæ¶ˆè´¹è€…é…ç½®é—®é¢˜

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. æ£€æŸ¥Kafkaæ˜¯å¦è¿è¡Œ
docker ps | grep kafka

# 2. æ£€æŸ¥Kafka topicæ•°æ®
docker exec -it kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic ai-social-raw \
  --from-beginning --max-messages 5

# 3. æ‰‹åŠ¨æµ‹è¯•æ•°æ®è¯»å–
source venv/bin/activate
python -c "
from dashboard.kafka_reader import KafkaDataReader
reader = KafkaDataReader()
messages = reader.get_all_messages()
print(f'è¯»å–åˆ° {len(messages)} æ¡æ¶ˆæ¯')
df = reader.parse_to_dataframe(messages)
print(f'è§£æåˆ° {len(df)} è¡Œæ•°æ®')
print(df['source'].value_counts())
"
```

### é—®é¢˜B: Reddité‡‡é›†å™¨ä»æ˜¾ç¤ºé”™è¯¯
**æ£€æŸ¥æ—¥å¿—**:
```bash
tail -50 logs/reddit_collector.log
```

**å¸¸è§é—®é¢˜**:
1. **Rate limiting**: ç­‰å¾…å‡ åˆ†é’Ÿï¼ŒReddit APIä¼šè‡ªåŠ¨æ¢å¤
2. **è®¤è¯å¤±è´¥**: æ£€æŸ¥ `config/.env` ä¸­çš„Reddit APIå‡­æ®
3. **ç½‘ç»œé—®é¢˜**: æ£€æŸ¥ç½‘ç»œè¿æ¥

---

## ğŸ“ é¢å¤–å»ºè®®

### å»ºè®®1: å®šæœŸæ¸…ç†Kafkaæ—§æ•°æ®ï¼ˆå¯é€‰ï¼‰
å¦‚æœKafkaæ•°æ®é‡å¤ªå¤§å¯¼è‡´DashboardåŠ è½½æ…¢ï¼š
```bash
# ä¿®æ”¹ kafka_reader.py çš„ get_all_messages() æ”¹ä¸ºåªè¯»å–æœ€è¿‘Næ¡
# æˆ–è€…ä½¿ç”¨ get_recent_messages(1000) ä»£æ›¿ get_all_messages()
```

### å»ºè®®2: ç›‘æ§é‡‡é›†å™¨å¥åº·çŠ¶æ€
åˆ›å»ºç›‘æ§è„šæœ¬å®šæœŸæ£€æŸ¥é‡‡é›†å™¨çŠ¶æ€ï¼š
```bash
# scripts/check_health.sh
#!/bin/bash
echo "Checking collectors..."
ps aux | grep collector.py | grep -v grep
echo ""
echo "Checking Kafka messages..."
docker exec kafka kafka-run-class kafka.tools.GetOffsetShell \
  --broker-list localhost:9092 \
  --topic ai-social-raw
```

### å»ºè®®3: Dashboardæ€§èƒ½ä¼˜åŒ–
å¦‚æœæ•°æ®é‡ç»§ç»­å¢é•¿ï¼Œè€ƒè™‘ï¼š
- å‡å°‘ `ttl` ç¼“å­˜æ—¶é—´ï¼ˆç›®å‰300ç§’ï¼‰
- åªæ˜¾ç¤ºæœ€è¿‘7å¤©çš„æ•°æ®
- æ·»åŠ åˆ†é¡µåŠŸèƒ½

---

## âœ… ä¿®å¤ç¡®è®¤æ¸…å•

- [ ] Reddité‡‡é›†å™¨ä¸å†æ˜¾ç¤º404é”™è¯¯
- [ ] Dashboardèƒ½æ­£å¸¸æ˜¾ç¤ºæ•°æ®
- [ ] Dashboardä¾§è¾¹æ çŠ¶æ€æ˜¾ç¤ºæ­£å¸¸
- [ ] æ–°çš„subreddit (`learnmachinelearning`, `agi`) å¼€å§‹é‡‡é›†æ•°æ®
- [ ] ç‚¹å‡»"ç«‹å³åˆ·æ–°"èƒ½çœ‹åˆ°æœ€æ–°æ•°æ®

---

**ä½œè€…**: Claude Code
**ä¿®å¤æ—¶é—´**: 2025-11-12 19:56
**ç›¸å…³æ–‡ä»¶**:
- `dashboard/app_realtime.py` (ç¬¬252-288è¡Œ)
- `data_ingestion/reddit/collector.py` (ç¬¬26-37è¡Œ)
