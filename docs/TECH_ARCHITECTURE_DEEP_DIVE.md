# AIè¶‹åŠ¿ç›‘æ§ç³»ç»Ÿ - æŠ€æœ¯æ¶æ„æ·±åº¦è§£æ

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0
**æœ€åæ›´æ–°**: 2025-11-12
**ç›®æ ‡è¯»è€…**: æœ‰ç¼–ç¨‹åŸºç¡€ï¼Œæƒ³æ·±å…¥ç†è§£æ•°æ®å·¥ç¨‹æ¶æ„é€‰å‹
**é˜…è¯»æ—¶é—´**: æ ¸å¿ƒå†…å®¹15-20åˆ†é’Ÿï¼Œç²¾è¯»å…¨æ–‡30-45åˆ†é’Ÿ

---

## ç›®å½•

- [Part 1: å…¨æ™¯è§†å›¾](#part-1-å…¨æ™¯è§†å›¾)
- [Part 2: æ ¸å¿ƒæŠ€æœ¯æ·±åº¦è§£æ](#part-2-æ ¸å¿ƒæŠ€æœ¯æ·±åº¦è§£æ)
- [Part 3: æ¶æ„å†³ç­–æ·±åº¦å‰–æ](#part-3-æ¶æ„å†³ç­–æ·±åº¦å‰–æ)
- [Part 4: å…³é”®ä»£ç æ·±åº¦è§£æ](#part-4-å…³é”®ä»£ç æ·±åº¦è§£æ)
- [Part 5: ç”Ÿäº§åŒ–è·¯å¾„å’Œæœªæ¥æ‰©å±•](#part-5-ç”Ÿäº§åŒ–è·¯å¾„å’Œæœªæ¥æ‰©å±•)
- [Part 6: é™„å½•](#part-6-é™„å½•)

---

# Part 1: å…¨æ™¯è§†å›¾

## 30ç§’ç”µæ¢¯æ¼”è®²

**è¿™ä¸ªç³»ç»Ÿæ˜¯ä»€ä¹ˆï¼Ÿ**

ä¸€ä¸ª**å®æ—¶AIè¶‹åŠ¿ç›‘æ§ç³»ç»Ÿ**ï¼Œä»Twitterå’ŒReddité‡‡é›†AIç›¸å…³è®¨è®ºï¼Œé€šè¿‡æµå¼å¤„ç†pipelineå®æ—¶åˆ†æï¼Œå­˜å‚¨åˆ°æ•°æ®æ¹–ï¼Œæœ€ååœ¨Dashboardå¯è§†åŒ–å±•ç¤ºçƒ­é—¨è¯é¢˜å’Œè¶‹åŠ¿ã€‚

**æŠ€æœ¯ç‰¹ç‚¹ï¼Ÿ**

- âš¡ **å®æ—¶å¤„ç†**ï¼šæ•°æ®ä»é‡‡é›†åˆ°å±•ç¤ºå»¶è¿Ÿ < 60ç§’
- ğŸ—ï¸ **Lakehouseæ¶æ„**ï¼šç»“åˆæ•°æ®æ¹–çš„çµæ´»æ€§å’Œæ•°æ®ä»“åº“çš„äº‹åŠ¡æ€§
- ğŸ“Š **æµæ‰¹ä¸€ä½“**ï¼šåŒæ—¶æ”¯æŒå®æ—¶æµå¤„ç†å’Œæ‰¹é‡åˆ†æ
- ğŸ”§ **å®¹å™¨åŒ–éƒ¨ç½²**ï¼šDocker Composeä¸€é”®å¯åŠ¨ï¼Œæ˜“äºå¼€å‘å’Œæµ‹è¯•
- ğŸš€ **å¯æ‰©å±•è®¾è®¡**ï¼šæ¯ä¸ªç»„ä»¶éƒ½å¯ä»¥ç‹¬ç«‹æ‰©å±•åˆ°ç”Ÿäº§ç¯å¢ƒ

**æŠ€æœ¯æ ˆç²¾åï¼Ÿ**

```
æ•°æ®é‡‡é›†: Python + Tweepy/PRAW
æ¶ˆæ¯é˜Ÿåˆ—: Kafka (é«˜åå)
æµå¤„ç†: Spark Streaming (åˆ†å¸ƒå¼)
å­˜å‚¨: MinIO (S3å…¼å®¹) + Delta Lake (ACIDäº‹åŠ¡)
å¯è§†åŒ–: Streamlit (å¿«é€ŸåŸå‹)
```

---

## æ•°æ®ç”Ÿå‘½å‘¨æœŸå¯è§†åŒ–

### å®Œæ•´æ•°æ®æ—…ç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           æ•°æ®çš„ä¸€ç”Ÿï¼ˆLife of Dataï¼‰                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ç¬¬1ç«™: æ•°æ®æº (Data Sources)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    ğŸ¦ Twitter API          ğŸ¤– Reddit API
         â”‚                        â”‚
         â”œâ”€ æ¨æ–‡å†…å®¹              â”œâ”€ å¸–å­æ ‡é¢˜
         â”œâ”€ ä½œè€…ä¿¡æ¯              â”œâ”€ å¸–å­æ­£æ–‡
         â”œâ”€ äº’åŠ¨æ•°æ®              â”œâ”€ Subreddit
         â””â”€ æ—¶é—´æˆ³                â””â”€ è¯„åˆ†æ•°æ®
              â”‚                        â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“

ç¬¬2ç«™: æ•°æ®é‡‡é›†å±‚ (Data Ingestion)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
         Pythoné‡‡é›†å™¨ (60ç§’/æ¬¡)
              â”‚
              â”œâ”€ APIè°ƒç”¨ (tweepy/praw)
              â”œâ”€ æ•°æ®æ¸…æ´— (å»é™¤HTMLæ ‡ç­¾)
              â”œâ”€ æ ¼å¼æ ‡å‡†åŒ– (JSON)
              â””â”€ æ·»åŠ å…ƒæ•°æ® (source, timestamp)
              â”‚
              â†“

ç¬¬3ç«™: æ¶ˆæ¯é˜Ÿåˆ— (Message Queue)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
         Apache Kafka
         Topic: ai-social-raw
              â”‚
              â”œâ”€ åˆ†åŒºå­˜å‚¨ (Partitioned)
              â”œâ”€ æŒä¹…åŒ–7å¤© (Retention: 7 days)
              â”œâ”€ é¡ºåºä¿è¯ (Ordering guaranteed)
              â””â”€ é«˜åå (10K+ msg/s capable)
              â”‚
              â†“

ç¬¬4ç«™: æµå¤„ç†å±‚ (Stream Processing)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
         Spark Streaming
              â”‚
              â”œâ”€ è¯»å–Kafkaæ¶ˆæ¯ (micro-batch: 30s)
              â”œâ”€ è§£æJSON (Schema validation)
              â”œâ”€ æ•°æ®è½¬æ¢ (Transform)
              â”œâ”€ åˆ†åŒº (Partition by date & source)
              â””â”€ å†™å…¥å­˜å‚¨ (Write to MinIO)
              â”‚
              â†“

ç¬¬5ç«™: å­˜å‚¨å±‚ (Storage Layer)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
         MinIO + Delta Lake
              â”‚
         Bronze Layer (Raw Data)
              â”œâ”€ æ ¼å¼: Parquet (åˆ—å¼å‹ç¼©)
              â”œâ”€ åˆ†åŒº: partition_date=YYYY-MM-DD/source=reddit|twitter
              â”œâ”€ ä¿ç•™: å®Œæ•´åŸå§‹æ•°æ®
              â””â”€ ç”¨é€”: æ•°æ®å›æº¯ã€é‡æ–°å¤„ç†
              â”‚
         [æœªæ¥] Silver Layer (Cleaned Data)
              â”œâ”€ å»é‡ (Deduplication by post_id)
              â”œâ”€ æ ‡å‡†åŒ– (Schema standardization)
              â””â”€ å…³é”®è¯æå– (NLP processing)
              â”‚
         [æœªæ¥] Gold Layer (Aggregated Data)
              â”œâ”€ æŒ‰å°æ—¶èšåˆ (Hourly aggregation)
              â””â”€ è¶‹åŠ¿è®¡ç®— (Trend scoring)
              â”‚
              â†“

ç¬¬6ç«™: åˆ†æå±‚ (Analytics Layer)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
         DuckDB / Spark SQL
              â”‚
              â”œâ”€ OLAPæŸ¥è¯¢ (Analytical queries)
              â”œâ”€ ç»Ÿè®¡åˆ†æ (Statistical analysis)
              â””â”€ æ•°æ®èšåˆ (Data aggregation)
              â”‚
              â†“

ç¬¬7ç«™: å¯è§†åŒ–å±‚ (Visualization Layer)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
         Streamlit Dashboard
              â”‚
              â”œâ”€ å®æ—¶è¯»å–Kafka (Direct Kafka read)
              â”œâ”€ NLPå…³é”®è¯æå– (spaCy)
              â”œâ”€ è¯äº‘ç”Ÿæˆ (WordCloud)
              â”œâ”€ äº¤äº’å¼å›¾è¡¨ (Plotly)
              â””â”€ è‡ªåŠ¨åˆ·æ–° (60s auto-refresh)
              â”‚
              â†“

ç»ˆç‚¹ç«™: ç”¨æˆ·ç•Œé¢ (User Interface)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    http://localhost:8501
         â”‚
         â”œâ”€ ğŸ“Š Overview: æ•°æ®æºåˆ†å¸ƒ
         â”œâ”€ ğŸ”¥ Trending Keywords: çƒ­é—¨è¯é¢˜è¯äº‘
         â””â”€ ğŸ“ Recent Posts: Reddité£æ ¼å¡ç‰‡

```

### å…³é”®æ€§èƒ½æŒ‡æ ‡

| æŒ‡æ ‡ | å½“å‰å€¼ | è¯´æ˜ |
|------|--------|------|
| **ç«¯åˆ°ç«¯å»¶è¿Ÿ** | < 60ç§’ | ä»APIé‡‡é›†åˆ°Dashboardæ˜¾ç¤º |
| **æ•°æ®é‡‡é›†é¢‘ç‡** | 60ç§’/æ¬¡ | Reddité‡‡é›†é—´éš”ï¼ˆé¿å…é™æµï¼‰ |
| **Sparkæ‰¹æ¬¡é—´éš”** | 30ç§’ | micro-batchå¤„ç†å‘¨æœŸ |
| **Dashboardåˆ·æ–°** | 60ç§’ | è‡ªåŠ¨åˆ·æ–°é—´éš” |
| **Kafkaååé‡** | ~100 msg/s (å®é™…) | 10K+ msg/s (ç†è®º) |
| **æ•°æ®ä¿ç•™** | Kafka: 7å¤©<br>MinIO: æ— é™ | å¯é…ç½® |

---

## æ¶æ„åˆ†å±‚å›¾

### 6å±‚æ¶æ„è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 6: å±•ç¤ºå±‚ (Presentation Layer)                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Streamlit Dashboard (http://localhost:8501)               â”‚ â”‚
â”‚  â”‚  - å®æ—¶æ•°æ®å±•ç¤º                                              â”‚ â”‚
â”‚  â”‚  - NLPå…³é”®è¯æå–                                             â”‚ â”‚
â”‚  â”‚  - äº¤äº’å¼å¯è§†åŒ–                                              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†‘ â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 5: åˆ†æå±‚ (Analytics Layer)                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  DuckDB (OLAP)                 Spark SQL                    â”‚ â”‚
â”‚  â”‚  - Ad-hocæŸ¥è¯¢                   - å¤§è§„æ¨¡æ•°æ®åˆ†æ              â”‚ â”‚
â”‚  â”‚  - è½»é‡çº§æœ¬åœ°åˆ†æ               - åˆ†å¸ƒå¼è®¡ç®—                 â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†‘ â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 4: å­˜å‚¨å±‚ (Storage Layer)                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  MinIO (S3-compatible Object Storage)                       â”‚ â”‚
â”‚  â”‚  + Delta Lake (ACID Transactions)                           â”‚ â”‚
â”‚  â”‚                                                              â”‚ â”‚
â”‚  â”‚  Bronze â”€â”€â†’ [Raw Data]       (Parquet)                      â”‚ â”‚
â”‚  â”‚  Silver â”€â”€â†’ [Cleaned Data]   (Delta) [æœªæ¥]                 â”‚ â”‚
â”‚  â”‚  Gold   â”€â”€â†’ [Aggregated]     (Delta) [æœªæ¥]                 â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†‘ â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 3: æµå¤„ç†å±‚ (Stream Processing Layer)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Apache Spark Streaming                                     â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚ â”‚
â”‚  â”‚  â”‚ Master   â”‚  â”‚ Worker-1 â”‚  â”‚ Worker-N â”‚                 â”‚ â”‚
â”‚  â”‚  â”‚ :8080    â”‚  â”‚ :8081    â”‚  â”‚          â”‚                 â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚ â”‚
â”‚  â”‚  - Structured Streaming API                                 â”‚ â”‚
â”‚  â”‚  - Micro-batch Processing (30s)                             â”‚ â”‚
â”‚  â”‚  - Exactly-once Semantics                                   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†‘ â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 2: æ¶ˆæ¯é˜Ÿåˆ—å±‚ (Message Queue Layer)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Apache Kafka (:9092)                                       â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚  â”‚  â”‚  Topic: ai-social-raw                                 â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  Partitions: 1 (å•èŠ‚ç‚¹)                               â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  Replication: 1                                       â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  Retention: 7 days                                    â”‚  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â”‚  â”‚                                                              â”‚ â”‚
â”‚  â”‚  Zookeeper (:2181) - Kafkaåè°ƒæœåŠ¡                          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†‘ â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 1: æ•°æ®é‡‡é›†å±‚ (Data Ingestion Layer)                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Python Collectors (æ¯60ç§’é‡‡é›†ä¸€æ¬¡)                          â”‚ â”‚
â”‚  â”‚                                                              â”‚ â”‚
â”‚  â”‚  twitter/collector.py  â¸ï¸ (æš‚åœ)                            â”‚ â”‚
â”‚  â”‚  â”œâ”€ Tweepy (Twitter API v2)                                â”‚ â”‚
â”‚  â”‚  â””â”€ KafkaProducer                                           â”‚ â”‚
â”‚  â”‚                                                              â”‚ â”‚
â”‚  â”‚  reddit/collector.py  âœ… (è¿è¡Œä¸­)                            â”‚ â”‚
â”‚  â”‚  â”œâ”€ PRAW (Reddit API)                                      â”‚ â”‚
â”‚  â”‚  â”œâ”€ Target: r/MachineLearning, r/LocalLLaMA               â”‚ â”‚
â”‚  â”‚  â””â”€ KafkaProducer                                           â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 0: æ•°æ®æº (Data Sources)                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  ğŸ¦ Twitter API v2        ğŸ¤– Reddit API                     â”‚ â”‚
â”‚  â”‚  - Bearer Tokenè®¤è¯       - OAuth2 (script mode)            â”‚ â”‚
â”‚  â”‚  - Essential access       - Read-only                       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å®¹å™¨åŒ–åŸºç¡€è®¾æ–½

æ‰€æœ‰æœåŠ¡è¿è¡Œåœ¨Dockerå®¹å™¨ä¸­ï¼Œé€šè¿‡`docker-compose-full.yml`ç¼–æ’ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Docker Network: lakehouse-network (bridge)                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  zookeeper:2181                                       â”‚  â”‚
â”‚  â”‚  â”œâ”€ Volume: zookeeper-data                           â”‚  â”‚
â”‚  â”‚  â””â”€ Volume: zookeeper-logs                           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  kafka:9092, :29092                                   â”‚  â”‚
â”‚  â”‚  â”œâ”€ Depends on: zookeeper                            â”‚  â”‚
â”‚  â”‚  â””â”€ Volume: kafka-data                               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  minio:9000 (API), :9001 (Console)                    â”‚  â”‚
â”‚  â”‚  â”œâ”€ Volume: minio-data                               â”‚  â”‚
â”‚  â”‚  â””â”€ Buckets: lakehouse, bronze, silver, gold        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  spark-master:8080 (UI), :7077 (Port)                â”‚  â”‚
â”‚  â”‚  â””â”€ Volume: spark-logs                               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  spark-worker:8081                                    â”‚  â”‚
â”‚  â”‚  â”œâ”€ Depends on: spark-master                         â”‚  â”‚
â”‚  â”‚  â”œâ”€ Cores: 2                                         â”‚  â”‚
â”‚  â”‚  â”œâ”€ Memory: 2G                                       â”‚  â”‚
â”‚  â”‚  â””â”€ Volume: spark-worker-logs                        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å®¿ä¸»æœºPythonè¿›ç¨‹:
â”œâ”€ data_ingestion/reddit/collector.py (PID saved to logs/reddit.pid)
â”œâ”€ streaming/spark/processor_with_minio.py (Spark Submit)
â””â”€ dashboard/app_realtime.py (Streamlit)
```

---

## ä¸ºä»€ä¹ˆé€‰æ‹©è¿™ä¸ªæ¶æ„ï¼Ÿ

### æ ¸å¿ƒè®¾è®¡åŸåˆ™

#### 1. **Lambdaæ¶æ„å˜ä½“**ï¼ˆæµæ‰¹ä¸€ä½“ï¼‰

ä¼ ç»ŸLambdaæ¶æ„æœ‰ä¸¤æ¡è·¯å¾„ï¼š
- **Batch Layer**ï¼šå¤„ç†å†å²å…¨é‡æ•°æ®ï¼ˆæ…¢ä½†å‡†ç¡®ï¼‰
- **Speed Layer**ï¼šå¤„ç†å®æ—¶å¢é‡æ•°æ®ï¼ˆå¿«ä½†å¯èƒ½ä¸å®Œæ•´ï¼‰

æˆ‘ä»¬çš„ç®€åŒ–ç‰ˆæœ¬ï¼š
- **å®æ—¶è·¯å¾„**ï¼šKafka â†’ Spark Streaming â†’ MinIOï¼ˆBronzeï¼‰
- **æ‰¹å¤„ç†è·¯å¾„**ï¼ˆæœªæ¥ï¼‰ï¼šMinIO Bronze â†’ Spark Batch â†’ Silver/Gold

**ä¼˜åŠ¿**ï¼š
- âœ… æ—¢èƒ½å®æ—¶çœ‹åˆ°æœ€æ–°æ•°æ®ï¼ˆDashboardè¯»Kafkaï¼‰
- âœ… åˆèƒ½å­˜å‚¨å†å²æ•°æ®è¿›è¡Œå¤æ‚åˆ†æï¼ˆMinIOæŒä¹…åŒ–ï¼‰
- âœ… å¦‚æœå®æ—¶å¤„ç†å‡ºé—®é¢˜ï¼Œå¯ä»¥ä»Bronzeå±‚é‡æ–°å¤„ç†

#### 2. **Lakehouseæ¶æ„**ï¼ˆæ•°æ®æ¹– + æ•°æ®ä»“åº“ï¼‰

**ä¼ ç»Ÿé€‰æ‹©**ï¼š
- **æ•°æ®æ¹–**ï¼ˆå¦‚HDFSï¼‰ï¼šå­˜å‚¨çµæ´»ï¼Œä½†ç¼ºä¹äº‹åŠ¡ä¿è¯
- **æ•°æ®ä»“åº“**ï¼ˆå¦‚Snowflakeï¼‰ï¼šäº‹åŠ¡æ€§å¼ºï¼Œä½†å­˜å‚¨æ˜‚è´µä¸”ä¸çµæ´»

**Lakehouseç»“åˆä¸¤è€…ä¼˜åŠ¿**ï¼š
- ğŸ“¦ MinIOï¼šæä¾›ä¾¿å®œçš„å¯¹è±¡å­˜å‚¨ï¼ˆç±»ä¼¼S3ï¼‰
- ğŸ”’ Delta Lakeï¼šåœ¨å¯¹è±¡å­˜å‚¨ä¹‹ä¸Šæä¾›ACIDäº‹åŠ¡
- ğŸ“Š Bronze/Silver/Goldï¼šæ•°æ®åˆ†å±‚ç®¡ç†ï¼Œé€æ­¥æå‡æ•°æ®è´¨é‡

#### 3. **å®¹å™¨åŒ–å¼€å‘ç¯å¢ƒ**

**ä¸ºä»€ä¹ˆç”¨Docker Composeè€Œä¸æ˜¯Kubernetesï¼Ÿ**

**å¼€å‘é˜¶æ®µéœ€æ±‚**ï¼š
- âœ… å¿«é€Ÿå¯åŠ¨/åœæ­¢
- âœ… æœ¬åœ°è¿è¡Œä¸éœ€è¦äº‘èµ„æº
- âœ… æ˜“äºè°ƒè¯•å’Œä¿®æ”¹é…ç½®
- âœ… èµ„æºå ç”¨å¯æ§ï¼ˆç¬”è®°æœ¬ç”µè„‘èƒ½è·‘ï¼‰

Docker Composeå®Œç¾æ»¡è¶³è¿™äº›éœ€æ±‚ï¼Œè€ŒKuberneteså¯¹äºå•æœºå¼€å‘æ¥è¯´è¿‡äºé‡é‡çº§ã€‚

**ç”Ÿäº§ç¯å¢ƒå†è¿ç§»åˆ°K8s**æ—¶ï¼Œå®¹å™¨åŒ–çš„å¥½å¤„ï¼š
- é…ç½®å·²ç»å®¹å™¨åŒ–ï¼Œåªéœ€ç¼–å†™K8s manifest
- åº”ç”¨ä»£ç æ— éœ€ä¿®æ”¹
- å¯ä»¥é€ä¸ªæœåŠ¡è¿ç§»

#### 4. **è§£è€¦è®¾è®¡**ï¼ˆæ¾è€¦åˆæ¶æ„ï¼‰

æ¯ä¸ªç»„ä»¶éƒ½å¯ä»¥ç‹¬ç«‹æ›¿æ¢ï¼š
- é‡‡é›†å™¨ï¼šå¯ä»¥æ¢æˆå…¶ä»–æ•°æ®æºï¼ˆHackerNewsã€Podcastç­‰ï¼‰
- Kafkaï¼šå¯ä»¥æ¢æˆPulsarã€Redis Streams
- Sparkï¼šå¯ä»¥æ¢æˆFlinkã€Storm
- MinIOï¼šå¯ä»¥æ¢æˆAWS S3ã€Azure Blob
- Dashboardï¼šå¯ä»¥æ¢æˆGrafanaã€Tableau

è¿™ç§è®¾è®¡è®©ç³»ç»Ÿéå¸¸çµæ´»ï¼Œæ–¹ä¾¿å­¦ä¹ å’Œè¿­ä»£ã€‚

---

**æ¥ä¸‹æ¥**ï¼š[Part 2: æ ¸å¿ƒæŠ€æœ¯æ·±åº¦è§£æ](#part-2-æ ¸å¿ƒæŠ€æœ¯æ·±åº¦è§£æ) å°†è¯¦ç»†è§£é‡Šæ¯ä¸ªæŠ€æœ¯çš„å·¥ä½œåŸç†ã€æ¶æ„é€‰å‹åŸå› ã€ä»¥åŠåœ¨é¡¹ç›®ä¸­çš„å…·ä½“ä½¿ç”¨ã€‚

---

# Part 2: æ ¸å¿ƒæŠ€æœ¯æ·±åº¦è§£æ

> æŒ‰ç…§æ•°æ®æµçš„é¡ºåºï¼Œé€å±‚æ·±å…¥è®²è§£æ¯ä¸ªæŠ€æœ¯

---

## 2.1 æ•°æ®é‡‡é›†å±‚ (Data Ingestion Layer)

### Pythoné‡‡é›†å™¨æ¡†æ¶

#### ä¸€å¥è¯ä»‹ç»
**ç”¨Pythonè„šæœ¬å®šæœŸè°ƒç”¨ç¤¾äº¤åª’ä½“APIï¼Œå°†æ•°æ®æ ‡å‡†åŒ–åå‘é€åˆ°Kafkaã€‚**

#### ä¸ºä»€ä¹ˆéœ€è¦é‡‡é›†å±‚ï¼Ÿ

**é—®é¢˜**ï¼šç¤¾äº¤åª’ä½“APIå„æœ‰å„çš„æ ¼å¼ã€é™æµè§„åˆ™ã€è®¤è¯æ–¹å¼ã€‚

**è§£å†³**ï¼šç»Ÿä¸€çš„é‡‡é›†å™¨æ¡†æ¶ï¼š
- ğŸ”„ å®šæœŸé‡‡é›†ï¼ˆscheduleåº“ï¼‰
- ğŸ”Œ ç»Ÿä¸€æ¥å£ï¼ˆKafka Producerï¼‰
- ğŸ›¡ï¸ é”™è¯¯å¤„ç†ï¼ˆé‡è¯•ã€æ—¥å¿—ï¼‰
- â±ï¸ é™æµæ§åˆ¶ï¼ˆé¿å…API banï¼‰

#### æŠ€æœ¯é€‰å‹ï¼šTweepy (Twitter) vs PRAW (Reddit)

| ç‰¹æ€§ | Tweepy | PRAW |
|------|--------|------|
| **APIç‰ˆæœ¬** | Twitter API v2 | Reddit API (OAuth2) |
| **è®¤è¯æ–¹å¼** | Bearer Token | Client ID + Secret |
| **é€Ÿç‡é™åˆ¶** | ä¸¥æ ¼ï¼ˆ500K tweets/æœˆï¼‰ | å®½æ¾ï¼ˆ60æ¬¡/åˆ†é’Ÿï¼‰ |
| **æ•°æ®ç»“æ„** | å¤æ‚çš„åµŒå¥—JSON | æ‰å¹³åŒ–å¯¹è±¡ |
| **æ˜“ç”¨æ€§** | â­â­â­ | â­â­â­â­â­ |
| **æ–‡æ¡£è´¨é‡** | å®˜æ–¹è¯¦ç»† | ç¤¾åŒºä¼˜ç§€ |

**ä¸ºä»€ä¹ˆåŒæ—¶ç”¨ä¸¤ä¸ªï¼Ÿ**
- Twitterï¼šè¦†ç›–ä¸»æµç¤¾äº¤åª’ä½“
- Redditï¼šæŠ€æœ¯è®¨è®ºç¤¾åŒºï¼ˆæ›´æ·±å…¥ï¼‰

---

### Tweepyæ·±åº¦è§£æ

#### å·¥ä½œåŸç†ï¼ˆç™½è¯ï¼‰

Tweepyæ˜¯Twitter APIçš„Pythonå°è£…ï¼Œæƒ³è±¡æˆï¼š
- **ä½ **ï¼šPythonè„šæœ¬
- **Tweepy**ï¼šç¿»è¯‘å™¨
- **Twitter**ï¼šæ•°æ®åº“

ä½ ç”¨Pythonè¯´"ç»™æˆ‘æœ€è¿‘çš„AIç›¸å…³æ¨æ–‡"ï¼ŒTweepyç¿»è¯‘æˆTwitter APIèƒ½ç†è§£çš„HTTPè¯·æ±‚ï¼Œç„¶åæŠŠè¿”å›çš„JSONç¿»è¯‘å›Pythonå¯¹è±¡ã€‚

#### é¡¹ç›®ä¸­çš„å…·ä½“ä½¿ç”¨

**æ–‡ä»¶**ï¼š`data_ingestion/twitter/collector.py`

```python
# ç¬¬1æ­¥ï¼šåˆå§‹åŒ–å®¢æˆ·ç«¯
client = tweepy.Client(bearer_token=TWITTER_BEARER_TOKEN)

# ç¬¬2æ­¥ï¼šå®šä¹‰æœç´¢æŸ¥è¯¢
query = '(AI OR "artificial intelligence" OR "machine learning") lang:en -is:retweet'
# è§£é‡Šï¼š
# - AIç›¸å…³å…³é”®è¯
# - lang:enï¼šåªè¦è‹±æ–‡
# - -is:retweetï¼šæ’é™¤è½¬å‘

# ç¬¬3æ­¥ï¼šè°ƒç”¨API
tweets = client.search_recent_tweets(
    query=query,
    max_results=100,  # æ¯æ¬¡æœ€å¤š100æ¡
    tweet_fields=['created_at', 'public_metrics', 'author_id'],
    expansions=['author_id'],
    user_fields=['username']
)

# ç¬¬4æ­¥ï¼šè§£ææ•°æ®
for tweet in tweets.data:
    data = {
        'id': tweet.id,
        'text': tweet.text,
        'created_at': tweet.created_at.isoformat(),
        'author_username': get_author_username(tweet, tweets.includes),
        'metrics': {
            'likes': tweet.public_metrics['like_count'],
            'retweets': tweet.public_metrics['retweet_count']
        }
    }

    # ç¬¬5æ­¥ï¼šå‘é€åˆ°Kafka
    producer.send_tweet(data)
```

#### é…ç½®è¦ç‚¹

**ç¯å¢ƒå˜é‡**ï¼ˆ`config/.env`ï¼‰ï¼š
```bash
TWITTER_BEARER_TOKEN=AAAAAAAA...  # ä»Twitter Developer Portalè·å–
```

**é‡è¦å‚æ•°**ï¼š
- `max_results=100`ï¼šAPIé™åˆ¶çš„å•æ¬¡æœ€å¤§è¿”å›æ•°
- `query`å¤æ‚åº¦ï¼šå¤ªå¤æ‚ä¼šé™ä½åŒ¹é…ç‡ï¼Œå¤ªç®€å•ä¼šæœ‰å™ªéŸ³

#### å¸¸è§å‘ç‚¹

1. **Rate Limitï¼ˆé™æµï¼‰**
   - **ç—‡çŠ¶**ï¼š`429 Too Many Requests`
   - **åŸå› **ï¼šè¶…è¿‡APIé…é¢ï¼ˆEssential access: 500K tweets/æœˆï¼‰
   - **è§£å†³**ï¼š
     - å¢åŠ é‡‡é›†é—´éš”ï¼ˆ600ç§’ â†’ 3600ç§’ï¼‰
     - å‡çº§åˆ°Elevated accessï¼ˆ2M tweets/æœˆï¼‰
     - æœ¬é¡¹ç›®æš‚æ—¶ç¦ç”¨Twitterï¼Œåªç”¨Reddit

2. **Tokenæƒé™ä¸è¶³**
   - **ç—‡çŠ¶**ï¼š`401 Unauthorized`
   - **åŸå› **ï¼šä½¿ç”¨äº†API Keyè€Œä¸æ˜¯Bearer Token
   - **è§£å†³**ï¼šåœ¨Twitter Developer Portalæ£€æŸ¥"Keys and Tokens"ï¼Œä½¿ç”¨Bearer Token

---

### PRAWæ·±åº¦è§£æ

#### å·¥ä½œåŸç†ï¼ˆç™½è¯ï¼‰

PRAW = **P**ython **R**eddit **A**PI **W**rapper

æƒ³è±¡Redditæ˜¯ä¸€ä¸ªå·¨å¤§çš„å›¾ä¹¦é¦†ï¼š
- **Subreddit**ï¼šä¸åŒä¸»é¢˜çš„ä¹¦æ¶ï¼ˆr/MachineLearningã€r/LocalLLaMAï¼‰
- **Post**ï¼šä¹¦æ¶ä¸Šçš„ä¹¦
- **PRAW**ï¼šå›¾ä¹¦ç®¡ç†å‘˜ï¼Œå¸®ä½ æ‰¾ä¹¦

#### é¡¹ç›®ä¸­çš„å…·ä½“ä½¿ç”¨

**æ–‡ä»¶**ï¼š`data_ingestion/reddit/collector.py`

```python
# ç¬¬1æ­¥ï¼šåˆå§‹åŒ–PRAWå®¢æˆ·ç«¯
reddit = praw.Reddit(
    client_id=REDDIT_CLIENT_ID,        # åº”ç”¨ID
    client_secret=REDDIT_CLIENT_SECRET, # åº”ç”¨å¯†é’¥
    user_agent=REDDIT_USER_AGENT       # æ ‡è¯†ä½ çš„åº”ç”¨ï¼ˆå¦‚"AI_Trend_Monitor/1.0"ï¼‰
)

# ç¬¬2æ­¥ï¼šå®šä¹‰ç›®æ ‡Subreddits
TARGET_SUBREDDITS = [
    'MachineLearning',  # å­¦æœ¯è®¨è®º
    'LocalLLaMA',       # æœ¬åœ°LLMè¿è¡Œ
    'artificial',       # AIé€šç”¨è®¨è®º
]

# ç¬¬3æ­¥ï¼šé‡‡é›†æ–°å¸–å­
for subreddit_name in TARGET_SUBREDDITS:
    subreddit = reddit.subreddit(subreddit_name)

    # è·å–æœ€æ–°çš„10ç¯‡å¸–å­
    for post in subreddit.new(limit=10):
        data = {
            'id': post.id,
            'title': post.title,
            'text': post.selftext,  # å¸–å­æ­£æ–‡
            'author': str(post.author),
            'subreddit': subreddit_name,
            'created_utc': post.created_utc,
            'metrics': {
                'score': post.score,      # èµ-è¸©
                'comments': post.num_comments
            }
        }

        # ç¬¬4æ­¥ï¼šå‘é€åˆ°Kafka
        producer.send_reddit_post(data)
```

#### å…³é”®æ¦‚å¿µè§£é‡Š

**1. OAuth2 Script Mode**

Reddit APIæœ‰å¤šç§è®¤è¯æ–¹å¼ï¼Œæˆ‘ä»¬ç”¨çš„æ˜¯**Scriptæ¨¡å¼**ï¼š
- âœ… é€‚åˆï¼šä¸ªäººè„šæœ¬ã€åå°ä»»åŠ¡
- âœ… æ— éœ€ç”¨æˆ·äº¤äº’ï¼ˆä¸éœ€è¦æµè§ˆå™¨æˆæƒï¼‰
- âŒ é™åˆ¶ï¼šåªèƒ½è¯»å–å…¬å¼€æ•°æ®ï¼ˆå¤Ÿç”¨äº†ï¼‰

**2. User Agent**

Redditè¦æ±‚æ¯ä¸ªåº”ç”¨æä¾›User Agentæ ‡è¯†ï¼š
```python
user_agent = "AI_Trend_Monitor/1.0 by /u/your_reddit_username"
```

**æ ¼å¼**ï¼š`<App Name>/<Version> by /u/<Reddit Username>`

**ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ**
- Redditç”¨æ¥è¯†åˆ«æµé‡æ¥æº
- å¦‚æœUser Agentä¸è§„èŒƒï¼Œå¯èƒ½è¢«é™æµ

**3. Rate Limitï¼ˆé™æµï¼‰**

Reddit APIé™åˆ¶ï¼š
- **60æ¬¡è¯·æ±‚/åˆ†é’Ÿ**ï¼ˆæ¯”Twitterå®½æ¾å¾ˆå¤šï¼‰
- æˆ‘ä»¬æ¯60ç§’é‡‡é›†ä¸€æ¬¡ï¼Œå®Œå…¨å®‰å…¨

#### é…ç½®è¦ç‚¹

**ç¯å¢ƒå˜é‡**ï¼ˆ`config/.env`ï¼‰ï¼š
```bash
REDDIT_CLIENT_ID=abcd1234efgh          # åº”ç”¨ID
REDDIT_CLIENT_SECRET=xyz789secret      # åº”ç”¨å¯†é’¥
REDDIT_USER_AGENT=AI_Trend_Monitor/1.0 # è‡ªå®šä¹‰æ ‡è¯†
```

**å¦‚ä½•è·å–ï¼Ÿ**
1. è®¿é—® https://www.reddit.com/prefs/apps
2. ç‚¹å‡»"Create App" â†’ é€‰æ‹©**"script"**ç±»å‹
3. redirect_uriå¡«`http://localhost:8080`ï¼ˆå¿…å¡«ä½†ä¸ä¼šç”¨ï¼‰
4. è®°å½•Client IDï¼ˆappåç§°ä¸‹æ–¹ï¼‰å’ŒSecret

---

### Kafka Producerï¼ˆå‘é€æ•°æ®åˆ°Kafkaï¼‰

#### ä¸€å¥è¯ä»‹ç»
**Pythonç¨‹åºé€šè¿‡kafka-pythonåº“ï¼Œå°†é‡‡é›†çš„æ•°æ®å‘é€åˆ°Kafka topicã€‚**

#### å·¥ä½œåŸç†ï¼ˆç™½è¯ï¼‰

æƒ³è±¡Kafkaæ˜¯ä¸€ä¸ª**é‚®å±€**ï¼š
- **Topic**ï¼šä¸åŒçš„é‚®ç®±ï¼ˆæˆ‘ä»¬çš„æ˜¯`ai-social-raw`ï¼‰
- **Producer**ï¼šå¯„ä¿¡äººï¼ˆæˆ‘ä»¬çš„é‡‡é›†å™¨ï¼‰
- **Message**ï¼šä¿¡ä»¶ï¼ˆJSONæ ¼å¼çš„æ•°æ®ï¼‰
- **Key**ï¼šä¿¡å°ä¸Šçš„ç¼–å·ï¼ˆç”¨äºåˆ†åŒºï¼Œæˆ‘ä»¬ç”¨post_idï¼‰

ProduceræŠŠæ•°æ®å°è£…æˆä¿¡ä»¶ï¼Œæ‰”åˆ°é‚®ç®±é‡Œï¼Œç„¶åå°±ä¸ç®¡äº†ï¼ˆå¼‚æ­¥å‘é€ï¼‰ã€‚

#### é¡¹ç›®ä¸­çš„å…·ä½“ä½¿ç”¨

**æ–‡ä»¶**ï¼š`data_ingestion/kafka_producer.py`

```python
from kafka import KafkaProducer
import json

class SocialMediaProducer:
    def __init__(self):
        # åˆ›å»ºç”Ÿäº§è€…
        self.producer = KafkaProducer(
            # ===== è¿æ¥é…ç½® =====
            bootstrap_servers='localhost:9092',  # Kafkaåœ°å€

            # ===== åºåˆ—åŒ–é…ç½® =====
            # å°†Python dictè½¬æ¢ä¸ºJSON bytes
            value_serializer=lambda v: json.dumps(v).encode('utf-8'),

            # å°†post_idè½¬æ¢ä¸ºbytesï¼ˆç”¨äºåˆ†åŒºï¼‰
            key_serializer=lambda k: k.encode('utf-8') if k else None,

            # ===== å¯é æ€§é…ç½® =====
            acks='all',  # ç­‰å¾…æ‰€æœ‰å‰¯æœ¬ç¡®è®¤ï¼ˆæœ€é«˜å¯é æ€§ï¼‰
            # acks=0  # ä¸ç­‰å¾…ç¡®è®¤ï¼ˆæœ€å¿«ï¼Œä½†å¯èƒ½ä¸¢æ•°æ®ï¼‰
            # acks=1  # åªç­‰å¾…leaderç¡®è®¤ï¼ˆæŠ˜ä¸­ï¼‰

            retries=3,  # å¤±è´¥é‡è¯•3æ¬¡

            # ===== é¡ºåºä¿è¯é…ç½® =====
            max_in_flight_requests_per_connection=1,
            # åŒæ—¶åªå‘é€ä¸€ä¸ªè¯·æ±‚ï¼Œä¿è¯æ¶ˆæ¯é¡ºåº

            # ===== å‹ç¼©é…ç½® =====
            compression_type='gzip'  # å‹ç¼©æ•°æ®ï¼ŒèŠ‚çœå¸¦å®½
        )

    def send_reddit_post(self, post_data: dict):
        """å‘é€Redditå¸–å­åˆ°Kafka"""

        # æ„é€ æ¶ˆæ¯æ ¼å¼
        message = {
            'source': 'reddit',             # æ•°æ®æ¥æº
            'data': post_data,              # åŸå§‹æ•°æ®
            'timestamp': post_data.get('created_utc')  # æ—¶é—´æˆ³
        }

        # ä½¿ç”¨post_idä½œä¸ºkeyï¼ˆä¿è¯åŒä¸€å¸–å­çš„æ›´æ–°æœ‰åºï¼‰
        post_id = post_data.get('id')

        # å‘é€åˆ°Kafka
        future = self.producer.send(
            'ai-social-raw',  # topicåç§°
            value=message,    # æ¶ˆæ¯å†…å®¹
            key=str(post_id)  # åˆ†åŒºkey
        )

        # ç­‰å¾…å‘é€å®Œæˆï¼ˆåŒæ­¥ç¡®è®¤ï¼‰
        record_metadata = future.get(timeout=10)

        print(f"âœ… Sent to partition {record_metadata.partition}, "
              f"offset {record_metadata.offset}")
```

#### å…³é”®é…ç½®è§£æ

##### 1. `acks='all'`ï¼ˆå¯é æ€§vsæ€§èƒ½æƒè¡¡ï¼‰

| é…ç½® | å«ä¹‰ | å»¶è¿Ÿ | å¯é æ€§ | ä½¿ç”¨åœºæ™¯ |
|------|------|------|--------|----------|
| `acks=0` | ä¸ç­‰ç¡®è®¤ | æœ€ä½ | âŒ å¯èƒ½ä¸¢æ•°æ® | æ—¥å¿—é‡‡é›†ã€ç‚¹å‡»æµ |
| `acks=1` | Leaderç¡®è®¤ | ä¸­ç­‰ | âš ï¸ LeaderæŒ‚äº†å¯èƒ½ä¸¢ | ä¸€èˆ¬ä¸šåŠ¡æ•°æ® |
| `acks='all'` | æ‰€æœ‰å‰¯æœ¬ç¡®è®¤ | æœ€é«˜ | âœ… æœ€å¯é  | é‡‘èäº¤æ˜“ã€å…³é”®æ•°æ® |

**æˆ‘ä»¬é€‰æ‹©`acks='all'`**ï¼š
- ç¤¾äº¤åª’ä½“æ•°æ®è™½ç„¶ä¸æ˜¯é‡‘èçº§ï¼Œä½†ä¸¢äº†å¾ˆéš¾æ¢å¤
- æˆ‘ä»¬é‡‡é›†é¢‘ç‡æ˜¯60ç§’ï¼Œå»¶è¿Ÿä¸æ•æ„Ÿ

##### 2. `max_in_flight_requests_per_connection=1`ï¼ˆé¡ºåºä¿è¯ï¼‰

**é—®é¢˜åœºæ™¯**ï¼š
```
T1: å‘é€æ¶ˆæ¯Aï¼ˆoffset=100ï¼‰
T2: å‘é€æ¶ˆæ¯Bï¼ˆoffset=101ï¼‰
T3: Aå¤±è´¥ï¼Œé‡è¯•
T4: BæˆåŠŸå†™å…¥offset=101
T5: Aé‡è¯•æˆåŠŸå†™å…¥offset=102

ç»“æœï¼šBåœ¨Aå‰é¢ï¼é¡ºåºé”™ä¹±ï¼
```

**è§£å†³**ï¼šè®¾ç½®=1ï¼ŒåŒä¸€æ—¶é—´åªæœ‰ä¸€ä¸ªè¯·æ±‚åœ¨é€”ï¼š
```
T1: å‘é€æ¶ˆæ¯A
T2: ç­‰å¾…Aå®Œæˆ...
T3: AæˆåŠŸ
T4: å‘é€æ¶ˆæ¯B
ç»“æœï¼šé¡ºåºæ­£ç¡®ï¼
```

**ä»£ä»·**ï¼šååé‡é™ä½ï¼ˆä½†æˆ‘ä»¬ä¸éœ€è¦é«˜ååï¼‰

##### 3. `compression_type='gzip'`ï¼ˆå‹ç¼©ï¼‰

ç¤¾äº¤åª’ä½“æ–‡æœ¬æ•°æ®å‹ç¼©æ¯”å¾ˆé«˜ï¼š
- åŸå§‹JSONï¼š~2KB/æ¡
- Gzipå‹ç¼©ï¼š~500B/æ¡ï¼ˆå‹ç¼©æ¯”75%ï¼‰

**æƒè¡¡**ï¼š
- âœ… èŠ‚çœç½‘ç»œå¸¦å®½
- âœ… èŠ‚çœKafkaå­˜å‚¨ç©ºé—´
- âŒ CPUå¼€é”€å¢åŠ ï¼ˆä½†Pythoné‡‡é›†å™¨CPUé—²ç€ï¼‰

---

## 2.2 æ¶ˆæ¯é˜Ÿåˆ—å±‚ (Message Queue Layer)

### Apache Kafkaæ·±åº¦è§£æ

#### ä¸€å¥è¯ä»‹ç»
**Kafkaæ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼çš„ã€é«˜ååé‡çš„æ¶ˆæ¯é˜Ÿåˆ—ï¼Œä¸“ä¸ºæµå¼æ•°æ®è®¾è®¡ã€‚**

#### ä¸ºä»€ä¹ˆéœ€è¦æ¶ˆæ¯é˜Ÿåˆ—ï¼Ÿ

**æ²¡æœ‰æ¶ˆæ¯é˜Ÿåˆ—çš„é—®é¢˜**ï¼š

```
é‡‡é›†å™¨ â”€â”€â”€â”€â”€â”€ç›´æ¥å†™å…¥â”€â”€â”€â”€â”€â”€â†’ Spark Streaming
   â”‚                            â”‚
   â”œâ”€â”€ å¦‚æœSparkæŒ‚äº†ï¼Ÿ          â”œâ”€â”€ æ•°æ®ä¸¢å¤±ï¼
   â”œâ”€â”€ Sparkå¤„ç†æ…¢ï¼Ÿ            â”œâ”€â”€ é‡‡é›†å™¨è¢«é˜»å¡ï¼
   â””â”€â”€ å¤šä¸ªæ¶ˆè´¹è€…ï¼Ÿ             â””â”€â”€ æ— æ³•å®ç°ï¼
```

**æœ‰Kafkaçš„æ¶æ„**ï¼š

```
é‡‡é›†å™¨ â†’ Kafkaï¼ˆç¼“å†²ï¼‰ â†’ Spark
   â”‚        â”‚            â”‚
   âœ…        âœ…            âœ…
   å¿«é€Ÿ      æŒä¹…åŒ–        æ…¢æ…¢å¤„ç†
   å®Œæˆ      7å¤©          å¯é‡å¯
```

**Kafkaè§£å†³çš„æ ¸å¿ƒé—®é¢˜**ï¼š
1. **è§£è€¦**ï¼šç”Ÿäº§è€…å’Œæ¶ˆè´¹è€…äº’ä¸å½±å“
2. **ç¼“å†²**ï¼šç”Ÿäº§é€Ÿåº¦ â‰  æ¶ˆè´¹é€Ÿåº¦
3. **æŒä¹…åŒ–**ï¼šæ•°æ®å†™å…¥ç£ç›˜ï¼Œä¸æ€•ä¸¢
4. **é‡æ”¾**ï¼šå¯ä»¥ä»ä»»æ„offseté‡æ–°æ¶ˆè´¹
5. **æ‰©å±•**ï¼šæ”¯æŒå¤šä¸ªæ¶ˆè´¹è€…å¹¶è¡Œå¤„ç†

#### Kafkaæ ¸å¿ƒæ¦‚å¿µï¼ˆç™½è¯è§£é‡Šï¼‰

##### 1. Topicï¼ˆä¸»é¢˜ï¼‰= æ•°æ®çš„åˆ†ç±»

æƒ³è±¡Kafkaæ˜¯ä¸€ä¸ªå›¾ä¹¦é¦†ï¼š
- **Topic**ï¼šä¹¦æ¶çš„æ ‡ç­¾ï¼ˆ"AIç›¸å…³"ã€"è´¢ç»æ–°é—»"ï¼‰
- æˆ‘ä»¬çš„topicï¼š`ai-social-raw`ï¼ˆAIç¤¾äº¤åª’ä½“åŸå§‹æ•°æ®ï¼‰

##### 2. Partitionï¼ˆåˆ†åŒºï¼‰= ä¹¦æ¶çš„æ ¼å­

ä¸€ä¸ªtopicå¯ä»¥åˆ†æˆå¤šä¸ªpartitionï¼š
```
Topic: ai-social-raw
â”œâ”€ Partition 0: [msg1, msg2, msg5, ...]
â”œâ”€ Partition 1: [msg3, msg6, msg7, ...]
â””â”€ Partition 2: [msg4, msg8, msg9, ...]
```

**ä¸ºä»€ä¹ˆåˆ†åŒºï¼Ÿ**
- **å¹¶è¡Œå¤„ç†**ï¼šå¤šä¸ªæ¶ˆè´¹è€…å¯ä»¥åŒæ—¶è¯»ä¸åŒåˆ†åŒº
- **æ‰©å±•æ€§**ï¼šåˆ†åŒºå¯ä»¥åˆ†å¸ƒåœ¨ä¸åŒæœºå™¨
- **é¡ºåºä¿è¯**ï¼šåŒä¸€åˆ†åŒºå†…æ¶ˆæ¯æœ‰åº

**åˆ†åŒºè§„åˆ™**ï¼š
```python
# å¦‚æœæŒ‡å®šäº†keyï¼ˆå¦‚post_idï¼‰
partition = hash(key) % num_partitions

# å¦‚æœæ²¡æœ‰key
partition = round_robin  # è½®è¯¢åˆ†é…
```

**æˆ‘ä»¬çš„é…ç½®**ï¼šå•åˆ†åŒºï¼ˆå› ä¸ºæ˜¯å•æœºç¯å¢ƒï¼‰

##### 3. Offsetï¼ˆåç§»é‡ï¼‰= ä¹¦çš„é¡µç 

æ¯æ¡æ¶ˆæ¯åœ¨åˆ†åŒºä¸­æœ‰å”¯ä¸€çš„offsetï¼š
```
Partition 0:
[offset=0] â†’ æ¶ˆæ¯A
[offset=1] â†’ æ¶ˆæ¯B
[offset=2] â†’ æ¶ˆæ¯C
```

**æ¶ˆè´¹è€…è®°ä½offset**ï¼Œä¸‹æ¬¡ä»æ–­ç‚¹ç»§ç»­è¯»ï¼š
```python
# æ¶ˆè´¹è€…Aè¯»åˆ°offset=1000
# æ¶ˆè´¹è€…é‡å¯åï¼Œä»offset=1001ç»§ç»­
```

##### 4. Consumer Groupï¼ˆæ¶ˆè´¹è€…ç»„ï¼‰= è¯»ä¹¦å°ç»„

```
Topic: ai-social-raw (3ä¸ªåˆ†åŒº)
â”œâ”€ Partition 0 â†’ Consumer 1 (Group: spark-processor)
â”œâ”€ Partition 1 â†’ Consumer 2 (Group: spark-processor)
â””â”€ Partition 2 â†’ Consumer 3 (Group: spark-processor)

åŒæ—¶ï¼š
â”œâ”€ Partition 0 â†’ Consumer A (Group: dashboard)
â”œâ”€ Partition 1 â†’ Consumer B (Group: dashboard)
â””â”€ Partition 2 â†’ Consumer C (Group: dashboard)
```

**åŒä¸€Groupå†…**ï¼š
- æ¯ä¸ªåˆ†åŒºåªè¢«ä¸€ä¸ªæ¶ˆè´¹è€…è¯»ï¼ˆé¿å…é‡å¤ï¼‰
- æ¶ˆè´¹è€…æ•° â‰¤ åˆ†åŒºæ•°ï¼ˆå¤šäº†ä¹Ÿæ²¡ç”¨ï¼‰

**ä¸åŒGroupé—´**ï¼š
- å„è‡ªç‹¬ç«‹æ¶ˆè´¹ï¼ˆäº’ä¸å½±å“ï¼‰
- Dashboardå’ŒSparkå¯ä»¥åŒæ—¶è¯»åŒä¸€æ•°æ®

#### é¡¹ç›®ä¸­çš„Kafkaé…ç½®

**Docker Composeé…ç½®**ï¼ˆ`docker-compose-full.yml:19-45`ï¼‰ï¼š

```yaml
kafka:
  image: confluentinc/cp-kafka:7.5.0
  container_name: kafka
  hostname: kafka
  depends_on:
    - zookeeper
  ports:
    - "9092:9092"    # å®¿ä¸»æœºè®¿é—®ç«¯å£
    - "29092:29092"  # å®¹å™¨é—´è®¿é—®ç«¯å£
  environment:
    KAFKA_BROKER_ID: 1  # Brokerå”¯ä¸€ID

    # Zookeeperè¿æ¥
    KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181

    # ç›‘å¬å™¨é…ç½®ï¼ˆé‡è¦ï¼ï¼‰
    KAFKA_ADVERTISED_LISTENERS: >
      PLAINTEXT://kafka:29092,        # å®¹å™¨é—´é€šä¿¡
      PLAINTEXT_HOST://localhost:9092 # å®¿ä¸»æœºé€šä¿¡

    KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: >
      PLAINTEXT:PLAINTEXT,
      PLAINTEXT_HOST:PLAINTEXT

    KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT

    # Topicé…ç½®
    KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1  # å•èŠ‚ç‚¹ï¼Œå‰¯æœ¬æ•°=1
    KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"    # è‡ªåŠ¨åˆ›å»ºtopic

    # æ•°æ®ä¿ç•™
    KAFKA_LOG_RETENTION_HOURS: 168  # 7å¤©ï¼ˆ7*24=168å°æ—¶ï¼‰
```

#### ç›‘å¬å™¨é…ç½®è¯¦è§£ï¼ˆé‡è¦å‘ç‚¹ï¼‰

**é—®é¢˜**ï¼šä¸ºä»€ä¹ˆéœ€è¦ä¸¤ä¸ªç›‘å¬å™¨ï¼Ÿ

```
åœºæ™¯1: Sparkå®¹å™¨è®¿é—®Kafka
  Spark â†’ kafka:29092 âœ… (å®¹å™¨åè§£æ)

åœºæ™¯2: å®¿ä¸»æœºPythonè„šæœ¬è®¿é—®Kafka
  Python â†’ localhost:9092 âœ… (ç«¯å£æ˜ å°„)

å¦‚æœåªé…ç½®ä¸€ä¸ªï¼Ÿ
  Spark â†’ localhost:9092 âŒ (localhostæŒ‡å‘Sparkè‡ªå·±)
  Python â†’ kafka:29092 âŒ (æ— æ³•è§£ækafkaå®¹å™¨å)
```

**è§£å†³**ï¼šä¸¤ä¸ªç›‘å¬å™¨ï¼Œå„å¸å…¶èŒï¼š
- `PLAINTEXT://kafka:29092`ï¼šå®¹å™¨é—´é€šä¿¡
- `PLAINTEXT_HOST://localhost:9092`ï¼šå®¿ä¸»æœºé€šä¿¡

#### Kafkaå¸¸ç”¨å‘½ä»¤

```bash
# 1. æŸ¥çœ‹æ‰€æœ‰topics
docker exec kafka kafka-topics --bootstrap-server localhost:9092 --list

# 2. æŸ¥çœ‹topicè¯¦æƒ…
docker exec kafka kafka-topics --bootstrap-server localhost:9092 \
  --describe --topic ai-social-raw

# 3. æ¶ˆè´¹æ¶ˆæ¯ï¼ˆä»å¤´å¼€å§‹ï¼‰
docker exec -it kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic ai-social-raw \
  --from-beginning \
  --max-messages 10

# 4. æŸ¥çœ‹æ¶ˆæ¯æ•°é‡
docker exec kafka kafka-run-class kafka.tools.GetOffsetShell \
  --broker-list localhost:9092 \
  --topic ai-social-raw
```

---

### Zookeeperæ·±åº¦è§£æ

#### ä¸€å¥è¯ä»‹ç»
**Zookeeperæ˜¯Kafkaçš„åè°ƒæœåŠ¡ï¼Œç®¡ç†é›†ç¾¤å…ƒæ•°æ®å’Œleaderé€‰ä¸¾ã€‚**

#### ä¸ºä»€ä¹ˆKafkaéœ€è¦Zookeeperï¼Ÿ

Kafkaæ˜¯åˆ†å¸ƒå¼ç³»ç»Ÿï¼Œéœ€è¦è§£å†³ï¼š
1. **è°æ˜¯Leaderï¼Ÿ**ï¼ˆæ¯ä¸ªåˆ†åŒºçš„ä¸»å‰¯æœ¬ï¼‰
2. **å“ªäº›Brokeræ´»ç€ï¼Ÿ**ï¼ˆå¥åº·æ£€æŸ¥ï¼‰
3. **Topicé…ç½®å­˜å“ªï¼Ÿ**ï¼ˆå…ƒæ•°æ®ç®¡ç†ï¼‰
4. **Consumer GroupçŠ¶æ€ï¼Ÿ**ï¼ˆoffsetè®°å½•ï¼‰

Zookeeperå°±æ˜¯é‚£ä¸ª**è®°äº‹æœ¬**ï¼Œè®°å½•è¿™äº›ä¿¡æ¯ã€‚

#### Zookeeperå­˜å‚¨çš„Kafkaå…ƒæ•°æ®

```
/brokers
  /ids
    /1 â†’ {"host":"kafka", "port":9092}  # Brokerä¿¡æ¯
  /topics
    /ai-social-raw
      /partitions
        /0
          /state â†’ {"leader":1, "isr":[1]}  # Leaderå’Œå‰¯æœ¬
```

#### é¡¹ç›®ä¸­çš„Zookeeperé…ç½®

**Docker Composeé…ç½®**ï¼ˆ`docker-compose-full.yml:3-16`ï¼‰ï¼š

```yaml
zookeeper:
  image: confluentinc/cp-zookeeper:7.5.0
  container_name: zookeeper
  hostname: zookeeper
  ports:
    - "2181:2181"  # Clientç«¯å£
  environment:
    ZOOKEEPER_CLIENT_PORT: 2181      # å®¢æˆ·ç«¯è¿æ¥ç«¯å£
    ZOOKEEPER_TICK_TIME: 2000        # å¿ƒè·³é—´éš”ï¼ˆæ¯«ç§’ï¼‰
  volumes:
    - zookeeper-data:/var/lib/zookeeper/data   # æ•°æ®æŒä¹…åŒ–
    - zookeeper-logs:/var/lib/zookeeper/log    # æ—¥å¿—æŒä¹…åŒ–
```

#### Kafka Raftæ¨¡å¼ï¼ˆKRaftï¼‰- æœªæ¥è¶‹åŠ¿

**é‡å¤§å˜åŒ–**ï¼šKafka 3.xå¼€å§‹å¯ä»¥ä¸ä¾èµ–Zookeeperï¼

```
ä¼ ç»Ÿæ¨¡å¼:
Kafka â†’ Zookeeper (ç®¡ç†å…ƒæ•°æ®)

KRaftæ¨¡å¼:
Kafka (å†…ç½®Raftåè®®ï¼Œè‡ªå·±ç®¡ç†å…ƒæ•°æ®)
```

**ä¼˜åŠ¿**ï¼š
- âœ… ç®€åŒ–æ¶æ„ï¼ˆå°‘ä¸€ä¸ªç»„ä»¶ï¼‰
- âœ… æ›´å¿«çš„å…ƒæ•°æ®æ“ä½œ
- âœ… æ”¯æŒæ›´å¤šåˆ†åŒºï¼ˆç™¾ä¸‡çº§ï¼‰

**æˆ‘ä»¬ä¸ºä»€ä¹ˆè¿˜ç”¨Zookeeperï¼Ÿ**
- Confluentå®˜æ–¹é•œåƒé»˜è®¤é…ç½®
- ç”Ÿäº§ç¯å¢ƒè¿˜åœ¨è¿‡æ¸¡æœŸ
- å­¦ä¹ ä¼ ç»Ÿæ¶æ„æœ‰ä»·å€¼

**æœªæ¥è¿ç§»**ï¼šå¾ˆç®€å•ï¼Œæ”¹ä¸€ä¸‹é…ç½®å³å¯ã€‚

---

## 2.3 æµå¤„ç†å±‚ (Stream Processing Layer)

### Apache Spark Streamingæ·±åº¦è§£æ

#### ä¸€å¥è¯ä»‹ç»
**Spark Streamingæ˜¯Apache Sparkçš„æµå¤„ç†æ¨¡å—ï¼Œå°†å®æ—¶æ•°æ®æµåˆ†æˆå°æ‰¹æ¬¡ï¼ˆmicro-batchï¼‰è¿›è¡Œå¤„ç†ã€‚**

#### ä¸ºä»€ä¹ˆéœ€è¦æµå¤„ç†ï¼Ÿ

**ä¼ ç»Ÿæ‰¹å¤„ç†çš„é—®é¢˜**ï¼š
```
æ¯å¤©å‡Œæ™¨3ç‚¹ï¼š
  1. è¯»å–æ˜¨å¤©å…¨éƒ¨æ•°æ®ï¼ˆå¯èƒ½TBçº§ï¼‰
  2. å¤„ç†æ•°å°æ—¶
  3. å†™å…¥ç»“æœ

ç”¨æˆ·ï¼šæˆ‘è¦çœ‹å®æ—¶æ•°æ®ï¼
ç³»ç»Ÿï¼šè¯·ç­‰åˆ°æ˜å¤©å‡Œæ™¨4ç‚¹...
```

**æµå¤„ç†çš„ä¼˜åŠ¿**ï¼š
```
æ¯30ç§’ï¼š
  1. è¯»å–è¿™30ç§’çš„æ–°æ•°æ®ï¼ˆKBçº§ï¼‰
  2. å¤„ç†å‡ ç§’é’Ÿ
  3. å†™å…¥ç»“æœ

ç”¨æˆ·ï¼šæˆ‘è¦çœ‹å®æ—¶æ•°æ®ï¼
ç³»ç»Ÿï¼šå¥½çš„ï¼Œå»¶è¿Ÿ<1åˆ†é’Ÿï¼
```

#### Spark Streaming vs Spark Batch

| ç‰¹æ€§ | Spark Batch | Spark Streaming |
|------|-------------|-----------------|
| **å¤„ç†æ¨¡å¼** | ä¸€æ¬¡æ€§å¤„ç†å®Œæ•´æ•°æ®é›† | æŒç»­å¤„ç†æ— ç•Œæ•°æ®æµ |
| **å»¶è¿Ÿ** | åˆ†é’Ÿ-å°æ—¶çº§ | ç§’-åˆ†é’Ÿçº§ |
| **è¾“å…¥** | é™æ€æ–‡ä»¶ï¼ˆHDFS, S3ï¼‰ | åŠ¨æ€æµï¼ˆKafka, Socketï¼‰ |
| **API** | DataFrame/RDD | Structured Streaming |
| **å®¹é”™** | Taské‡è¯• | Checkpoint + WAL |
| **é€‚ç”¨åœºæ™¯** | æŠ¥è¡¨ã€å†å²åˆ†æ | å®æ—¶ç›‘æ§ã€é¢„è­¦ |

**æˆ‘ä»¬çš„é€‰æ‹©**ï¼šSpark Streamingï¼ˆStructured Streaming APIï¼‰

#### æ ¸å¿ƒæ¦‚å¿µï¼šMicro-batch Processing

**ç™½è¯è§£é‡Š**ï¼š

æƒ³è±¡æ•°æ®æµæ˜¯ä¸€æ¡æ²³ï¼š
- **çœŸæ­£çš„æµå¤„ç†**ï¼ˆFlinkï¼‰ï¼šæ¯ä¸€æ»´æ°´ï¼ˆæ¶ˆæ¯ï¼‰æ¥äº†å°±å¤„ç†
- **Micro-batch**ï¼ˆSparkï¼‰ï¼šæ¯éš”30ç§’ï¼Œç”¨æ¡¶æ¥ä¸€æ¡¶æ°´ï¼Œç„¶åæ‰¹é‡å¤„ç†

```
Timeline:
T0-T30ç§’:  æ”¶é›†æ•°æ® â†’ Batch 1 (100æ¡æ¶ˆæ¯)
T30-T60ç§’: æ”¶é›†æ•°æ® â†’ Batch 2 (120æ¡æ¶ˆæ¯)
           åŒæ—¶å¤„ç† Batch 1
T60-T90ç§’: æ”¶é›†æ•°æ® â†’ Batch 3 (95æ¡æ¶ˆæ¯)
           åŒæ—¶å¤„ç† Batch 2
```

**ä¼˜åŠ¿**ï¼š
- âœ… å¯ä»¥ç”¨æ‰¹å¤„ç†çš„ä¼˜åŒ–æŠ€æœ¯
- âœ… ä»£ç å…¼å®¹æ€§å¥½ï¼ˆDataFrame APIç»Ÿä¸€ï¼‰
- âœ… ååé‡é«˜

**åŠ£åŠ¿**ï¼š
- âŒ å»¶è¿Ÿæ¯”çœŸæµå¤„ç†é«˜ï¼ˆä½†ç§’çº§å·²ç»å¤Ÿç”¨ï¼‰

#### é¡¹ç›®ä¸­çš„Sparké…ç½®

**Docker Composeé…ç½®**ï¼ˆ`docker-compose-full.yml:89-134`ï¼‰ï¼š

```yaml
# Spark Master
spark-master:
  image: apache/spark:3.5.0-python3  # å®˜æ–¹é•œåƒï¼Œå†…ç½®Pythonæ”¯æŒ
  container_name: spark-master
  hostname: spark-master
  ports:
    - "8080:8080"  # Web UI
    - "7077:7077"  # Masterç«¯å£ï¼ˆWorkerè¿æ¥ï¼‰
    - "4040:4040"  # Application UIï¼ˆä½œä¸šè¿è¡Œæ—¶ï¼‰
  environment:
    - SPARK_MODE=master
    - SPARK_MASTER_HOST=spark-master
    - SPARK_MASTER_PORT=7077
    - SPARK_MASTER_WEBUI_PORT=8080
  command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
  volumes:
    - ./streaming/spark:/opt/spark-apps  # æŒ‚è½½ä»£ç 
    - ./storage:/opt/storage
    - spark-logs:/opt/spark/logs

# Spark Worker
spark-worker:
  image: apache/spark:3.5.0-python3
  container_name: spark-worker
  hostname: spark-worker
  user: root  # é‡è¦ï¼é¿å…æƒé™é—®é¢˜
  depends_on:
    - spark-master
  ports:
    - "8081:8081"  # Worker Web UI
  environment:
    - SPARK_MODE=worker
    - SPARK_MASTER_URL=spark://spark-master:7077
    - SPARK_WORKER_CORES=2       # ä½¿ç”¨2ä¸ªCPUæ ¸å¿ƒ
    - SPARK_WORKER_MEMORY=2G     # ä½¿ç”¨2GBå†…å­˜
    - SPARK_WORKER_WEBUI_PORT=8081
    - SPARK_WORKER_DIR=/tmp/spark-work  # å·¥ä½œç›®å½•
  command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
  volumes:
    - ./streaming/spark:/opt/spark-apps
    - ./storage:/opt/storage
    - spark-worker-logs:/tmp/spark-work
```

**æ¶æ„æ¨¡å¼**ï¼šStandaloneæ¨¡å¼

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Spark Master (:8080)                â”‚
â”‚  - ç®¡ç†é›†ç¾¤èµ„æº                       â”‚
â”‚  - è°ƒåº¦ä½œä¸š                          â”‚
â”‚  - ç›‘æ§WorkerçŠ¶æ€                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
          spark://spark-master:7077
                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                           â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚ Worker 1     â”‚      â”‚ Worker N    â”‚
â”‚ :8081        â”‚      â”‚ (å¯æ‰©å±•)     â”‚
â”‚ 2 Cores      â”‚      â”‚             â”‚
â”‚ 2G Memory    â”‚      â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å…¶ä»–æ¨¡å¼å¯¹æ¯”**ï¼š

| æ¨¡å¼ | ç®¡ç†å™¨ | é€‚ç”¨åœºæ™¯ | å¤æ‚åº¦ |
|------|--------|----------|--------|
| **Standalone** | Sparkè‡ªå¸¦ | å¼€å‘ã€å°è§„æ¨¡ | â­ ç®€å• |
| **YARN** | Hadoop YARN | å…±äº«Hadoopé›†ç¾¤ | â­â­â­ ä¸­ç­‰ |
| **Kubernetes** | K8s | äº‘åŸç”Ÿéƒ¨ç½² | â­â­â­â­ å¤æ‚ |
| **Mesos** | Apache Mesos | å¤šæ¡†æ¶å…±äº« | â­â­â­â­ å¤æ‚ |

#### Structured Streamingæ ¸å¿ƒAPI

**æ–‡ä»¶**ï¼š`streaming/spark/processor_with_minio.py`

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json, current_timestamp, to_date
from pyspark.sql.types import StructType, StructField, StringType, MapType

# ========== ç¬¬1æ­¥ï¼šåˆ›å»ºSpark Session ==========
spark = (
    SparkSession.builder
    .appName("AI_Trend_Monitor_MinIO")
    .master("spark://spark-master:7077")  # è¿æ¥åˆ°Master

    # Kafkaä¾èµ–
    .config("spark.jars.packages",
            "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,"
            "org.apache.hadoop:hadoop-aws:3.3.4")  # S3Aæ”¯æŒ

    # MinIOé…ç½®ï¼ˆS3å…¼å®¹ï¼‰
    .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000")
    .config("spark.hadoop.fs.s3a.access.key", "minioadmin")
    .config("spark.hadoop.fs.s3a.secret.key", "minioadmin")
    .config("spark.hadoop.fs.s3a.path.style.access", "true")  # è·¯å¾„é£æ ¼è®¿é—®
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
    .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false")  # å…³é—­SSL

    .getOrCreate()
)

# ========== ç¬¬2æ­¥ï¼šä»Kafkaè¯»å–æµ ==========
kafka_df = (
    spark.readStream  # æ³¨æ„ï¼šreadStreamï¼Œä¸æ˜¯read
    .format("kafka")
    .option("kafka.bootstrap.servers", "kafka:29092")  # å®¹å™¨å†…åœ°å€
    .option("subscribe", "ai-social-raw")  # è®¢é˜…topic
    .option("startingOffsets", "earliest")  # ä»å¤´è¯»ï¼ˆé¦–æ¬¡è¿è¡Œï¼‰
    .option("failOnDataLoss", "false")  # å®¹å¿æ•°æ®ä¸¢å¤±ï¼ˆå¼€å‘ç¯å¢ƒï¼‰
    .load()
)

# Kafkaè¿”å›çš„DataFrame Schema:
# |-- key: binary (å¯ç©º)
# |-- value: binary (æ¶ˆæ¯å†…å®¹ï¼ŒJSON bytes)
# |-- topic: string
# |-- partition: integer
# |-- offset: long
# |-- timestamp: timestamp

# ========== ç¬¬3æ­¥ï¼šè§£æJSONæ•°æ® ==========

# å®šä¹‰JSON Schema
KAFKA_MESSAGE_SCHEMA = StructType([
    StructField("source", StringType(), True),
    StructField("timestamp", StringType(), True),
    StructField("data", MapType(StringType(), StringType()), True)
])

parsed_df = (
    kafka_df
    .selectExpr("CAST(value AS STRING) as json_value")  # bytes â†’ string
    .select(from_json(col("json_value"), KAFKA_MESSAGE_SCHEMA).alias("data"))
    .select(
        col("data.source").alias("source"),
        col("data.timestamp").alias("event_timestamp"),
        col("data.data").alias("raw_data"),
        current_timestamp().alias("processed_at")  # å¤„ç†æ—¶é—´æˆ³
    )
    .withColumn("partition_date", to_date(col("processed_at")))  # æ—¥æœŸåˆ†åŒº
)

# ç»“æœDataFrame Schema:
# |-- source: string (reddit/twitter)
# |-- event_timestamp: string (åŸå§‹äº‹ä»¶æ—¶é—´)
# |-- raw_data: map<string,string> (åŸå§‹æ•°æ®)
# |-- processed_at: timestamp (å¤„ç†æ—¶é—´)
# |-- partition_date: date (åˆ†åŒºå­—æ®µï¼Œå¦‚2025-11-12)

# ========== ç¬¬4æ­¥ï¼šå†™å…¥MinIO ==========
query = (
    parsed_df
    .writeStream  # æ³¨æ„ï¼šwriteStreamï¼Œä¸æ˜¯write
    .format("parquet")  # è¾“å‡ºæ ¼å¼
    .outputMode("append")  # è¿½åŠ æ¨¡å¼ï¼ˆvs complete/updateï¼‰
    .option("checkpointLocation", "s3a://lakehouse/checkpoints/bronze")  # å®¹é”™æ£€æŸ¥ç‚¹
    .option("path", "s3a://lakehouse/bronze/social_media")  # è¾“å‡ºè·¯å¾„
    .partitionBy("partition_date", "source")  # æŒ‰æ—¥æœŸå’Œæ¥æºåˆ†åŒº
    .trigger(processingTime='30 seconds')  # æ¯30ç§’ä¸€ä¸ªæ‰¹æ¬¡
    .start()
)

# è¾“å‡ºç›®å½•ç»“æ„:
# s3a://lakehouse/bronze/social_media/
#   partition_date=2025-11-12/
#     source=reddit/
#       part-00000-xxx.snappy.parquet
#       part-00001-xxx.snappy.parquet
#     source=twitter/
#       part-00000-xxx.snappy.parquet

# ========== ç¬¬5æ­¥ï¼šç­‰å¾…ç»ˆæ­¢ ==========
query.awaitTermination()  # é˜»å¡ï¼ŒæŒç»­è¿è¡Œ
```

#### å…³é”®æ¦‚å¿µæ·±åº¦è§£æ

##### 1. Output Modeï¼ˆè¾“å‡ºæ¨¡å¼ï¼‰

| æ¨¡å¼ | å«ä¹‰ | é€‚ç”¨åœºæ™¯ | æˆ‘ä»¬çš„é€‰æ‹© |
|------|------|----------|------------|
| **append** | åªå†™æ–°å¢è¡Œ | åªè¿½åŠ æ•°æ®ï¼Œä¸ä¿®æ”¹å†å² | âœ… åŸå§‹æ•°æ®é‡‡é›† |
| **complete** | æ¯æ¬¡è¾“å‡ºå…¨éƒ¨ç»“æœ | èšåˆç»“æœï¼Œéœ€è¦å…¨é‡ | è¶‹åŠ¿ç»Ÿè®¡è¡¨ |
| **update** | åªè¾“å‡ºæ›´æ–°çš„è¡Œ | å¢é‡æ›´æ–°ï¼Œæœ‰ä¸»é”® | ç”¨æˆ·ç”»åƒè¡¨ |

**ä¸ºä»€ä¹ˆé€‰appendï¼Ÿ**
- æˆ‘ä»¬æ˜¯åŸå§‹æ•°æ®é‡‡é›†ï¼Œæ¯æ¡æ¶ˆæ¯éƒ½æ˜¯æ–°çš„
- ä¸éœ€è¦ä¿®æ”¹å†å²æ•°æ®
- æ€§èƒ½æœ€å¥½ï¼ˆåªå†™å¢é‡ï¼‰

##### 2. Checkpointï¼ˆæ£€æŸ¥ç‚¹ï¼‰

**é—®é¢˜**ï¼šSparkä½œä¸šå´©æºƒé‡å¯ï¼Œå¦‚ä½•é¿å…é‡å¤å¤„ç†ï¼Ÿ

```
T1: å¤„ç†offset 0-100
T2: å¤„ç†offset 101-200
T3: å´©æºƒï¼

é‡å¯åï¼š
  å¦‚æœæ²¡æœ‰Checkpoint â†’ ä»offset 0é‡æ–°å¼€å§‹ï¼ˆé‡å¤ï¼ï¼‰
  å¦‚æœæœ‰Checkpoint â†’ ä»offset 201ç»§ç»­ï¼ˆå®Œç¾ï¼ï¼‰
```

**Checkpointå­˜å‚¨ä»€ä¹ˆï¼Ÿ**
```
s3a://lakehouse/checkpoints/bronze/
  commits/  # å·²æäº¤çš„batch ID
    0, 1, 2, ...
  offsets/  # Kafka offsetè®°å½•
    0: {"ai-social-raw":{"0":123}}
    1: {"ai-social-raw":{"0":456}}
  sources/  # SourceçŠ¶æ€
  state/    # èšåˆçŠ¶æ€ï¼ˆå¦‚æœæœ‰ï¼‰
```

**å®¹é”™ä¿è¯**ï¼š
- âœ… Exactly-onceè¯­ä¹‰ï¼ˆç»“åˆKafka offsetå’ŒCheckpointï¼‰
- âœ… å´©æºƒåè‡ªåŠ¨æ¢å¤
- âœ… ä¸é‡å¤ã€ä¸ä¸¢å¤±

##### 3. Triggerï¼ˆè§¦å‘å™¨ï¼‰

| Triggerç±»å‹ | è¯´æ˜ | å»¶è¿Ÿ | ååé‡ |
|------------|------|------|--------|
| `processingTime='30 seconds'` | å›ºå®šé—´éš” | 30ç§’+ | é«˜ |
| `once=True` | è¿è¡Œä¸€æ¬¡å°±åœ | N/A | - |
| `continuous='1 second'` | è¿ç»­å¤„ç†ï¼ˆå®éªŒæ€§ï¼‰ | 1ç§’+ | æœ€é«˜ |
| é»˜è®¤ï¼ˆmicro-batchï¼‰ | å°½å¿«å¤„ç† | æœ€ä½ | ä¸­ |

**æˆ‘ä»¬é€‰æ‹©30ç§’é—´éš”**ï¼š
- å¹³è¡¡å»¶è¿Ÿå’Œåå
- é¿å…äº§ç”Ÿå¤ªå¤šå°æ–‡ä»¶
- Kafkaæ•°æ®é‡ä¸å¤§ï¼Œä¸éœ€è¦æ›´é¢‘ç¹

##### 4. Partitioningï¼ˆåˆ†åŒºï¼‰

```python
.partitionBy("partition_date", "source")
```

**æ•ˆæœ**ï¼š
```
bronze/social_media/
  partition_date=2025-11-12/source=reddit/part-00000.parquet
  partition_date=2025-11-12/source=twitter/part-00000.parquet
  partition_date=2025-11-13/source=reddit/part-00000.parquet
  partition_date=2025-11-13/source=twitter/part-00000.parquet
```

**å¥½å¤„**ï¼š
- âœ… **æŸ¥è¯¢åŠ é€Ÿ**ï¼šåªè¯»éœ€è¦çš„åˆ†åŒºï¼ˆPartition Pruningï¼‰
  ```sql
  SELECT * FROM bronze
  WHERE partition_date = '2025-11-12'  -- åªæ‰«æ1å¤©çš„æ•°æ®ï¼Œè·³è¿‡å…¶ä»–
  AND source = 'reddit'                 -- åªæ‰«æredditåˆ†åŒº
  ```
- âœ… **æ•°æ®ç®¡ç†**ï¼šæ–¹ä¾¿åˆ é™¤æ—§æ•°æ®
  ```bash
  # åˆ é™¤30å¤©å‰çš„æ•°æ®
  aws s3 rm s3://lakehouse/bronze/social_media/partition_date=2025-10-12/ --recursive
  ```

#### Spark Submitå¯åŠ¨è„šæœ¬

**æ–‡ä»¶**ï¼š`scripts/02-start_spark_minio.sh`

```bash
#!/bin/bash

echo "ğŸš€ Starting Spark Streaming with MinIO..."

# æ¿€æ´»Pythonç¯å¢ƒ
source venv/bin/activate

# Spark Submit
spark-submit \
  --master spark://localhost:7077 \
  --deploy-mode client \
  --name "AI_Trend_Monitor_MinIO" \
  \
  # ===== èµ„æºé…ç½® =====
  --executor-memory 1G \
  --executor-cores 1 \
  --total-executor-cores 2 \
  \
  # ===== Jarä¾èµ–ï¼ˆæœ¬åœ°è·¯å¾„ï¼‰ =====
  --jars streaming/spark/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar,\
streaming/spark/jars/kafka-clients-3.4.1.jar,\
streaming/spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar,\
streaming/spark/jars/commons-pool2-2.11.1.jar,\
streaming/spark/jars/hadoop-aws-3.3.4.jar,\
streaming/spark/jars/aws-java-sdk-bundle-1.12.262.jar \
  \
  # ===== MinIO/S3Aé…ç½® =====
  --conf spark.hadoop.fs.s3a.endpoint=http://localhost:9000 \
  --conf spark.hadoop.fs.s3a.access.key=minioadmin \
  --conf spark.hadoop.fs.s3a.secret.key=minioadmin \
  --conf spark.hadoop.fs.s3a.path.style.access=true \
  --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \
  --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false \
  \
  # ===== Pythonæ–‡ä»¶ =====
  streaming/spark/processor_with_minio.py
```

**å…³é”®å‚æ•°è§£é‡Š**ï¼š

- `--master spark://localhost:7077`ï¼šè¿æ¥åˆ°Spark Master
- `--deploy-mode client`ï¼šDriverè¿è¡Œåœ¨å®¢æˆ·ç«¯ï¼ˆå¯ä»¥çœ‹åˆ°æ—¥å¿—ï¼‰
  - `client`ï¼šDriveråœ¨æäº¤æœºå™¨ï¼ˆé€‚åˆå¼€å‘ï¼‰
  - `cluster`ï¼šDriveråœ¨Workerä¸Šï¼ˆé€‚åˆç”Ÿäº§ï¼‰

- `--executor-memory 1G`ï¼šæ¯ä¸ªExecutorä½¿ç”¨1GBå†…å­˜
- `--total-executor-cores 2`ï¼šæ€»å…±ä½¿ç”¨2ä¸ªCPUæ ¸å¿ƒ

- `--jars`ï¼šä¾èµ–çš„jaræ–‡ä»¶ï¼ˆKafka + S3Aï¼‰
  - ä¸ºä»€ä¹ˆä¸ç”¨`--packages`è‡ªåŠ¨ä¸‹è½½ï¼ŸMavenä¸‹è½½å¯èƒ½å¤±è´¥æˆ–æ…¢

#### Spark Web UIæ·±åº¦è§£è¯»

è®¿é—® http://localhost:8080 å¯ä»¥çœ‹åˆ°ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Spark Master UI (http://localhost:8080)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                        â”‚
â”‚  Workers (1)                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Worker ID: worker-20251112-spark-worker          â”‚  â”‚
â”‚  â”‚ State: ALIVE                                     â”‚  â”‚
â”‚  â”‚ Cores: 2 (0 Used)                                â”‚  â”‚
â”‚  â”‚ Memory: 2.0 GB (0.0 B Used)                      â”‚  â”‚
â”‚  â”‚ UI: http://spark-worker:8081                     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                        â”‚
â”‚  Running Applications (1)                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ App ID: app-20251112120000-0001                  â”‚  â”‚
â”‚  â”‚ Name: AI_Trend_Monitor_MinIO                     â”‚  â”‚
â”‚  â”‚ Cores: 2                                         â”‚  â”‚
â”‚  â”‚ Memory: 2.0 GB                                   â”‚  â”‚
â”‚  â”‚ Submit Date: 2025-11-12 12:00:00                 â”‚  â”‚
â”‚  â”‚ Duration: 1.5 hours                              â”‚  â”‚
â”‚  â”‚ UI: http://localhost:4040                        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Application UI** (http://localhost:4040) æ›´è¯¦ç»†ï¼š

```
Tabs:
â”œâ”€ Jobs: æ¯ä¸ªmicro-batchæ˜¯ä¸€ä¸ªjob
â”œâ”€ Stages: æ¯ä¸ªjobçš„æ‰§è¡Œé˜¶æ®µ
â”œâ”€ Storage: ç¼“å­˜çš„æ•°æ®
â”œâ”€ Environment: Sparké…ç½®
â”œâ”€ Executors: Executorè¯¦æƒ…
â””â”€ Streaming: æµå¤„ç†ä¸“å±
     â”œâ”€ Input Rate: æ¯ç§’å¤„ç†å¤šå°‘æ¡æ¶ˆæ¯
     â”œâ”€ Processing Time: æ¯ä¸ªbatchå¤„ç†è€—æ—¶
     â”œâ”€ Scheduling Delay: æ’é˜Ÿç­‰å¾…æ—¶é—´
     â””â”€ Total Delay: ç«¯åˆ°ç«¯å»¶è¿Ÿ
```

**æ€§èƒ½ç›‘æ§å…³é”®æŒ‡æ ‡**ï¼š
- **Processing Time < Batch Interval**ï¼šå¥åº·ï¼ˆå¦‚15ç§’ < 30ç§’ï¼‰
- **Processing Time â‰ˆ Batch Interval**ï¼šä¸´ç•Œï¼ˆå¦‚28ç§’ < 30ç§’ï¼‰
- **Processing Time > Batch Interval**ï¼šè­¦å‘Šï¼ç§¯å‹ï¼ï¼ˆå¦‚35ç§’ > 30ç§’ï¼‰

---

## 2.4 å­˜å‚¨å±‚ (Storage Layer)

### MinIOæ·±åº¦è§£æ

#### ä¸€å¥è¯ä»‹ç»
**MinIOæ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„å¯¹è±¡å­˜å‚¨æœåŠ¡ï¼Œ100%å…¼å®¹AWS S3 APIã€‚**

#### ä¸ºä»€ä¹ˆéœ€è¦å¯¹è±¡å­˜å‚¨ï¼Ÿ

**å¯¹æ¯”ä¼ ç»Ÿæ–‡ä»¶ç³»ç»Ÿ**ï¼š

| ç‰¹æ€§ | æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ | HDFS | MinIO/S3 |
|------|-------------|------|----------|
| **æ‰©å±•æ€§** | âŒ å•æœºé™åˆ¶ | âœ… åˆ†å¸ƒå¼ï¼ŒPBçº§ | âœ… åˆ†å¸ƒå¼ï¼ŒEBçº§ |
| **å¯é æ€§** | âŒ å•ç‚¹æ•…éšœ | âœ… å‰¯æœ¬æœºåˆ¶ | âœ… çº åˆ ç /å‰¯æœ¬ |
| **API** | æ–‡ä»¶è·¯å¾„ | HDFS CLI | âœ… HTTP REST API |
| **äº‘å…¼å®¹** | âŒ | âŒ | âœ… S3å…¼å®¹ |
| **æˆæœ¬** | ç¡¬ç›˜æˆæœ¬ | æœåŠ¡å™¨æˆæœ¬ | âœ… æŒ‰éœ€ä»˜è´¹ |
| **è¿ç»´å¤æ‚åº¦** | ä½ | é«˜ï¼ˆNameNode/DataNodeï¼‰ | âœ… ä½ï¼ˆæ— çŠ¶æ€ï¼‰ |

**ä¸ºä»€ä¹ˆé€‰MinIOè€Œä¸æ˜¯çœŸæ­£çš„AWS S3ï¼Ÿ**

| åœºæ™¯ | MinIO | AWS S3 |
|------|-------|--------|
| **å¼€å‘/æµ‹è¯•** | âœ… æœ¬åœ°è¿è¡Œï¼Œå…è´¹ | âŒ éœ€è¦ç½‘ç»œï¼Œæœ‰è´¹ç”¨ |
| **æ•°æ®ä¸»æƒ** | âœ… æ•°æ®åœ¨æœ¬åœ° | âŒ æ•°æ®åœ¨AWS |
| **å»¶è¿Ÿ** | âœ… æœ¬åœ°<1ms | âŒ ç½‘ç»œ50-200ms |
| **ç”Ÿäº§ç¯å¢ƒ** | éœ€è¦è‡ªå·±è¿ç»´ | âœ… æ‰˜ç®¡æœåŠ¡ |

**ç­–ç•¥**ï¼š
- å¼€å‘ï¼šMinIOï¼ˆæœ¬åœ°ï¼‰
- ç”Ÿäº§ï¼šè¿ç§»åˆ°S3ï¼ˆæ”¹é…ç½®å³å¯ï¼ŒAPIå…¼å®¹ï¼‰

#### MinIOæ ¸å¿ƒæ¦‚å¿µ

##### 1. Bucketï¼ˆæ¡¶ï¼‰= é¡¶å±‚å‘½åç©ºé—´

```
MinIO Server
â”œâ”€ Bucket: lakehouse
â”‚  â”œâ”€ bronze/social_media/...
â”‚  â”œâ”€ silver/...
â”‚  â””â”€ gold/...
â”œâ”€ Bucket: backups
â””â”€ Bucket: logs
```

**ç±»æ¯”**ï¼š
- S3 Bucket = ç¡¬ç›˜ä¸Šçš„æ ¹ç›®å½•
- Object = æ–‡ä»¶

##### 2. Objectï¼ˆå¯¹è±¡ï¼‰= æ–‡ä»¶ + å…ƒæ•°æ®

```
Object Key: bronze/social_media/partition_date=2025-11-12/source=reddit/part-00000.parquet

Object:
  â”œâ”€ Data: [ParquetäºŒè¿›åˆ¶æ•°æ®]
  â”œâ”€ Metadata:
  â”‚  â”œâ”€ Content-Type: application/octet-stream
  â”‚  â”œâ”€ Content-Length: 2048576 (2MB)
  â”‚  â”œâ”€ ETag: "abc123..."  # å†…å®¹å“ˆå¸Œ
  â”‚  â””â”€ Last-Modified: 2025-11-12 12:30:00
  â””â”€ User Metadata: (å¯è‡ªå®šä¹‰)
     â”œâ”€ x-amz-meta-source: kafka
     â””â”€ x-amz-meta-batch-id: 12345
```

**æ³¨æ„**ï¼š
- Object KeyåŒ…å«"/"ï¼Œä½†ä¸æ˜¯çœŸæ­£çš„ç›®å½•ï¼ˆæ‰å¹³å­˜å‚¨ï¼‰
- MinIO Console UIä¼šæ¨¡æ‹Ÿç›®å½•ç»“æ„

##### 3. S3A FileSystemï¼ˆHadoopé›†æˆï¼‰

Sparké€šè¿‡S3Aåè®®è®¿é—®MinIOï¼š

```
Spark â”€â”€S3A://â”€â”€â†’ MinIO

s3a://lakehouse/bronze/social_media/
  â†“
HTTP API:
  GET http://minio:9000/lakehouse/bronze/social_media/...
```

**é…ç½®**ï¼š
```python
.config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000")
.config("spark.hadoop.fs.s3a.access.key", "minioadmin")
.config("spark.hadoop.fs.s3a.secret.key", "minioadmin")
.config("spark.hadoop.fs.s3a.path.style.access", "true")  # é‡è¦ï¼
```

**`path.style.access=true`æ˜¯ä»€ä¹ˆï¼Ÿ**

```
è™šæ‹Ÿä¸»æœºé£æ ¼ (false):
  http://lakehouse.minio:9000/bronze/social_media/...
  éœ€è¦DNSè§£æ lakehouse.minio

è·¯å¾„é£æ ¼ (true):
  http://minio:9000/lakehouse/bronze/social_media/...
  ä¸éœ€è¦DNSï¼Œç›´æ¥è·¯å¾„
```

æœ¬åœ°å¼€å‘ç”¨è·¯å¾„é£æ ¼æ›´ç®€å•ã€‚

#### é¡¹ç›®ä¸­çš„MinIOé…ç½®

**Docker Composeé…ç½®**ï¼ˆ`docker-compose-full.yml:48-86`ï¼‰ï¼š

```yaml
# MinIOæœåŠ¡
minio:
  image: minio/minio:latest
  container_name: minio
  hostname: minio
  ports:
    - "9000:9000"  # S3 APIç«¯å£
    - "9001:9001"  # Web Consoleç«¯å£
  environment:
    MINIO_ROOT_USER: minioadmin      # è®¿é—®å¯†é’¥
    MINIO_ROOT_PASSWORD: minioadmin  # ç§˜å¯†å¯†é’¥
  command: server /data --console-address ":9001"
  volumes:
    - minio-data:/data  # æ•°æ®æŒä¹…åŒ–
  networks:
    - lakehouse-network

# MinIOåˆå§‹åŒ–ï¼ˆåˆ›å»ºbucketsï¼‰
minio-init:
  image: minio/mc:latest  # MinIO Client
  container_name: minio-init
  depends_on:
    minio:
      condition: service_healthy
  entrypoint: >
    /bin/sh -c "
    mc alias set myminio http://minio:9000 minioadmin minioadmin;
    mc mb myminio/lakehouse --ignore-existing;
    mc mb myminio/bronze --ignore-existing;
    mc mb myminio/silver --ignore-existing;
    mc mb myminio/gold --ignore-existing;
    mc anonymous set download myminio/lakehouse;
    echo 'MinIO buckets created successfully';
    "
  networks:
    - lakehouse-network
```

**MinIO Client (mc) å¸¸ç”¨å‘½ä»¤**ï¼š

```bash
# 1. é…ç½®alias
docker exec minio mc alias set myminio http://localhost:9000 minioadmin minioadmin

# 2. åˆ—å‡ºæ‰€æœ‰buckets
docker exec minio mc ls myminio/

# 3. é€’å½’åˆ—å‡ºlakehouse bucketçš„å†…å®¹
docker exec minio mc ls --recursive myminio/lakehouse/bronze/social_media/

# 4. æŸ¥çœ‹å¯¹è±¡è¯¦æƒ…
docker exec minio mc stat myminio/lakehouse/bronze/social_media/partition_date=2025-11-12/source=reddit/part-00000.parquet

# 5. ä¸‹è½½æ–‡ä»¶åˆ°æœ¬åœ°
docker exec minio mc cp myminio/lakehouse/bronze/social_media/part-00000.parquet /tmp/

# 6. åˆ é™¤æ—§æ•°æ®
docker exec minio mc rm --recursive --force myminio/lakehouse/bronze/social_media/partition_date=2025-10-01/
```

#### MinIO Console Web UI

è®¿é—® http://localhost:9001ï¼š

```
Login:
  Username: minioadmin
  Password: minioadmin

ç•Œé¢:
â”œâ”€ Buckets
â”‚  â”œâ”€ lakehouse (ä¸»æ•°æ®)
â”‚  â”‚  â”œâ”€ bronze/
â”‚  â”‚  â”‚  â””â”€ social_media/
â”‚  â”‚  â”‚     â””â”€ partition_date=2025-11-12/
â”‚  â”‚  â”‚        â”œâ”€ source=reddit/
â”‚  â”‚  â”‚        â”‚  â””â”€ part-00000.parquet (2.1 MB)
â”‚  â”‚  â”‚        â””â”€ source=twitter/
â”‚  â”‚  â”œâ”€ checkpoints/
â”‚  â”‚  â”œâ”€ silver/  (æœªæ¥)
â”‚  â”‚  â””â”€ gold/    (æœªæ¥)
â”‚  â”œâ”€ bronze
â”‚  â”œâ”€ silver
â”‚  â””â”€ gold
â”‚
â”œâ”€ Object Browser (æµè§ˆæ–‡ä»¶)
â”œâ”€ Access Keys (è®¿é—®å¯†é’¥ç®¡ç†)
â”œâ”€ Monitoring (ç›‘æ§)
â””â”€ Settings (è®¾ç½®)
```

---

### Delta Lakeæ·±åº¦è§£æ

#### ä¸€å¥è¯ä»‹ç»
**Delta Lakeæ˜¯æ„å»ºåœ¨Parquetä¹‹ä¸Šçš„å¼€æºå­˜å‚¨å±‚ï¼Œæä¾›ACIDäº‹åŠ¡ã€æ—¶é—´æ—…è¡Œã€Schemaæ¼”åŒ–ç­‰ç‰¹æ€§ã€‚**

#### ä¸ºä»€ä¹ˆéœ€è¦Delta Lakeï¼Ÿ

**Parquetçš„é—®é¢˜**ï¼š

```
åœºæ™¯ï¼šä¿®æ”¹å·²å†™å…¥çš„æ•°æ®

Parquet:
  1. å†™å…¥ part-00000.parquet (1000è¡Œ)
  2. å‘ç°æœ‰é”™ï¼Œæƒ³åˆ é™¤ID=500çš„é‚£è¡Œ
  3. âŒ æ— æ³•ä¿®æ”¹ï¼Parquetæ˜¯ä¸å¯å˜çš„ï¼
  4. åªèƒ½ï¼šé‡å†™æ•´ä¸ªæ–‡ä»¶ï¼ˆä½æ•ˆï¼‰æˆ–æ–°å¢æ ‡è®°æ–‡ä»¶ï¼ˆå¤æ‚ï¼‰

Delta Lake:
  1. å†™å…¥ part-00000.parquet (1000è¡Œ)
  2. æ‰§è¡Œ DELETE WHERE id=500
  3. âœ… åªæ›´æ–°_delta_log/ï¼Œä¸é‡å†™æ•°æ®æ–‡ä»¶
  4. è¯»å–æ—¶è‡ªåŠ¨è¿‡æ»¤
```

**æ ¸å¿ƒä¼˜åŠ¿**ï¼š

| ç‰¹æ€§ | Parquet | Delta Lake |
|------|---------|------------|
| **ACIDäº‹åŠ¡** | âŒ | âœ… å¤šå†™å…¥è€…éš”ç¦» |
| **æ—¶é—´æ—…è¡Œ** | âŒ | âœ… æŸ¥è¯¢å†å²ç‰ˆæœ¬ |
| **Schemaæ¼”åŒ–** | âŒ éœ€è¦é‡å†™ | âœ… è‡ªåŠ¨åˆå¹¶ |
| **UPSERT** | âŒ | âœ… Merge Into |
| **DELETE/UPDATE** | âŒ | âœ… æ”¯æŒ |
| **æ–‡ä»¶å‹ç¼©** | æ‰‹åŠ¨ | âœ… è‡ªåŠ¨Optimize |
| **æ•°æ®æ ¡éªŒ** | âŒ | âœ… SchemaéªŒè¯ |

#### Delta Lakeå·¥ä½œåŸç†

**æ ¸å¿ƒï¼šTransaction Logï¼ˆäº‹åŠ¡æ—¥å¿—ï¼‰**

```
Delta Tableç›®å½•ç»“æ„:

delta_table/
â”œâ”€ _delta_log/
â”‚  â”œâ”€ 00000000000000000000.json  # Version 0
â”‚  â”œâ”€ 00000000000000000001.json  # Version 1
â”‚  â”œâ”€ 00000000000000000002.json  # Version 2
â”‚  â””â”€ ...
â”œâ”€ part-00000-xxx.snappy.parquet  # æ•°æ®æ–‡ä»¶
â”œâ”€ part-00001-xxx.snappy.parquet
â””â”€ ...
```

**äº‹åŠ¡æ—¥å¿—ç¤ºä¾‹**ï¼ˆ`00000000000000000000.json`ï¼‰ï¼š

```json
{
  "commitInfo": {
    "timestamp": 1699785600000,
    "operation": "WRITE",
    "operationParameters": {"mode": "Append"}
  }
}
{
  "add": {
    "path": "part-00000-xxx.snappy.parquet",
    "size": 2048576,
    "modificationTime": 1699785600000,
    "dataChange": true,
    "stats": "{\"numRecords\":1000,\"minValues\":{\"id\":1},\"maxValues\":{\"id\":1000}}"
  }
}
```

**DELETEæ“ä½œ**ï¼ˆ`00000000000000000001.json`ï¼‰ï¼š

```json
{
  "remove": {
    "path": "part-00000-xxx.snappy.parquet",
    "deletionTimestamp": 1699785700000
  }
}
{
  "add": {
    "path": "part-00001-yyy.snappy.parquet",  # æ–°æ–‡ä»¶ï¼Œä¸åŒ…å«id=500
    "size": 2040000,
    "dataChange": true
  }
}
```

**æ—¶é—´æ—…è¡Œ**ï¼š

```python
# æŸ¥è¯¢Version 0ï¼ˆç¬¬ä¸€æ¬¡å†™å…¥çš„æ•°æ®ï¼‰
df_v0 = spark.read.format("delta").option("versionAsOf", 0).load("s3a://lakehouse/silver/posts")

# æŸ¥è¯¢æ˜¨å¤©çš„æ•°æ®
df_yesterday = spark.read.format("delta").option("timestampAsOf", "2025-11-11").load("...")
```

#### é¡¹ç›®ä¸­çš„Delta Lakeä½¿ç”¨ï¼ˆæœªæ¥è®¡åˆ’ï¼‰

**å½“å‰çŠ¶æ€**ï¼šBronzeå±‚ä½¿ç”¨Parquetï¼ˆç®€å•ï¼‰

**Phase 3è®¡åˆ’**ï¼šè¿ç§»åˆ°Delta Lake

```python
# Bronze â†’ Silverè½¬æ¢ï¼ˆæ¯å°æ—¶è¿è¡Œï¼‰
from delta.tables import DeltaTable

# è¯»å–Bronzeï¼ˆParquetï¼‰
bronze_df = spark.read.parquet("s3a://lakehouse/bronze/social_media/partition_date=2025-11-12/")

# æ¸…æ´—æ•°æ®
silver_df = (
    bronze_df
    .dropDuplicates(["post_id"])  # å»é‡
    .filter(col("text").isNotNull())  # è¿‡æ»¤ç©ºå€¼
    .withColumn("text_length", length(col("text")))
    .withColumn("keywords", extract_keywords_udf(col("text")))  # NLPå¤„ç†
)

# å†™å…¥Silverï¼ˆDeltaï¼‰
(silver_df
 .write
 .format("delta")
 .mode("append")
 .partitionBy("partition_date", "source")
 .save("s3a://lakehouse/silver/posts"))

# æ›´æ–°æ“ä½œï¼ˆUPSERTï¼‰
deltaTable = DeltaTable.forPath(spark, "s3a://lakehouse/silver/posts")

deltaTable.alias("target").merge(
    new_data.alias("source"),
    "target.post_id = source.post_id"
).whenMatchedUpdateAll(  # åŒ¹é…æ—¶æ›´æ–°
).whenNotMatchedInsertAll(  # ä¸åŒ¹é…æ—¶æ’å…¥
).execute()
```

**ä¼˜åŒ–æ“ä½œ**ï¼š

```python
# Optimize: åˆå¹¶å°æ–‡ä»¶
deltaTable.optimize().executeCompaction()

# Z-Order: æŒ‰å¸¸ç”¨æŸ¥è¯¢åˆ—æ’åºï¼ŒåŠ é€ŸæŸ¥è¯¢
deltaTable.optimize().executeZOrderBy("post_id", "created_at")

# Vacuum: æ¸…ç†æ—§ç‰ˆæœ¬æ–‡ä»¶ï¼ˆä¿ç•™7å¤©ï¼‰
deltaTable.vacuum(retentionHours=168)
```

---

## 2.5 å¯è§†åŒ–å±‚ (Visualization Layer)

### Streamlit Dashboardæ·±åº¦è§£æ

#### ä¸€å¥è¯ä»‹ç»
**Streamlitæ˜¯ä¸€ä¸ªPythonæ¡†æ¶ï¼Œå¯ä»¥å¿«é€Ÿæ„å»ºäº¤äº’å¼æ•°æ®åº”ç”¨å’ŒDashboardï¼Œæ— éœ€å‰ç«¯å¼€å‘ç»éªŒã€‚**

#### ä¸ºä»€ä¹ˆé€‰æ‹©Streamlitï¼Ÿ

**å¯¹æ¯”å…¶ä»–å¯è§†åŒ–å·¥å…·**ï¼š

| å·¥å…· | å¼€å‘é€Ÿåº¦ | å­¦ä¹ æ›²çº¿ | äº¤äº’æ€§ | é€‚ç”¨åœºæ™¯ |
|------|---------|---------|--------|----------|
| **Streamlit** | âœ… æœ€å¿«ï¼ˆçº¯Pythonï¼‰ | â­ ç®€å• | âœ… ä¸°å¯Œ | æ•°æ®åŸå‹ã€å†…éƒ¨å·¥å…· |
| **Dashï¼ˆPlotlyï¼‰** | ä¸­ç­‰ | â­â­ ä¸­ç­‰ | âœ… ä¸°å¯Œ | ä¼ä¸šDashboard |
| **Gradio** | å¿« | â­ ç®€å• | âš ï¸ æœ‰é™ | MLæ¨¡å‹æ¼”ç¤º |
| **Flask + React** | æ…¢ | â­â­â­â­ é«˜ | âœ… å®Œå…¨è‡ªå®šä¹‰ | ç”Ÿäº§çº§åº”ç”¨ |
| **Grafana** | é…ç½®å‹ | â­â­ ä¸­ç­‰ | âš ï¸ é¢„å®šä¹‰ | ç›‘æ§æŒ‡æ ‡ |
| **Tableau** | æ‹–æ‹½å¼ | â­â­ ä¸­ç­‰ | âœ… ä¸°å¯Œ | å•†ä¸šæ™ºèƒ½ |

**æˆ‘ä»¬é€‰æ‹©Streamlitçš„åŸå› **ï¼š
- âœ… **çº¯Pythonå¼€å‘**ï¼šæ•°æ®å·¥ç¨‹å¸ˆç†Ÿæ‚‰çš„è¯­è¨€
- âœ… **å¿«é€ŸåŸå‹**ï¼šå‡ å°æ—¶å°±èƒ½æ­å»ºDashboard
- âœ… **è‡ªåŠ¨åˆ·æ–°**ï¼šæ”¯æŒå®æ—¶æ•°æ®æµ
- âœ… **ä¸°å¯Œç»„ä»¶**ï¼šå›¾è¡¨ã€è¡¨æ ¼ã€è¿‡æ»¤å™¨å¼€ç®±å³ç”¨
- âœ… **æ˜“äºéƒ¨ç½²**ï¼šå¯ä»¥ç›´æ¥éƒ¨ç½²åˆ°Streamlit Cloud

#### Streamlitæ ¸å¿ƒæ¦‚å¿µ

##### 1. å“åº”å¼ç¼–ç¨‹æ¨¡å‹ï¼ˆReactive Programmingï¼‰

**ç™½è¯è§£é‡Š**ï¼š

ä¼ ç»ŸWebå¼€å‘ï¼š
```python
# Flask/Django: éœ€è¦å®šä¹‰è·¯ç”±å’Œæ¨¡æ¿
@app.route('/')
def index():
    data = get_data()
    return render_template('index.html', data=data)
```

Streamlitçš„é­”æ³•ï¼š
```python
# Streamlit: åƒå†™è„šæœ¬ä¸€æ ·
import streamlit as st

st.title("Dashboard")
data = get_data()  # æ¯æ¬¡ç”¨æˆ·äº¤äº’ï¼Œæ•´ä¸ªè„šæœ¬é‡æ–°è¿è¡Œï¼
st.write(data)
```

**å…³é”®åŸç†**ï¼š
- ç”¨æˆ·æ¯æ¬¡äº¤äº’ï¼ˆç‚¹å‡»ã€è¾“å…¥ï¼‰ï¼ŒStreamlitä¼šä»å¤´åˆ°å°¾é‡æ–°è¿è¡Œæ•´ä¸ªè„šæœ¬
- ç»„ä»¶çš„çŠ¶æ€é€šè¿‡`st.session_state`ä¿å­˜
- ä½¿ç”¨`@st.cache_data`ç¼“å­˜æ˜‚è´µçš„è®¡ç®—

##### 2. Session Stateï¼ˆä¼šè¯çŠ¶æ€ï¼‰

```python
# åˆå§‹åŒ–session state
if 'counter' not in st.session_state:
    st.session_state.counter = 0

# ä½¿ç”¨
if st.button("Increment"):
    st.session_state.counter += 1

st.write(f"Counter: {st.session_state.counter}")
```

**ç”¨é€”**ï¼š
- ä¿å­˜ç”¨æˆ·é€‰æ‹©ï¼ˆç­›é€‰æ¡ä»¶ã€é¡µç ï¼‰
- è®°å½•å†å²æ•°æ®ï¼ˆä¸Šæ¬¡åˆ·æ–°æ—¶é—´ã€æ•°æ®å¢é‡ï¼‰
- è·¨ç»„ä»¶é€šä¿¡

##### 3. Cachingï¼ˆç¼“å­˜æœºåˆ¶ï¼‰

```python
@st.cache_data(ttl=300)  # ç¼“å­˜5åˆ†é’Ÿ
def load_expensive_data():
    # è¿™ä¸ªå‡½æ•°åªåœ¨ç¼“å­˜å¤±æ•ˆæ—¶è¿è¡Œ
    data = expensive_operation()
    return data
```

**ä¸¤ç§ç¼“å­˜è£…é¥°å™¨**ï¼š

| è£…é¥°å™¨ | ç”¨é€” | ä»€ä¹ˆæ—¶å€™ç”¨ |
|--------|------|------------|
| `@st.cache_data` | ç¼“å­˜æ•°æ®ï¼ˆDataFrame, Listï¼‰ | æ•°æ®åŠ è½½ã€APIè°ƒç”¨ |
| `@st.cache_resource` | ç¼“å­˜å¯¹è±¡ï¼ˆæ•°æ®åº“è¿æ¥ã€æ¨¡å‹ï¼‰ | å•ä¾‹èµ„æº |

#### é¡¹ç›®ä¸­çš„Dashboardæ¶æ„

**æ–‡ä»¶**ï¼š`dashboard/app_realtime.py`

**æ ¸å¿ƒåŠŸèƒ½**ï¼š

```
Dashboardæ¶æ„:
â”œâ”€ é¡¶éƒ¨çŠ¶æ€æ 
â”‚  â”œâ”€ å®æ—¶æ—¶é’Ÿï¼ˆæ˜¾ç¤ºå½“å‰æ—¶é—´ï¼‰
â”‚  â”œâ”€ å€’è®¡æ—¶ï¼ˆä¸‹æ¬¡åˆ·æ–°å€’è®¡æ—¶ï¼‰
â”‚  â””â”€ æ‰‹åŠ¨åˆ·æ–°æŒ‰é’®
â”‚
â”œâ”€ æ ¸å¿ƒæŒ‡æ ‡å¡ç‰‡
â”‚  â”œâ”€ æ€»å¸–å­æ•°ï¼ˆTotal Postsï¼‰
â”‚  â”œâ”€ Twitterå¸–å­æ•°
â”‚  â”œâ”€ Redditå¸–å­æ•°
â”‚  â””â”€ æ€»äº’åŠ¨æ•°
â”‚
â”œâ”€ ä¸‰ä¸ªTabé¡µ
â”‚  â”œâ”€ ğŸ“Š Overview: æ•°æ®æºåˆ†å¸ƒé¥¼å›¾
â”‚  â”œâ”€ ğŸ”¥ Trending Keywords: å…³é”®è¯è¯äº‘ + æŸ±çŠ¶å›¾
â”‚  â””â”€ ğŸ“ Recent Posts: Reddité£æ ¼å¸–å­åˆ—è¡¨
â”‚
â”œâ”€ ä¾§è¾¹æ 
â”‚  â”œâ”€ è‡ªåŠ¨åˆ·æ–°å¼€å…³
â”‚  â”œâ”€ Kafkaè¿æ¥çŠ¶æ€
â”‚  â””â”€ é‡‡é›†å™¨çŠ¶æ€
â”‚
â””â”€ è‡ªåŠ¨åˆ·æ–°æœºåˆ¶ï¼ˆ60ç§’å€’è®¡æ—¶ï¼‰
```

#### å…³é”®ä»£ç è§£æ

##### 1. æ•°æ®åŠ è½½ï¼ˆå¸¦ç¼“å­˜ï¼‰

```python
@st.cache_data(ttl=300)  # ç¼“å­˜5åˆ†é’Ÿ
def load_real_data():
    """ä»KafkaåŠ è½½å®æ—¶æ•°æ®"""
    if not KAFKA_AVAILABLE:
        return None, 0

    try:
        reader = KafkaDataReader(
            bootstrap_servers='localhost:9092',
            topic='ai-social-raw'
        )

        # è·å–æ€»æ¶ˆæ¯æ•°ï¼ˆä¸è¯»å–å†…å®¹ï¼Œåªè®¡æ•°ï¼‰
        total_count = reader.get_message_count()

        # è·å–æ‰€æœ‰æ¶ˆæ¯
        messages = reader.get_all_messages()

        if not messages:
            return None, total_count

        # è§£æä¸ºDataFrame
        df = reader.parse_to_dataframe(messages)

        return df, total_count

    except Exception as e:
        st.error(f"Error loading data: {e}")
        return None, 0
```

**é‡ç‚¹**ï¼š
- `ttl=300`ï¼šç¼“å­˜5åˆ†é’Ÿï¼Œé¿å…é¢‘ç¹è¯»å–Kafka
- åˆ†ä¸¤æ­¥è·å–ï¼šå…ˆç»Ÿè®¡æ•°é‡ï¼ˆå¿«ï¼‰ï¼Œå†è¯»å–å†…å®¹ï¼ˆæ…¢ï¼‰
- å¼‚å¸¸å¤„ç†ï¼šKafkaä¸å¯ç”¨æ—¶ä¼˜é›…é™çº§

##### 2. è‡ªåŠ¨åˆ·æ–°æœºåˆ¶ï¼ˆçœŸæ­£çš„å€’è®¡æ—¶ï¼‰

```python
# åˆ›å»ºå€’è®¡æ—¶å ä½ç¬¦
countdown_placeholder = col_countdown.empty()
progress_placeholder = col_countdown.empty()

# æ˜¾ç¤ºä¸»è¦å†…å®¹...
# ...

# === å€’è®¡æ—¶å¾ªç¯ï¼ˆ60ç§’çœŸå®å€’è®¡æ—¶ï¼‰ ===
if auto_refresh:
    for remaining in range(60, 0, -1):
        # æ ¼å¼åŒ–æ˜¾ç¤ºæ—¶é—´
        time_str = f"{remaining}ç§’"

        # åŠ¨æ€æ›´æ–°å€’è®¡æ—¶
        with countdown_placeholder:
            st.markdown(f'<div class="countdown">â³ <b>ä¸‹æ¬¡åˆ·æ–°</b>: {time_str}</div>',
                       unsafe_allow_html=True)

        # åŠ¨æ€æ›´æ–°è¿›åº¦æ¡
        with progress_placeholder:
            progress = (60 - remaining) / 60
            st.progress(progress)

        time.sleep(1)  # ç­‰å¾…1ç§’

    # 60ç§’åè‡ªåŠ¨åˆ·æ–°
    st.rerun()  # é‡æ–°è¿è¡Œæ•´ä¸ªè„šæœ¬
```

**å…³é”®æŠ€å·§**ï¼š
- `empty()`: åˆ›å»ºå ä½ç¬¦ï¼Œå¯ä»¥åç»­åŠ¨æ€æ›´æ–°
- `time.sleep(1)`: çœŸæ­£ç­‰å¾…1ç§’ï¼ˆä¸æ˜¯æ¨¡æ‹Ÿï¼‰
- `st.rerun()`: åˆ·æ–°æ•´ä¸ªé¡µé¢ï¼Œé‡æ–°åŠ è½½æ•°æ®

##### 3. Reddité£æ ¼å¡ç‰‡æ¸²æŸ“

```python
def render_reddit_card(row):
    """æ¸²æŸ“Reddité£æ ¼å¡ç‰‡"""
    source = row.get('source', 'Unknown')
    author = row.get('author', 'Unknown')
    text = str(row.get('text', ''))[:200]
    engagement = row.get('engagement', 0)
    subreddit = row.get('subreddit', '')
    created_at = row.get('created_at', '')

    # æ ¹æ®æ¥æºè®¾ç½®badgeæ ·å¼
    badge_class = {
        'Reddit': 'badge-reddit',
        'Bluesky': 'badge-bluesky',
        'Twitter': 'badge-twitter'
    }.get(source, 'badge-reddit')

    # æ ¼å¼åŒ–æ—¶é—´æˆ³ä¸ºç›¸å¯¹æ—¶é—´
    time_display = format_time_ago(created_at)

    # HTMLå¡ç‰‡
    card_html = f"""
    <div class="reddit-card">
        <div class="card-header">
            <span class="source-badge {badge_class}">{source}</span>
            <span class="card-meta">
                {'r/' + subreddit if subreddit else ''} by u/{author} â€¢ {time_display}
            </span>
        </div>
        <div class="card-title">{text}...</div>
        <div class="card-footer">
            <span>ğŸ‘ {engagement:,}</span>
            <span>ğŸ’¬ è¯„è®º</span>
            <span>ğŸ”— åˆ†äº«</span>
        </div>
    </div>
    """

    return card_html
```

**è®¾è®¡ç‰¹ç‚¹**ï¼š
- ä½¿ç”¨HTML + CSSæ¸²æŸ“è‡ªå®šä¹‰æ ·å¼
- æ”¯æŒå¤šç§æ•°æ®æºï¼ˆRedditã€Twitterã€Blueskyï¼‰
- ç›¸å¯¹æ—¶é—´æ˜¾ç¤ºï¼ˆ"2å°æ—¶å‰"ï¼‰

##### 4. ç­›é€‰å’Œæ’åºåŠŸèƒ½

```python
# === ç­›é€‰å™¨æ§åˆ¶ ===
col_filter1, col_filter2, col_filter3 = st.columns(3)

with col_filter1:
    date_filter = st.selectbox(
        "ğŸ“… æ—¶é—´èŒƒå›´",
        ["æ‰€æœ‰", "ä»Šå¤©", "æ˜¨å¤©", "æœ¬å‘¨", "æœ¬æœˆ"],
        index=0
    )

with col_filter2:
    source_filter = st.selectbox(
        "ğŸ“¡ æ¥æº",
        ["æ‰€æœ‰"] + list(df['source'].unique()),
        index=0
    )

with col_filter3:
    sort_by = st.selectbox(
        "ğŸ“Š æ’åº",
        ["æœ€æ–°", "æœ€çƒ­", "å‚ä¸åº¦æœ€é«˜"],
        index=0
    )

# === åº”ç”¨ç­›é€‰æ¡ä»¶ ===
filtered_df = df.copy()

# æ—¥æœŸç­›é€‰
if date_filter == "ä»Šå¤©":
    filtered_df = filtered_df[
        filtered_df['created_at_parsed'].dt.date == datetime.now().date()
    ]

# æ¥æºç­›é€‰
if source_filter != "æ‰€æœ‰":
    filtered_df = filtered_df[filtered_df['source'] == source_filter]

# æ’åº
if sort_by == "æœ€æ–°":
    filtered_df = filtered_df.sort_values('created_at', ascending=False)
elif sort_by == "å‚ä¸åº¦æœ€é«˜":
    filtered_df = filtered_df.sort_values('engagement', ascending=False)
```

---

### NLPå…³é”®è¯æå–æ·±åº¦è§£æ

#### ä¸€å¥è¯ä»‹ç»
**ä½¿ç”¨spaCyå’ŒNLTKå¯¹ç¤¾äº¤åª’ä½“æ–‡æœ¬è¿›è¡Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼Œæå–æœ‰æ„ä¹‰çš„AIç›¸å…³æ¦‚å¿µå’ŒçŸ­è¯­ã€‚**

#### ä¸ºä»€ä¹ˆéœ€è¦NLPï¼Ÿ

**é—®é¢˜**ï¼šåŸå§‹æ–‡æœ¬æ•°æ®å¤ªæ‚ä¹±

```
åŸå§‹Redditå¸–å­:
"I've been experimenting with local LLMs on my M1 Mac using llama.cpp and it's
amazing how well they run! Anyone else trying Mistral 7B? The quality is impressive
for the size..."

æˆ‘ä»¬æƒ³è¦ï¼š
- local LLMsï¼ˆæœ¬åœ°å¤§è¯­è¨€æ¨¡å‹ï¼‰
- llama.cppï¼ˆå·¥å…·åï¼‰
- Mistral 7Bï¼ˆæ¨¡å‹åï¼‰
- M1 Macï¼ˆç¡¬ä»¶ï¼‰

è€Œä¸æ˜¯ï¼š
- the, it, is, forï¼ˆåœç”¨è¯ï¼‰
- anyone, trying, qualityï¼ˆé€šç”¨è¯ï¼‰
```

#### NLPæŠ€æœ¯æ ˆé€‰æ‹©

| åº“ | ç”¨é€” | ä¼˜åŠ¿ |
|---|------|------|
| **spaCy** | æ ¸å¿ƒNLPå¤„ç† | å¿«é€Ÿã€å‡†ç¡®çš„è¯æ€§æ ‡æ³¨å’Œå®ä½“è¯†åˆ« |
| **NLTK** | åœç”¨è¯åº“ | ä¸°å¯Œçš„è¯­è¨€èµ„æº |
| **WordCloud** | è¯äº‘ç”Ÿæˆ | ç¾è§‚çš„å¯è§†åŒ– |

#### å…³é”®è¯æå–ç®—æ³•

**æ–‡ä»¶**ï¼š`dashboard/app_realtime.py:285-367`

**æ ¸å¿ƒæµç¨‹**ï¼š

```
è¾“å…¥: æ‰€æœ‰å¸–å­çš„æ–‡æœ¬
  â†“
1. åˆå¹¶æ–‡æœ¬ + æ¸…ç†
  - ç§»é™¤URL
  - ç§»é™¤æ ‡ç‚¹
  â†“
2. spaCy NLPå¤„ç†
  - è¯æ€§æ ‡æ³¨ (POS tagging)
  - å‘½åå®ä½“è¯†åˆ« (NER)
  - åè¯çŸ­è¯­æå– (noun chunks)
  â†“
3. å¤šé‡è¿‡æ»¤è§„åˆ™
  - é•¿åº¦ï¼š2-4ä¸ªè¯ï¼ˆè¦çŸ­è¯­ï¼Œä¸è¦å•è¯ï¼‰
  - ä¸æ˜¯åœç”¨è¯
  - ä¸ä»¥é€šç”¨è¯å¼€å¤´/ç»“å°¾
  - ä¸åŒ…å«æ•°å­—
  - ä¸æ˜¯é€šç”¨AIè¯ï¼ˆå¦‚"machine learning"ï¼‰
  â†“
4. ç»Ÿè®¡é¢‘ç‡
  - è®¡æ•°å‡ºç°æ¬¡æ•°
  - å–Top 20
  â†“
è¾“å‡º: [(å…³é”®è¯, é¢‘æ¬¡), ...]
```

##### å…³é”®ä»£ç ï¼š

```python
def extract_real_keywords(df, top_n=20):
    """ä½¿ç”¨NLPæå–çœŸå®çš„AIæ¦‚å¿µçŸ­è¯­"""
    if df is None or df.empty or not NLP_AVAILABLE:
        return pd.DataFrame()

    try:
        # 1. åˆå¹¶æ‰€æœ‰æ–‡æœ¬
        all_text = ' '.join(df['text'].astype(str).tolist())

        # 2. æ¸…ç†æ–‡æœ¬
        all_text = re.sub(r'http\S+|www\S+|https\S+', '', all_text)  # ç§»é™¤URL
        all_text = re.sub(r'[^\w\s]', ' ', all_text)  # ç§»é™¤æ ‡ç‚¹

        # 3. spaCyå¤„ç†ï¼ˆé™åˆ¶é•¿åº¦é¿å…è¶…æ—¶ï¼‰
        doc = nlp(all_text[:200000])

        # 4. é€šç”¨è¯é»‘åå•
        GENERIC_WORDS_BLACKLIST = {
            'data', 'image', 'model', 'tool', 'system', 'code', 'language',
            'result', 'problem', 'example', 'project', 'paper', 'test',
            # ... æ›´å¤šé€šç”¨è¯
        }

        # 5. æå–åè¯çŸ­è¯­ï¼ˆnoun chunksï¼‰
        phrases = []
        for chunk in doc.noun_chunks:
            phrase = chunk.text.lower().strip()
            words = phrase.split()
            num_words = len(words)

            # è¿‡æ»¤è§„åˆ™ï¼š
            if (2 <= num_words <= 4  # å¤šè¯çŸ­è¯­
                and all(w not in STOP_WORDS for w in words)  # ä¸æ˜¯åœç”¨è¯
                and words[0] not in GENERIC_WORDS_BLACKLIST  # é¦–è¯ä¸æ˜¯é€šç”¨è¯
                and words[-1] not in GENERIC_WORDS_BLACKLIST  # å°¾è¯ä¸æ˜¯é€šç”¨è¯
                and not any(w.isdigit() for w in words)  # ä¸åŒ…å«æ•°å­—
                and phrase not in ['artificial intelligence', 'machine learning']  # è¿‡æ»¤é€šç”¨AIè¯
                and len(phrase) >= 8  # æ€»å­—ç¬¦æ•°è‡³å°‘8
                ):
                phrases.append(phrase)

        # 6. ç»Ÿè®¡é¢‘ç‡
        phrase_counts = Counter(phrases).most_common(top_n)

        return pd.DataFrame(phrase_counts, columns=['keyword', 'mentions'])

    except Exception as e:
        print(f"âŒ å…³é”®è¯æå–é”™è¯¯: {e}")
        return pd.DataFrame()
```

**ä¸ºä»€ä¹ˆè¦è¿™ä¹ˆå¤æ‚çš„è¿‡æ»¤ï¼Ÿ**

```
å®é™…è¾“å‡ºå¯¹æ¯”ï¼š

âŒ æ²¡æœ‰è¿‡æ»¤ï¼š
- the model
- the data
- the result
- a lot
- this is
ï¼ˆå…¨æ˜¯åºŸè¯ï¼ï¼‰

âœ… æœ‰è¿‡æ»¤ï¼š
- local llama
- mistral model
- quantization method
- inference speed
- fine tuning
ï¼ˆçœŸæ­£çš„æŠ€æœ¯æ¦‚å¿µï¼ï¼‰
```

#### è¯äº‘ç”Ÿæˆ

```python
def create_word_cloud(df):
    """ç”Ÿæˆè¯äº‘"""
    keywords_df = extract_real_keywords(df, top_n=100)

    if keywords_df.empty:
        st.warning("æ— æ³•ç”Ÿæˆè¯äº‘ï¼šæ²¡æœ‰æå–åˆ°å…³é”®è¯")
        return

    # æ¸…ç†å…³é”®è¯æ–‡æœ¬ï¼šç§»é™¤æ¢è¡Œç¬¦
    clean_keywords = {}
    for keyword, count in zip(keywords_df['keyword'], keywords_df['mentions']):
        clean_keyword = ' '.join(str(keyword).split())
        clean_keywords[clean_keyword] = count

    wordcloud = WordCloud(
        width=800,
        height=400,
        background_color='white',
        colormap='Blues',
        max_words=50,
        relative_scaling=0.5,
        min_font_size=12,
        collocations=False,
        prefer_horizontal=0.7  # ä¼˜å…ˆæ°´å¹³æ˜¾ç¤º
    ).generate_from_frequencies(clean_keywords)

    fig, ax = plt.subplots(figsize=(10, 5))
    ax.imshow(wordcloud, interpolation='bilinear')
    ax.axis('off')
    st.pyplot(fig)
```

**WordCloudå‚æ•°è§£é‡Š**ï¼š

| å‚æ•° | å«ä¹‰ | æˆ‘ä»¬çš„é€‰æ‹© |
|------|------|------------|
| `max_words` | æœ€å¤šæ˜¾ç¤ºå•è¯æ•° | 50ï¼ˆé¿å…æ‹¥æŒ¤ï¼‰ |
| `relative_scaling` | é¢‘æ¬¡å¯¹å­—å·çš„å½±å“ | 0.5ï¼ˆé«˜é¢‘è¯ä¸è¦å¤ªå¤§ï¼‰ |
| `colormap` | é¢œè‰²æ–¹æ¡ˆ | 'Blues'ï¼ˆä¸“ä¸šé£æ ¼ï¼‰ |
| `collocations` | æ˜¯å¦æ£€æµ‹æ­é… | Falseï¼ˆæˆ‘ä»¬å·²ç»æå–çŸ­è¯­ï¼‰ |
| `prefer_horizontal` | æ°´å¹³æ˜¾ç¤ºä¼˜å…ˆ | 0.7ï¼ˆ70%æ°´å¹³ï¼Œæ˜“è¯»ï¼‰ |

---

### Kafka Readeræ·±åº¦è§£æ

#### ä¸€å¥è¯ä»‹ç»
**Dashboardé€šè¿‡KafkaDataReaderç›´æ¥è¯»å–Kafka topicï¼Œå®ç°å®æ—¶æ•°æ®å±•ç¤ºã€‚**

#### ä¸ºä»€ä¹ˆDashboardç›´æ¥è¯»Kafkaï¼Ÿ

**æ¶æ„å¯¹æ¯”**ï¼š

```
æ–¹æ¡ˆA: Dashboard â†’ MinIO (Bronze)
  âœ… æ•°æ®å®Œæ•´
  âŒ å»¶è¿Ÿé«˜ï¼ˆç­‰Sparkå†™å…¥ï¼‰
  âŒ éœ€è¦æŸ¥è¯¢MinIO

æ–¹æ¡ˆB: Dashboard â†’ Kafka (æˆ‘ä»¬çš„é€‰æ‹©)
  âœ… å»¶è¿Ÿä½ï¼ˆ< 1ç§’ï¼‰
  âœ… ç®€å•ç›´æ¥
  âŒ åªèƒ½çœ‹æœ€è¿‘æ•°æ®ï¼ˆ7å¤©ï¼‰
```

**æˆ‘ä»¬çš„ç­–ç•¥**ï¼š
- **å®æ—¶ç›‘æ§**ï¼šDashboardè¯»Kafkaï¼ˆå¿«ï¼‰
- **å†å²åˆ†æ**ï¼šåˆ†æå·¥å…·è¯»MinIOï¼ˆå®Œæ•´ï¼‰

#### KafkaDataReaderæ ¸å¿ƒåŠŸèƒ½

**æ–‡ä»¶**ï¼š`dashboard/kafka_reader.py`

##### 1. è·å–æ¶ˆæ¯æ€»æ•°ï¼ˆä¸è¯»å–å†…å®¹ï¼‰

```python
def get_message_count(self) -> int:
    """è·å–Kafka topicçš„æ€»æ¶ˆæ¯æ•°ï¼ˆä¸è¯»å–å†…å®¹ï¼Œåªè®¡æ•°ï¼‰"""
    try:
        consumer = KafkaConsumer(
            bootstrap_servers=self.bootstrap_servers,
            group_id='dashboard-counter'
        )

        # è·å–topicçš„æ‰€æœ‰åˆ†åŒº
        partitions = consumer.partitions_for_topic(self.topic)
        topic_partitions = [TopicPartition(self.topic, p) for p in partitions]
        consumer.assign(topic_partitions)

        # ç§»åŠ¨åˆ°æ¯ä¸ªåˆ†åŒºçš„æœ«å°¾
        consumer.seek_to_end()

        # è®¡ç®—æ€»æ¶ˆæ¯æ•°
        total = sum(consumer.position(tp) for tp in topic_partitions)
        consumer.close()

        return total

    except Exception as e:
        print(f"Error counting messages: {e}")
        return 0
```

**å…³é”®æŠ€å·§**ï¼š
- `seek_to_end()`: ç›´æ¥è·³åˆ°æœ«å°¾
- `position()`: è·å–offsetä½ç½®
- ä¸è¯»å–æ¶ˆæ¯å†…å®¹ï¼Œé€Ÿåº¦æå¿«

##### 2. è¯»å–æ‰€æœ‰æ¶ˆæ¯

```python
def get_all_messages(self) -> List[Dict[str, Any]]:
    """è¯»å–æ‰€æœ‰å¯ç”¨æ¶ˆæ¯"""
    messages = []

    try:
        consumer = KafkaConsumer(
            self.topic,
            bootstrap_servers=self.bootstrap_servers,
            auto_offset_reset='earliest',  # ä»å¤´å¼€å§‹
            enable_auto_commit=False,
            consumer_timeout_ms=10000,  # 10ç§’è¶…æ—¶
            group_id='dashboard-viewer',
            value_deserializer=lambda m: json.loads(m.decode('utf-8'))
        )

        # è¯»å–å…¨éƒ¨æ•°æ®ç›´åˆ°è¶…æ—¶
        for message in consumer:
            messages.append(message.value)

        consumer.close()

    except Exception as e:
        print(f"Error reading messages: {e}")
        return []

    return messages
```

**é‡è¦é…ç½®**ï¼š
- `auto_offset_reset='earliest'`: ä»å¤´è¯»ï¼ˆè·å–å…¨éƒ¨å†å²ï¼‰
- `consumer_timeout_ms=10000`: 10ç§’æ²¡æœ‰æ–°æ¶ˆæ¯å°±åœæ­¢
- `enable_auto_commit=False`: ä¸æäº¤offsetï¼ˆåªè¯»ä¸æ”¹ï¼‰

##### 3. è§£æä¸ºDataFrame

```python
def parse_to_dataframe(self, messages: List[Dict[str, Any]]) -> pd.DataFrame:
    """å°†Kafkaæ¶ˆæ¯è§£æä¸ºDataFrame"""
    parsed_data = []

    for msg in messages:
        source = msg.get('source', 'unknown')
        data = msg.get('data', {})

        if source == 'reddit':
            # æ¸…ç†HTMLæ ‡ç­¾å’Œè½¬ä¹‰å­—ç¬¦
            title_raw = data.get('title', '')
            text_raw = data.get('text', '')

            # 1. è§£ç HTMLå®ä½“ï¼ˆå¦‚&amp; &lt;ï¼‰
            title_clean = html.unescape(title_raw)
            text_clean = html.unescape(text_raw)

            # 2. ç§»é™¤HTMLæ ‡ç­¾
            title_clean = re.sub(r'<[^>]+>', '', title_clean)
            text_clean = re.sub(r'<[^>]+>', '', text_clean)

            # 3. ç§»é™¤å¤šä½™ç©ºç™½
            title_clean = ' '.join(title_clean.split())
            text_clean = ' '.join(text_clean.split())

            parsed_data.append({
                'source': 'Reddit',
                'post_id': data.get('id'),
                'text': f"{title_clean} {text_clean}",
                'author': data.get('author', 'Unknown'),
                'created_at': data.get('created_utc'),
                'engagement': data.get('metrics', {}).get('score', 0),
                'subreddit': data.get('subreddit', ''),
            })

        elif source == 'twitter':
            # Twitteræ•°æ®è§£æ...
            pass

    return pd.DataFrame(parsed_data)
```

**æ•°æ®æ¸…æ´—é‡ç‚¹**ï¼š
- HTMLå®ä½“è§£ç ï¼š`&amp;` â†’ `&`
- HTMLæ ‡ç­¾ç§»é™¤ï¼š`<div>text</div>` â†’ `text`
- ç©ºç™½è§„èŒƒåŒ–ï¼šå¤šä¸ªç©ºæ ¼ â†’ å•ä¸ªç©ºæ ¼

#### Dashboardæ€§èƒ½ä¼˜åŒ–

**é—®é¢˜**ï¼šDashboardæ¯æ¬¡åˆ·æ–°éƒ½è¦è¯»Kafkaï¼Œå¾ˆæ…¢

**è§£å†³æ–¹æ¡ˆ**ï¼š

```python
# 1. ç¼“å­˜æ•°æ®åŠ è½½ï¼ˆ5åˆ†é’Ÿï¼‰
@st.cache_data(ttl=300)
def load_real_data():
    # 5åˆ†é’Ÿå†…é‡å¤è®¿é—®ç›´æ¥ç”¨ç¼“å­˜
    pass

# 2. åªè¯»æœ€è¿‘æ•°æ®ï¼ˆéå…¨éƒ¨ï¼‰
reader.get_recent_messages(num_messages=100)  # åªè¯»100æ¡

# 3. å¼‚æ­¥åŠ è½½
with st.spinner("ğŸ“Š Loading..."):  # æ˜¾ç¤ºåŠ è½½åŠ¨ç”»
    df = load_real_data()
```

---

**æ¥ä¸‹æ¥**ï¼š[Part 3: æ¶æ„å†³ç­–æ·±åº¦å‰–æ](#part-3-æ¶æ„å†³ç­–æ·±åº¦å‰–æ) å°†å¯¹æ¯”ä¸åŒæŠ€æœ¯é€‰å‹ï¼Œæ·±å…¥åˆ†æä¸ºä»€ä¹ˆé€‰æ‹©å½“å‰æ–¹æ¡ˆã€‚

---

# Part 3: æ¶æ„å†³ç­–æ·±åº¦å‰–æ

> æ·±å…¥å¯¹æ¯”ä¸åŒæŠ€æœ¯é€‰å‹ï¼Œè§£é‡Šä¸ºä»€ä¹ˆé€‰æ‹©å½“å‰æ–¹æ¡ˆ

---

## 3.1 æ¶ˆæ¯é˜Ÿåˆ—é€‰å‹ï¼šKafka vs å…¶ä»–æ–¹æ¡ˆ

### å€™é€‰æ–¹æ¡ˆå¯¹æ¯”

| ç‰¹æ€§ | **Kafka** | RabbitMQ | Redis Streams | Apache Pulsar |
|------|-----------|----------|---------------|---------------|
| **ååé‡** | âœ… 10M+ msg/s | 50K msg/s | 1M+ msg/s | âœ… 10M+ msg/s |
| **å»¶è¿Ÿ** | ä¸­ï¼ˆmsçº§ï¼‰ | âœ… ä½ï¼ˆÎ¼sçº§ï¼‰ | âœ… ä½ï¼ˆÎ¼sçº§ï¼‰ | ä¸­ï¼ˆmsçº§ï¼‰ |
| **æŒä¹…åŒ–** | âœ… ç£ç›˜ï¼ˆå¯é‡æ”¾ï¼‰ | å†…å­˜+ç£ç›˜ | âŒ æœ‰é™ï¼ˆå†…å­˜ï¼‰ | âœ… åˆ†å±‚å­˜å‚¨ |
| **æ¶ˆè´¹æ¨¡å‹** | âœ… Pullï¼ˆæ¶ˆè´¹è€…æ§åˆ¶ï¼‰ | Push | Pull | ä¸¤è€…éƒ½æ”¯æŒ |
| **æµå¤„ç†é›†æˆ** | âœ… å®Œç¾ï¼ˆSpark/Flinkï¼‰ | âš ï¸ æœ‰é™ | âš ï¸ æœ‰é™ | âœ… å®Œç¾ |
| **å­¦ä¹ æ›²çº¿** | â­â­â­ ä¸­ç­‰ | â­â­ ç®€å• | â­ å¾ˆç®€å• | â­â­â­â­ å¤æ‚ |
| **ç”Ÿæ€æˆç†Ÿåº¦** | âœ… éå¸¸æˆç†Ÿ | âœ… æˆç†Ÿ | âš ï¸ æ–°å…´ | âš ï¸ è¾ƒæ–° |
| **è¿ç»´å¤æ‚åº¦** | â­â­â­ ä¸­ç­‰ | â­â­ è¾ƒä½ | â­ å¾ˆä½ | â­â­â­â­ é«˜ |

### ä¸ºä»€ä¹ˆé€‰æ‹©Kafkaï¼Ÿ

#### 1. **æŒä¹…åŒ–å’Œé‡æ”¾èƒ½åŠ›**

```
åœºæ™¯ï¼šSparkä½œä¸šå´©æºƒäº†

âŒ RabbitMQ:
  - æ¶ˆæ¯è¢«ç¡®è®¤åå°±åˆ é™¤äº†
  - æ— æ³•é‡æ–°å¤„ç†
  - æ•°æ®ä¸¢å¤±ï¼

âœ… Kafka:
  - æ¶ˆæ¯ä¿ç•™7å¤©ï¼ˆå¯é…ç½®ï¼‰
  - å¯ä»¥ä»ä»»æ„offseté‡æ–°æ¶ˆè´¹
  - Sparké‡å¯åç»§ç»­å¤„ç†
```

**ä»£ç å¯¹æ¯”**ï¼š

```python
# Kafka: é‡æ–°æ¶ˆè´¹æ˜¨å¤©çš„æ•°æ®
consumer.seek(partition, yesterday_offset)
for msg in consumer:
    process(msg)  # é‡æ–°å¤„ç†

# RabbitMQ: æ¶ˆæ¯å·²ç»æ²¡äº†ï¼
```

#### 2. **ä¸Sparkå®Œç¾é›†æˆ**

Kafka + Sparkæ˜¯ä¸šç•Œæ ‡å‡†ç»„åˆï¼š

```python
# Sparkè¯»Kafkaåªéœ€4è¡Œä»£ç 
df = (spark.readStream
      .format("kafka")
      .option("kafka.bootstrap.servers", "localhost:9092")
      .option("subscribe", "ai-social-raw")
      .load())
```

**å…¶ä»–æ¶ˆæ¯é˜Ÿåˆ—**ï¼š
- RabbitMQ + Sparkï¼šéœ€è¦è‡ªå®šä¹‰Source
- Redis Streams + Sparkï¼šç¤¾åŒºæ”¯æŒæœ‰é™

#### 3. **Pullæ¨¡å¼ï¼ˆæ¶ˆè´¹è€…æ§åˆ¶ï¼‰**

```
Pushæ¨¡å¼ (RabbitMQ):
  Broker â”€â”€å¼ºåˆ¶æ¨é€â”€â”€â†’ Consumer
  é—®é¢˜ï¼šConsumerå¤„ç†ä¸è¿‡æ¥ä¼šå´©æºƒ

Pullæ¨¡å¼ (Kafka):
  Consumer â”€â”€æŒ‰éœ€æ‹‰å–â”€â”€â†’ Broker
  ä¼˜åŠ¿ï¼šConsumerå¯ä»¥æ§åˆ¶æ¶ˆè´¹é€Ÿåº¦
```

**å®é™…åœºæ™¯**ï¼š

```python
# Sparkå¤„ç†é€Ÿåº¦æ…¢äº†
# Kafka: æ¶ˆæ¯ç§¯å‹åœ¨é˜Ÿåˆ—é‡Œï¼Œä¸ä¼šä¸¢
# Sparkå¤„ç†å®Œä¸€ä¸ªæ‰¹æ¬¡ï¼Œå†æ‹‰å–ä¸‹ä¸€ä¸ª

# RabbitMQ: Brokerä¸€ç›´æ¨ï¼ŒConsumerå´©æºƒ
```

#### 4. **ä¸é€‰æ‹©å…¶ä»–æ–¹æ¡ˆçš„åŸå› **

##### RabbitMQ

```
âœ… ä¼˜åŠ¿ï¼š
- è·¯ç”±çµæ´»ï¼ˆExchange + Queueï¼‰
- ä½å»¶è¿Ÿ
- AMQPåè®®æ ‡å‡†

âŒ åŠ£åŠ¿ï¼š
- ä¸é€‚åˆé«˜åååœºæ™¯
- æ¶ˆæ¯ä¸æŒä¹…åŒ–ï¼ˆæ¶ˆè´¹ååˆ é™¤ï¼‰
- ä¸Sparké›†æˆä¸å¥½
```

**é€‚ç”¨åœºæ™¯**ï¼šå¾®æœåŠ¡ä¹‹é—´çš„å¼‚æ­¥é€šä¿¡ã€ä»»åŠ¡é˜Ÿåˆ—

##### Redis Streams

```
âœ… ä¼˜åŠ¿ï¼š
- æä½å»¶è¿Ÿ
- éƒ¨ç½²ç®€å•
- è½»é‡çº§

âŒ åŠ£åŠ¿ï¼š
- æŒä¹…åŒ–æœ‰é™ï¼ˆä¸»è¦åœ¨å†…å­˜ï¼‰
- ä¸é€‚åˆå¤§è§„æ¨¡æ•°æ®
- æµå¤„ç†æ”¯æŒå¼±
```

**é€‚ç”¨åœºæ™¯**ï¼šå®æ—¶æ’è¡Œæ¦œã€ä¼šè¯ç¼“å­˜ã€ç®€å•æµå¤„ç†

##### Apache Pulsar

```
âœ… ä¼˜åŠ¿ï¼š
- åˆ†å±‚å­˜å‚¨ï¼ˆBookKeeper + å¯¹è±¡å­˜å‚¨ï¼‰
- å¤šç§Ÿæˆ·æ”¯æŒ
- Geo-replication

âŒ åŠ£åŠ¿ï¼š
- æ¶æ„å¤æ‚ï¼ˆBroker + BookKeeperï¼‰
- ç¤¾åŒºè¾ƒå°
- å­¦ä¹ æˆæœ¬é«˜
- è¿ç»´æˆæœ¬é«˜
```

**é€‚ç”¨åœºæ™¯**ï¼šäº‘åŸç”Ÿã€å¤šç§Ÿæˆ·SaaSã€éœ€è¦geo-replication

---

## 3.2 æµå¤„ç†æ¡†æ¶é€‰å‹ï¼šSpark Streaming vs Flink

### æ ¸å¿ƒå¯¹æ¯”

| ç‰¹æ€§ | **Spark Streaming** | Apache Flink | Apache Storm |
|------|---------------------|--------------|--------------|
| **å¤„ç†æ¨¡å‹** | Micro-batchï¼ˆæ‰¹å¤„ç†ï¼‰ | âœ… True Streamingï¼ˆé€æ¡ï¼‰ | True Streaming |
| **å»¶è¿Ÿ** | ç§’çº§ï¼ˆ0.5-2ç§’ï¼‰ | âœ… æ¯«ç§’çº§ï¼ˆ<100msï¼‰ | æ¯«ç§’çº§ |
| **ååé‡** | âœ… éå¸¸é«˜ | é«˜ | ä¸­ç­‰ |
| **APIæ˜“ç”¨æ€§** | âœ… DataFrame API | DataStream API | âš ï¸ è¾ƒéš¾ï¼ˆBolt/Spoutï¼‰ |
| **çŠ¶æ€ç®¡ç†** | Checkpoint | âœ… State Backend | âš ï¸ æœ‰é™ |
| **SQLæ”¯æŒ** | âœ… Spark SQL | Flink SQL | âŒ æ—  |
| **æ‰¹æµä¸€ä½“** | âœ… å®Œç¾ï¼ˆåŒä¸€å¥—APIï¼‰ | âœ… æ”¯æŒ | âŒ åªèƒ½æµå¤„ç† |
| **ç”Ÿæ€æˆç†Ÿåº¦** | âœ… æœ€æˆç†Ÿ | æˆç†Ÿ | è¾ƒè€æ—§ |
| **å­¦ä¹ æ›²çº¿** | â­â­ ä¸­ç­‰ | â­â­â­ è¾ƒéš¾ | â­â­â­â­ éš¾ |

### ä¸ºä»€ä¹ˆé€‰æ‹©Spark Streamingï¼Ÿ

#### 1. **æ‰¹æµä¸€ä½“ï¼ˆLambdaæ¶æ„ç®€åŒ–ï¼‰**

**ä¼ ç»Ÿé—®é¢˜**ï¼šæ‰¹å¤„ç†å’Œæµå¤„ç†éœ€è¦ä¸¤å¥—ä»£ç 

```
ä¼ ç»Ÿæ–¹æ¡ˆ:
  æ‰¹å¤„ç†: Hive/Spark SQL (SQL)
  æµå¤„ç†: Flink/Storm (Java/Scala)
  â†’ éœ€è¦ç»´æŠ¤ä¸¤å¥—ä»£ç ï¼

Sparkæ–¹æ¡ˆ:
  æ‰¹å¤„ç†: Spark DataFrame
  æµå¤„ç†: Spark Structured Streaming (åŒæ ·çš„DataFrame API!)
  â†’ ä»£ç å¤ç”¨ï¼Œå­¦ä¹ æˆæœ¬ä½ï¼
```

**å®é™…ä»£ç å¯¹æ¯”**ï¼š

```python
# æ‰¹å¤„ç†ï¼ˆè¯»å†å²æ•°æ®ï¼‰
batch_df = (spark.read
            .format("parquet")
            .load("s3a://lakehouse/bronze/social_media/"))

# æµå¤„ç†ï¼ˆè¯»å®æ—¶æ•°æ®ï¼‰
stream_df = (spark.readStream  # å”¯ä¸€åŒºåˆ«ï¼šreadStream
             .format("kafka")
             .load())

# ç›¸åŒçš„è½¬æ¢é€»è¾‘ï¼
result = (df
          .filter(col("source") == "reddit")
          .groupBy("subreddit")
          .count())

# æ‰¹å¤„ç†ï¼šå†™ä¸€æ¬¡
result.write.parquet("output")

# æµå¤„ç†ï¼šæŒç»­å†™å…¥
result.writeStream.start()  # å”¯ä¸€åŒºåˆ«ï¼šwriteStream
```

#### 2. **Micro-batchçš„ä¼˜åŠ¿ï¼ˆåœ¨æˆ‘ä»¬åœºæ™¯ä¸‹ï¼‰**

**Flink vs Sparkå»¶è¿Ÿå¯¹æ¯”**ï¼š

```
æ•°æ®æµç‰¹ç‚¹ï¼š
- é‡‡é›†é¢‘ç‡ï¼š60ç§’/æ¬¡ï¼ˆä¸æ˜¯æ¯«ç§’çº§ï¼‰
- æ•°æ®é‡ï¼š~100æ¡/æ¬¡ï¼ˆä¸æ˜¯ç™¾ä¸‡æ¡ï¼‰

å»¶è¿Ÿè¦æ±‚ï¼š
- ç”¨æˆ·æœŸæœ›ï¼š< 1åˆ†é’Ÿçœ‹åˆ°æœ€æ–°æ•°æ®
- å®é™…å»¶è¿Ÿï¼š
  - Spark: 30ç§’æ‰¹æ¬¡ + 5ç§’å¤„ç† = 35ç§’ âœ… å¤Ÿç”¨ï¼
  - Flink: 10mså»¶è¿Ÿ âœ… æ›´å¿«ï¼Œä½†æ²¡å¿…è¦

ç»“è®ºï¼šSparkçš„ç§’çº§å»¶è¿Ÿå®Œå…¨æ»¡è¶³éœ€æ±‚ï¼ŒFlinkçš„æ¯«ç§’çº§å»¶è¿Ÿæ˜¯è¿‡åº¦è®¾è®¡
```

**Micro-batchçš„ä¼˜åŠ¿**ï¼š

```
âœ… ååé‡é«˜ï¼š
  - æ‰¹é‡å¤„ç†æ•ˆç‡é«˜
  - å¯ä»¥ç”¨æ‰¹å¤„ç†çš„ä¼˜åŒ–æŠ€æœ¯ï¼ˆCatalystä¼˜åŒ–å™¨ï¼‰

âœ… å®¹é”™ç®€å•ï¼š
  - åªéœ€è®°å½•batch IDå’Œoffset
  - Flinkéœ€è¦å¤æ‚çš„state snapshot

âœ… èƒŒå‹å¤„ç†å¥½ï¼š
  - å¤„ç†æ…¢äº†ï¼Œæ‰¹æ¬¡é—´éš”è‡ªåŠ¨æ‹‰é•¿
  - Flinkéœ€è¦æ‰‹åŠ¨è°ƒæ•´å¹¶è¡Œåº¦
```

#### 3. **SQLæ”¯æŒå’Œæ•°æ®å·¥ç¨‹å¸ˆå‹å¥½**

```python
# Spark: æ•°æ®å·¥ç¨‹å¸ˆç†Ÿæ‚‰çš„SQL
spark.sql("""
    SELECT source, COUNT(*) as count
    FROM kafka_stream
    WHERE text LIKE '%GPT%'
    GROUP BY source
""").writeStream.start()

# Flink: éœ€è¦å­¦ä¹ DataStream API
stream.keyBy(lambda x: x['source'])
      .window(TumblingEventTimeWindows.of(Time.seconds(30)))
      .apply(CountWindowFunction())
```

#### 4. **ä¸é€‰æ‹©Flinkçš„åŸå› ï¼ˆä¸æ˜¯è¯´Flinkä¸å¥½ï¼‰**

```
Flinkçš„ä¼˜åŠ¿åœ¨äºï¼š
  âœ… æä½å»¶è¿Ÿï¼ˆé‡‘èäº¤æ˜“ã€å®æ—¶æ¨èï¼‰
  âœ… å¤æ‚äº‹ä»¶å¤„ç†ï¼ˆCEPï¼‰
  âœ… ç²¾ç¡®çš„äº‹ä»¶æ—¶é—´å¤„ç†

ä½†æˆ‘ä»¬çš„åœºæ™¯ï¼š
  âŒ ä¸éœ€è¦æ¯«ç§’çº§å»¶è¿Ÿï¼ˆç¤¾äº¤åª’ä½“ç›‘æ§ï¼Œç§’çº§å¤Ÿç”¨ï¼‰
  âŒ ä¸éœ€è¦CEPï¼ˆç®€å•çš„ETLï¼‰
  âŒ æ•°æ®æºæ˜¯å®šæœŸé‡‡é›†ï¼ˆ60ç§’ï¼‰ï¼Œä¸æ˜¯å®æ—¶æµ

é¢å¤–è€ƒè™‘ï¼š
  âŒ Flinkè¿ç»´å¤æ‚ï¼ˆéœ€è¦JobManager + TaskManagerï¼‰
  âŒ å­¦ä¹ æˆæœ¬é«˜ï¼ˆDatastream APIæ¯”DataFrameéš¾ï¼‰
  âŒ å›¢é˜Ÿç†Ÿæ‚‰Sparkï¼ˆè¿ç§»æˆæœ¬é«˜ï¼‰
```

**Flinké€‚ç”¨åœºæ™¯**ï¼š
- å®æ—¶é£æ§ï¼ˆå»¶è¿Ÿ < 100msï¼‰
- å®æ—¶æ¨èç³»ç»Ÿ
- IoTè®¾å¤‡ç›‘æ§ï¼ˆç™¾ä¸‡QPSï¼‰

---

## 3.3 å¯¹è±¡å­˜å‚¨é€‰å‹ï¼šMinIO vs S3 vs HDFS

### å¯¹æ¯”è¡¨

| ç‰¹æ€§ | **MinIO** | AWS S3 | HDFS | Azure Blob |
|------|-----------|--------|------|------------|
| **éƒ¨ç½²æ¨¡å¼** | âœ… è‡ªæ‰˜ç®¡ï¼ˆæœ¬åœ°ï¼‰ | äº‘ç«¯æ‰˜ç®¡ | è‡ªæ‰˜ç®¡ï¼ˆé›†ç¾¤ï¼‰ | äº‘ç«¯æ‰˜ç®¡ |
| **S3å…¼å®¹** | âœ… 100%å…¼å®¹ | åŸç”ŸS3 | âŒ ä¸å…¼å®¹ | âš ï¸ éƒ¨åˆ†å…¼å®¹ |
| **æˆæœ¬** | âœ… å…è´¹ï¼ˆç¡¬ä»¶ï¼‰ | æŒ‰ç”¨é‡ä»˜è´¹ | ç¡¬ä»¶+è¿ç»´ | æŒ‰ç”¨é‡ä»˜è´¹ |
| **å»¶è¿Ÿ** | âœ… <1msï¼ˆæœ¬åœ°ï¼‰ | 50-200ms | <1msï¼ˆå±€åŸŸç½‘ï¼‰ | 50-200ms |
| **å¯é æ€§** | çº åˆ ç  | âœ… 99.999999999% | å‰¯æœ¬æœºåˆ¶ | âœ… é«˜ |
| **æ‰©å±•æ€§** | EBçº§ | âœ… æ— é™ | PBçº§ | âœ… æ— é™ |
| **è¿ç»´å¤æ‚åº¦** | â­â­ ä¸­ç­‰ | â­ æ— ï¼ˆæ‰˜ç®¡ï¼‰ | â­â­â­â­ é«˜ | â­ æ— ï¼ˆæ‰˜ç®¡ï¼‰ |

### ä¸ºä»€ä¹ˆé€‰æ‹©MinIOï¼Ÿ

#### 1. **å¼€å‘ç¯å¢ƒéœ€æ±‚ï¼ˆä¸»è¦åŸå› ï¼‰**

```
å¼€å‘é˜¶æ®µç—›ç‚¹ï¼š
  âŒ AWS S3:
     - éœ€è¦ç½‘ç»œè¿æ¥
     - æ¯æ¬¡è¯»å†™éƒ½æœ‰è´¹ç”¨ï¼ˆ$0.005/1000è¯·æ±‚ï¼‰
     - è°ƒè¯•éœ€è¦çœ‹CloudWatchæ—¥å¿—ï¼ˆè´¹ç”¨ï¼‰
     - å¼€å‘ç¯å¢ƒæ•°æ®å’Œç”Ÿäº§ç¯å¢ƒæ··åœ¨ä¸€èµ·

  âœ… MinIO:
     - æœ¬åœ°Dockerè¿è¡Œï¼Œå®Œå…¨å…è´¹
     - æ— ç½‘ç»œä¾èµ–ï¼Œå»¶è¿Ÿ<1ms
     - å®Œæ•´æ—¥å¿—åœ¨æœ¬åœ°ï¼Œæ˜“äºè°ƒè¯•
     - å¼€å‘å’Œç”Ÿäº§æ•°æ®éš”ç¦»
```

#### 2. **S3 APIå…¼å®¹æ€§ï¼ˆç”Ÿäº§ç¯å¢ƒè¿ç§»ï¼‰**

```python
# MinIOé…ç½®ï¼ˆå¼€å‘ç¯å¢ƒï¼‰
spark.config("spark.hadoop.fs.s3a.endpoint", "http://localhost:9000")
spark.config("spark.hadoop.fs.s3a.access.key", "minioadmin")
spark.config("spark.hadoop.fs.s3a.secret.key", "minioadmin")

# è¿ç§»åˆ°ç”Ÿäº§ç¯å¢ƒï¼ˆAWS S3ï¼‰
# åªéœ€æ”¹3è¡Œé…ç½®ï¼
spark.config("spark.hadoop.fs.s3a.endpoint", "s3.amazonaws.com")
spark.config("spark.hadoop.fs.s3a.access.key", os.getenv("AWS_ACCESS_KEY"))
spark.config("spark.hadoop.fs.s3a.secret.key", os.getenv("AWS_SECRET_KEY"))

# ä»£ç å®Œå…¨ä¸å˜ï¼
df.write.parquet("s3a://lakehouse/bronze/social_media/")
```

#### 3. **æ€§èƒ½å¯¹æ¯”ï¼ˆå®æµ‹ï¼‰**

```
æµ‹è¯•åœºæ™¯ï¼šå†™å…¥100MB Parquetæ–‡ä»¶

MinIO (æœ¬åœ°):
  - å»¶è¿Ÿï¼š50ms
  - ååï¼š2GB/s

AWS S3 (us-east-1):
  - å»¶è¿Ÿï¼š300msï¼ˆç½‘ç»œRTTï¼‰
  - ååï¼š100MB/sï¼ˆå¸¦å®½é™åˆ¶ï¼‰

ç»“è®ºï¼šå¼€å‘ç¯å¢ƒç”¨MinIOå¿«40å€ï¼
```

#### 4. **ä¸ºä»€ä¹ˆä¸ç”¨HDFSï¼Ÿ**

```
HDFSçš„é—®é¢˜ï¼š
  âŒ æ¶æ„å¤æ‚ï¼š
     - éœ€è¦NameNodeï¼ˆå…ƒæ•°æ®ç®¡ç†ï¼‰
     - éœ€è¦DataNodeï¼ˆæ•°æ®å­˜å‚¨ï¼‰
     - éœ€è¦Secondary NameNodeï¼ˆå¤‡ä»½ï¼‰
     - å•æœºç¯å¢ƒå¾ˆéš¾æ­å»ºå®Œæ•´é›†ç¾¤

  âŒ è¿ç»´æˆæœ¬é«˜ï¼š
     - NameNodeæ˜¯å•ç‚¹æ•…éšœï¼ˆéœ€è¦HAï¼‰
     - å°æ–‡ä»¶é—®é¢˜ä¸¥é‡ï¼ˆNameNodeå†…å­˜ç“¶é¢ˆï¼‰
     - æ‰©å®¹éœ€è¦rebalanceï¼ˆæ…¢ï¼‰

  âŒ äº‘åŸç”Ÿå·®ï¼š
     - ä¸å…¼å®¹S3 API
     - æ— æ³•ä¸äº‘æœåŠ¡é›†æˆ
     - è¿ç§»åˆ°äº‘ç«¯éœ€è¦é‡å†™ä»£ç 
```

**HDFSé€‚ç”¨åœºæ™¯**ï¼š
- ç°æœ‰Hadoopç”Ÿæ€ï¼ˆHive, HBaseï¼‰
- æœ¬åœ°å¤§æ•°æ®é›†ç¾¤ï¼ˆä¸è€ƒè™‘ä¸Šäº‘ï¼‰
- æä½å»¶è¿Ÿè¦æ±‚ï¼ˆHDFSå±€åŸŸç½‘ < S3ç½‘ç»œï¼‰

#### 5. **è¿ç§»ç­–ç•¥ï¼ˆå¼€å‘ â†’ ç”Ÿäº§ï¼‰**

```
Phase 1: å¼€å‘ï¼ˆå½“å‰ï¼‰
  MinIO (Docker)
  â””â”€ localhost:9000

Phase 2: å†…éƒ¨æµ‹è¯•
  MinIO (K8sé›†ç¾¤)
  â””â”€ minio.internal:9000

Phase 3: ç”Ÿäº§ï¼ˆæœªæ¥ï¼‰
  é€‰é¡¹A: AWS S3
    â””â”€ s3.amazonaws.com
    â””â”€ æ”¹3è¡Œé…ç½®å³å¯

  é€‰é¡¹B: MinIOå¤šèŠ‚ç‚¹é›†ç¾¤
    â””â”€ minio-prod.company.com
    â””â”€ æ›´ä¾¿å®œï¼Œä½†éœ€è¦è¿ç»´
```

---

## 3.4 æ•°æ®æ¹–æ–¹æ¡ˆé€‰å‹ï¼šParquet vs Delta Lake vs Iceberg

### æ ¸å¿ƒå¯¹æ¯”

| ç‰¹æ€§ | **Parquet** | **Delta Lake** | Apache Iceberg | Apache Hudi |
|------|-------------|----------------|----------------|-------------|
| **ACIDäº‹åŠ¡** | âŒ æ—  | âœ… å®Œæ•´ | âœ… å®Œæ•´ | âœ… å®Œæ•´ |
| **æ—¶é—´æ—…è¡Œ** | âŒ æ—  | âœ… æ”¯æŒ | âœ… æ”¯æŒ | âœ… æ”¯æŒ |
| **UPDATE/DELETE** | âŒ ä¸æ”¯æŒ | âœ… æ”¯æŒ | âœ… æ”¯æŒ | âœ… æ”¯æŒ |
| **Schemaæ¼”åŒ–** | âš ï¸ æœ‰é™ | âœ… è‡ªåŠ¨ | âœ… è‡ªåŠ¨ | âœ… è‡ªåŠ¨ |
| **æ–‡ä»¶å‹ç¼©** | âŒ æ‰‹åŠ¨ | âœ… OPTIMIZE | âœ… Compaction | âœ… Compaction |
| **Sparké›†æˆ** | âœ… åŸç”Ÿ | âœ… å®Œç¾ | âœ… å®Œç¾ | âœ… å®Œç¾ |
| **å­¦ä¹ æ›²çº¿** | â­ ç®€å• | â­â­ ä¸­ç­‰ | â­â­â­ è¾ƒéš¾ | â­â­â­â­ éš¾ |
| **ç”Ÿæ€æˆç†Ÿåº¦** | âœ… æœ€æˆç†Ÿ | âœ… æˆç†Ÿ | âš ï¸ è¾ƒæ–° | âš ï¸ è¾ƒæ–° |

### æˆ‘ä»¬çš„é€‰æ‹©ï¼šParquetï¼ˆç°åœ¨ï¼‰ + Delta Lakeï¼ˆæœªæ¥ï¼‰

#### å½“å‰é˜¶æ®µï¼šParquet

**ä¸ºä»€ä¹ˆå…ˆç”¨Parquetï¼Ÿ**

```
Phase 1éœ€æ±‚ï¼ˆMVPï¼‰ï¼š
  âœ… åªéœ€è¿½åŠ å†™å…¥ï¼ˆappendï¼‰
  âœ… ä¸éœ€è¦ä¿®æ”¹æ•°æ®
  âœ… ä¸éœ€è¦äº‹åŠ¡ï¼ˆå•å†™å…¥è€…ï¼‰
  âœ… å¿«é€ŸéªŒè¯æ¶æ„

Parquetä¼˜åŠ¿ï¼š
  âœ… é›¶å­¦ä¹ æˆæœ¬ï¼ˆSparkåŸç”Ÿæ”¯æŒï¼‰
  âœ… åˆ—å¼å­˜å‚¨ï¼Œå‹ç¼©æ¯”é«˜
  âœ… æŸ¥è¯¢æ€§èƒ½å¥½
  âœ… ç®€å•å¯é 
```

**Bronzeå±‚çš„Parquetä½¿ç”¨**ï¼š

```python
# ç®€å•ç›´æ¥ï¼Œ5è¡Œä»£ç æå®š
(df.writeStream
   .format("parquet")
   .option("path", "s3a://lakehouse/bronze/social_media/")
   .partitionBy("partition_date", "source")
   .start())
```

#### æœªæ¥é˜¶æ®µï¼šDelta Lake

**ä»€ä¹ˆæ—¶å€™è¿ç§»åˆ°Delta Lakeï¼Ÿ**

```
Phase 2éœ€æ±‚ï¼ˆå¢å¼ºï¼‰ï¼š
  âœ… éœ€è¦å»é‡ï¼ˆUPDATEï¼‰
  âœ… éœ€è¦ä¿®æ­£é”™è¯¯æ•°æ®ï¼ˆDELETEï¼‰
  âœ… éœ€è¦UPSERTï¼ˆåˆå¹¶æ•°æ®ï¼‰
  âœ… éœ€è¦æ—¶é—´æ—…è¡Œï¼ˆæ•°æ®å›æº¯ï¼‰

è§¦å‘æ¡ä»¶ï¼š
  - å¼€å§‹æ„å»ºSilver/Goldå±‚
  - éœ€è¦æ•°æ®è´¨é‡ä¿è¯
  - éœ€è¦CDCï¼ˆChange Data Captureï¼‰
```

**Delta Lakeçš„æ€æ‰‹çº§åŠŸèƒ½**ï¼š

##### 1. UPSERTï¼ˆåˆå¹¶æ“ä½œï¼‰

```python
# åœºæ™¯ï¼šReddit APIæœ‰æ—¶ä¼šè¿”å›é‡å¤æ•°æ®ï¼Œéœ€è¦å»é‡

# Parquet: æ— æ³•ç›´æ¥å¤„ç†ï¼Œéœ€è¦ï¼š
#   1. è¯»å–å…¨éƒ¨æ•°æ®
#   2. å»é‡
#   3. é‡å†™æ•´ä¸ªåˆ†åŒºï¼ˆæ…¢ï¼ï¼‰

# Delta Lake: ä¸€è¡ŒMERGEæå®š
from delta.tables import DeltaTable

deltaTable = DeltaTable.forPath(spark, "s3a://lakehouse/silver/posts")

deltaTable.alias("target").merge(
    new_data.alias("source"),
    "target.post_id = source.post_id"  # ä¸»é”®
).whenMatchedUpdateAll(  # åŒ¹é…ï¼šæ›´æ–°
).whenNotMatchedInsertAll(  # ä¸åŒ¹é…ï¼šæ’å…¥
).execute()

# èƒŒååŸç†ï¼šåªæ›´æ–°_delta_log/ï¼Œä¸é‡å†™æ•°æ®æ–‡ä»¶
```

##### 2. æ—¶é—´æ—…è¡Œï¼ˆTime Travelï¼‰

```python
# åœºæ™¯ï¼šå‘ç°æ˜¨å¤©çš„æ•°æ®å¤„ç†æœ‰bugï¼Œæƒ³å›æº¯æŸ¥çœ‹

# Parquet: ä¸å¯èƒ½ï¼æ•°æ®è¢«è¦†ç›–äº†

# Delta Lake: è½»æ¾å›æº¯
yesterday = spark.read.format("delta") \
    .option("timestampAsOf", "2025-11-11") \
    .load("s3a://lakehouse/silver/posts")

# æˆ–è€…æŒ‰ç‰ˆæœ¬å·
version_10 = spark.read.format("delta") \
    .option("versionAsOf", 10) \
    .load("s3a://lakehouse/silver/posts")

# åº”ç”¨åœºæ™¯ï¼š
# - å®¡è®¡ï¼ˆaudit trailï¼‰
# - å›æ»šé”™è¯¯æ“ä½œ
# - A/Bæµ‹è¯•ï¼ˆå¯¹æ¯”ä¸åŒç‰ˆæœ¬ï¼‰
```

##### 3. OPTIMIZEï¼ˆæ–‡ä»¶å‹ç¼©ï¼‰

```python
# åœºæ™¯ï¼šæµå¼å†™å…¥äº§ç”Ÿå¤§é‡å°æ–‡ä»¶ï¼ŒæŸ¥è¯¢æ…¢

# Parquet: æ‰‹åŠ¨åˆå¹¶ï¼Œå¤æ‚ä¸”å±é™©
# éœ€è¦è¯»å–æ‰€æœ‰å°æ–‡ä»¶ â†’ åˆå¹¶ â†’ å†™å…¥ â†’ åˆ é™¤æ—§æ–‡ä»¶

# Delta Lake: ä¸€è¡Œå‘½ä»¤
deltaTable.optimize().executeCompaction()

# æ•ˆæœï¼š
# Before: 1000ä¸ªå°æ–‡ä»¶ï¼ˆ1MB eachï¼‰
# After:  10ä¸ªå¤§æ–‡ä»¶ï¼ˆ100MB eachï¼‰
# æŸ¥è¯¢é€Ÿåº¦ï¼šæå‡10å€ï¼
```

##### 4. Z-Orderingï¼ˆæ•°æ®å¸ƒå±€ä¼˜åŒ–ï¼‰

```python
# åœºæ™¯ï¼šç»å¸¸æŒ‰post_idå’Œcreated_atæŸ¥è¯¢ï¼Œæƒ³åŠ é€Ÿ

# Parquet: æ— æ³•ä¼˜åŒ–ï¼Œæ–‡ä»¶å†…æ•°æ®æ˜¯éšæœºçš„

# Delta Lake: Z-Orderæ’åº
deltaTable.optimize() \
    .executeZOrderBy("post_id", "created_at")

# åŸç†ï¼šå°†è¿™ä¸¤åˆ—ç›¸è¿‘çš„æ•°æ®æ”¾åœ¨ä¸€èµ·
# æ•ˆæœï¼šæŸ¥è¯¢æ—¶å¯ä»¥è·³è¿‡å¤§é‡ä¸ç›¸å…³æ–‡ä»¶
```

#### ä¸ºä»€ä¹ˆä¸é€‰Iceberg/Hudiï¼Ÿ

##### Apache Iceberg

```
âœ… ä¼˜åŠ¿ï¼š
- å‚å•†ä¸­ç«‹ï¼ˆNetflixå¼€æºï¼‰
- æ”¯æŒSparkã€Flinkã€Trino
- Schemaæ¼”åŒ–å¼ºå¤§

âŒ åŠ£åŠ¿ï¼š
- ç”Ÿæ€è¾ƒæ–°ï¼ˆ2018å¹´å¼€æºï¼‰
- ç¤¾åŒºå°äºDelta Lake
- é…ç½®å¤æ‚ï¼ˆéœ€è¦catalogï¼‰
```

**é€‚ç”¨åœºæ™¯**ï¼š
- å¤šå¼•æ“ç¯å¢ƒï¼ˆSpark + Flink + Trinoï¼‰
- éœ€è¦å‚å•†ä¸­ç«‹
- å¤§å‚æŠ€æœ¯æ ˆï¼ˆNetflix, Apple, Adobeï¼‰

##### Apache Hudi

```
âœ… ä¼˜åŠ¿ï¼š
- å¢é‡å¤„ç†ä¼˜åŒ–ï¼ˆMORæ¨¡å¼ï¼‰
- æµå¼å†™å…¥ä¼˜åŒ–
- Uberå¼€æºï¼Œç”Ÿäº§éªŒè¯

âŒ åŠ£åŠ¿ï¼š
- å­¦ä¹ æ›²çº¿é™¡å³­
- é…ç½®å¤æ‚ï¼ˆCOW vs MORï¼‰
- å¯¹Sparkç‰ˆæœ¬è¦æ±‚ä¸¥æ ¼
```

**é€‚ç”¨åœºæ™¯**ï¼š
- å¢é‡ETLä¸ºä¸»
- éœ€è¦CDCï¼ˆChange Data Captureï¼‰
- Uberå¼æ¶æ„

##### æˆ‘ä»¬é€‰Delta Lakeçš„åŸå› 

```
âœ… SparkåŸç”Ÿæ”¯æŒï¼ˆDatabricksç»´æŠ¤ï¼‰
âœ… ç¤¾åŒºæœ€å¤§ï¼ˆGitHub 6K+ starsï¼‰
âœ… æ–‡æ¡£æœ€å®Œå–„
âœ… ç®€å•æ˜“ç”¨ï¼ˆAPIå‹å¥½ï¼‰
âœ… ä¸Spark SQLæ— ç¼é›†æˆ
âœ… æˆç†Ÿåº¦é«˜ï¼ˆå¤§é‡ç”Ÿäº§æ¡ˆä¾‹ï¼‰
```

---

## 3.5 å¯è§†åŒ–å·¥å…·é€‰å‹ï¼šStreamlit vs å…¶ä»–æ–¹æ¡ˆ

### å¯¹æ¯”è¡¨

| å·¥å…· | **Streamlit** | Dash | Grafana | Flask+React | Tableau |
|------|---------------|------|---------|-------------|---------|
| **å¼€å‘è¯­è¨€** | âœ… çº¯Python | Python | âŒ é…ç½® | Python+JS | âŒ æ‹–æ‹½ |
| **å¼€å‘é€Ÿåº¦** | âœ… æœ€å¿«ï¼ˆå°æ—¶çº§ï¼‰ | å¤©çº§ | å°æ—¶çº§ | å‘¨çº§ | å°æ—¶çº§ |
| **è‡ªå®šä¹‰ç¨‹åº¦** | ä¸­ | é«˜ | ä½ | âœ… æœ€é«˜ | ä½ |
| **å®æ—¶æ•°æ®** | âœ… æ”¯æŒ | âœ… æ”¯æŒ | âœ… ä¸“æ³¨ | âœ… æ”¯æŒ | âš ï¸ æœ‰é™ |
| **éƒ¨ç½²éš¾åº¦** | â­ ç®€å• | â­â­ ä¸­ç­‰ | â­â­ ä¸­ç­‰ | â­â­â­â­ å¤æ‚ | â­â­â­ å¤æ‚ |
| **é€‚ç”¨åœºæ™¯** | âœ… åŸå‹/å†…éƒ¨ | ä¼ä¸šDashboard | ç›‘æ§æŒ‡æ ‡ | ç”Ÿäº§çº§åº”ç”¨ | å•†ä¸šæ™ºèƒ½ |

### ä¸ºä»€ä¹ˆé€‰æ‹©Streamlitï¼Ÿ

#### 1. **å¿«é€ŸåŸå‹éªŒè¯**

**æ—¶é—´å¯¹æ¯”**ï¼š

```
ä»»åŠ¡ï¼šæ„å»ºä¸€ä¸ªå®æ—¶Dashboardï¼Œæ˜¾ç¤ºKafkaæ•°æ®

Streamlit: 2å°æ—¶
  â”œâ”€ 30åˆ†é’Ÿï¼šæ­å»ºåŸºç¡€æ¡†æ¶
  â”œâ”€ 1å°æ—¶ï¼šå®ç°æ•°æ®è¯»å–å’Œå›¾è¡¨
  â””â”€ 30åˆ†é’Ÿï¼šæ·»åŠ ç­›é€‰å’Œæ ·å¼

Flask + React: 1å‘¨
  â”œâ”€ 1å¤©ï¼šæ­å»ºFlask API
  â”œâ”€ 2å¤©ï¼šReactç»„ä»¶å¼€å‘
  â”œâ”€ 1å¤©ï¼šWebSocketå®æ—¶æ¨é€
  â”œâ”€ 1å¤©ï¼šæ•°æ®å¯è§†åŒ–ï¼ˆEchartsï¼‰
  â””â”€ 2å¤©ï¼šéƒ¨ç½²å’Œä¼˜åŒ–
```

#### 2. **çº¯Pythonå¼€å‘**

```python
# Streamlit: æ•°æ®å·¥ç¨‹å¸ˆçš„èˆ’é€‚åŒº
import streamlit as st
import pandas as pd

st.title("Dashboard")
df = pd.read_csv("data.csv")
st.dataframe(df)
st.line_chart(df)

# å®Œå…¨ä¸éœ€è¦ï¼š
# âŒ HTML/CSS/JavaScript
# âŒ å‰åç«¯é€šä¿¡
# âŒ è·¯ç”±é…ç½®
# âŒ çŠ¶æ€ç®¡ç†
```

#### 3. **å®æ—¶åˆ·æ–°æœºåˆ¶**

```python
# Streamlitçš„è‡ªåŠ¨åˆ·æ–°ï¼ˆ60ç§’å€’è®¡æ—¶ï¼‰
import time

for remaining in range(60, 0, -1):
    st.markdown(f"åˆ·æ–°å€’è®¡æ—¶: {remaining}ç§’")
    time.sleep(1)

st.rerun()  # è‡ªåŠ¨åˆ·æ–°æ•´ä¸ªé¡µé¢

# Grafana: éœ€è¦é…ç½®æ•°æ®æºrefresh interval
# Dash: éœ€è¦ä½¿ç”¨dcc.Intervalç»„ä»¶
```

#### 4. **ä¸ºä»€ä¹ˆä¸ç”¨å…¶ä»–å·¥å…·ï¼Ÿ**

##### Grafana

```
âœ… é€‚åˆï¼š
  - ç³»ç»Ÿç›‘æ§ï¼ˆCPUã€å†…å­˜ã€ç½‘ç»œï¼‰
  - æ—¶åºæ•°æ®ï¼ˆPrometheusã€InfluxDBï¼‰
  - é¢„å®šä¹‰çš„é¢æ¿å’ŒæŸ¥è¯¢

âŒ ä¸é€‚åˆï¼š
  - è‡ªå®šä¹‰æ•°æ®å¤„ç†ï¼ˆæ— æ³•è¿è¡ŒPythonä»£ç ï¼‰
  - NLPåˆ†æï¼ˆéœ€è¦å¤–éƒ¨è®¡ç®—ï¼‰
  - å¤æ‚äº¤äº’ï¼ˆç­›é€‰ã€æ’åºã€åˆ†é¡µï¼‰
```

##### Dash (Plotly)

```
âœ… é€‚åˆï¼š
  - ä¼ä¸šçº§Dashboard
  - å¤æ‚äº¤äº’ï¼ˆå¤šé¡µé¢åº”ç”¨ï¼‰
  - éœ€è¦å›è°ƒæ§åˆ¶ï¼ˆcallbacksï¼‰

âŒ ä¸ºä»€ä¹ˆæˆ‘ä»¬ä¸ç”¨ï¼š
  - å­¦ä¹ æ›²çº¿è¾ƒé™¡ï¼ˆéœ€è¦ç†è§£å›è°ƒæœºåˆ¶ï¼‰
  - ä»£ç é‡å¤šï¼ˆéœ€è¦å®šä¹‰layout + callbacksï¼‰
  - å¯¹äºç®€å•Dashboardè¿‡äºé‡é‡çº§
```

**ä»£ç å¯¹æ¯”**ï¼š

```python
# Dash: éœ€è¦å®šä¹‰layoutå’Œcallbacks
from dash import Dash, html, dcc, Input, Output

app = Dash(__name__)

app.layout = html.Div([
    dcc.Graph(id='graph'),
    dcc.Interval(id='interval', interval=60000)
])

@app.callback(
    Output('graph', 'figure'),
    Input('interval', 'n_intervals')
)
def update_graph(n):
    data = load_data()
    return create_figure(data)

# Streamlit: ç›´æ¥å†™é€»è¾‘
import streamlit as st

data = load_data()
st.line_chart(data)
st.rerun()  # è‡ªåŠ¨åˆ·æ–°
```

##### Flask + React

```
âœ… é€‚åˆï¼š
  - ç”Ÿäº§çº§åº”ç”¨
  - éœ€è¦å®Œå…¨è‡ªå®šä¹‰UI
  - å¤æ‚çš„ç”¨æˆ·äº¤äº’

âŒ ä¸ºä»€ä¹ˆæˆ‘ä»¬ä¸ç”¨ï¼š
  - å¼€å‘å‘¨æœŸé•¿ï¼ˆå‰åç«¯åˆ†ç¦»ï¼‰
  - éœ€è¦å‰ç«¯æŠ€èƒ½ï¼ˆReactã€Reduxã€Webpackï¼‰
  - è¿ç»´å¤æ‚ï¼ˆéœ€è¦éƒ¨ç½²å‰åç«¯ï¼‰
```

#### 5. **æœªæ¥è¿ç§»ç­–ç•¥**

```
Phase 1: Streamlitï¼ˆå½“å‰ï¼‰
  - å¿«é€ŸéªŒè¯
  - å†…éƒ¨ä½¿ç”¨
  - MVPé˜¶æ®µ

Phase 2: Streamlit Cloud
  - ä¸€é”®éƒ¨ç½²
  - å›¢é˜Ÿå…±äº«
  - è‡ªåŠ¨HTTPS

Phase 3: ç”Ÿäº§çº§ï¼ˆå¦‚æœéœ€è¦ï¼‰
  é€‰é¡¹A: Dash
    - æ›´å¼ºçš„äº¤äº’
    - æ›´å¥½çš„æ€§èƒ½

  é€‰é¡¹B: Flask + React
    - å®Œå…¨è‡ªå®šä¹‰
    - ä¼ä¸šçº§éƒ¨ç½²
```

---

## 3.6 å¼€å‘è¯­è¨€é€‰å‹ï¼šPython vs å…¶ä»–è¯­è¨€

### ä¸ºä»€ä¹ˆå…¨æ ˆPythonï¼Ÿ

```
æˆ‘ä»¬çš„æŠ€æœ¯æ ˆï¼š
  â”œâ”€ æ•°æ®é‡‡é›†ï¼šPython (Tweepy, PRAW)
  â”œâ”€ æ¶ˆæ¯é˜Ÿåˆ—ï¼šPython (kafka-python)
  â”œâ”€ æµå¤„ç†ï¼šPython (PySpark)
  â”œâ”€ æ•°æ®åˆ†æï¼šPython (Pandas, DuckDB)
  â””â”€ å¯è§†åŒ–ï¼šPython (Streamlit)

ä¼˜åŠ¿ï¼š
  âœ… ç»Ÿä¸€è¯­è¨€ï¼Œé™ä½å­¦ä¹ æˆæœ¬
  âœ… ä¸°å¯Œçš„æ•°æ®å¤„ç†åº“
  âœ… å¿«é€ŸåŸå‹å¼€å‘
  âœ… å›¢é˜ŸæŠ€èƒ½åŒ¹é…
```

### ä¸ºä»€ä¹ˆä¸ç”¨Scalaï¼ˆSparkçš„åŸç”Ÿè¯­è¨€ï¼‰ï¼Ÿ

```
Scalaä¼˜åŠ¿ï¼š
  âœ… SparkåŸç”Ÿæ”¯æŒï¼ˆæ€§èƒ½æœ€å¥½ï¼‰
  âœ… ç±»å‹å®‰å…¨ï¼ˆç¼–è¯‘æ—¶æ£€æŸ¥ï¼‰
  âœ… å‡½æ•°å¼ç¼–ç¨‹ï¼ˆä»£ç ç®€æ´ï¼‰

ä½†æˆ‘ä»¬ä¸éœ€è¦ï¼š
  âŒ æ€§èƒ½å·®è·ä¸å¤§ï¼ˆPySparkè°ƒç”¨Spark Coreï¼‰
  âŒ å­¦ä¹ æ›²çº¿é™¡å³­ï¼ˆå›¢é˜Ÿä¸ç†Ÿæ‚‰ï¼‰
  âŒ ç”Ÿæ€ä¸å¦‚Pythonä¸°å¯Œ
```

**æ€§èƒ½å¯¹æ¯”**ï¼š

```
æµ‹è¯•åœºæ™¯ï¼šå¤„ç†1GBæ•°æ®

Scala (åŸç”ŸSpark):
  - è¿è¡Œæ—¶é—´ï¼š10ç§’
  - CPUä½¿ç”¨ï¼š80%

Python (PySpark):
  - è¿è¡Œæ—¶é—´ï¼š12ç§’ï¼ˆæ…¢20%ï¼‰
  - CPUä½¿ç”¨ï¼š85%

ç»“è®ºï¼šæ€§èƒ½å·®è·å¯æ¥å—ï¼ŒPythonçš„å¼€å‘æ•ˆç‡ä¼˜åŠ¿æ›´å¤§
```

---

**æ¥ä¸‹æ¥**ï¼š[Part 4: å…³é”®ä»£ç æ·±åº¦è§£æ](#part-4-å…³é”®ä»£ç æ·±åº¦è§£æ) å°†é€è¡Œè§£æé¡¹ç›®ä¸­çš„æ ¸å¿ƒä»£ç ã€‚

---

# Part 4: å…³é”®ä»£ç æ·±åº¦è§£æ

> ç²¾é€‰æ ¸å¿ƒä»£ç ç‰‡æ®µï¼Œé€è¡Œè§£è¯»å…³é”®å®ç°

ç”±äºPart 2å·²ç»è¯¦ç»†è®²è§£äº†å„ç»„ä»¶çš„ä»£ç ï¼Œè¿™é‡Œåªé‡ç‚¹è§£ææœ€å…³é”®çš„å®ç°æ¨¡å¼å’Œå‘ç‚¹ã€‚

---

## 4.1 å¯åŠ¨è„šæœ¬æ¨¡å¼ï¼ˆBashè„šæœ¬æœ€ä½³å®è·µï¼‰

**æ–‡ä»¶**ï¼š`scripts/00-start_all.sh`

### æ ¸å¿ƒæ¨¡å¼ï¼šPIDç®¡ç† + æ—¥å¿—é‡å®šå‘

```bash
#!/bin/bash

# ===== æ¨¡å¼1: PIDæ–‡ä»¶ç®¡ç†ï¼ˆé¿å…é‡å¤å¯åŠ¨ï¼‰ =====
PID_FILE="logs/reddit.pid"

# æ£€æŸ¥æ˜¯å¦å·²ç»è¿è¡Œ
if [ -f "$PID_FILE" ]; then
    OLD_PID=$(cat "$PID_FILE")
    if ps -p "$OLD_PID" > /dev/null 2>&1; then
        echo "âš ï¸ Reddit collector already running (PID: $OLD_PID)"
        exit 1
    else
        # æ—§è¿›ç¨‹å·²æ­»ï¼Œæ¸…ç†PIDæ–‡ä»¶
        rm "$PID_FILE"
    fi
fi

# ===== æ¨¡å¼2: åå°è¿è¡Œ + æ—¥å¿—é‡å®šå‘ =====
python data_ingestion/reddit/collector.py \
    > logs/reddit_collector.log 2>&1 &  # stdoutå’Œstderréƒ½é‡å®šå‘åˆ°æ—¥å¿—

# ä¿å­˜PID
echo $! > "$PID_FILE"
echo "âœ… Started (PID: $!)"

# ===== æ¨¡å¼3: å¥åº·æ£€æŸ¥ï¼ˆç­‰å¾…æœåŠ¡å¯åŠ¨ï¼‰ =====
MAX_WAIT=30
COUNTER=0

while [ $COUNTER -lt $MAX_WAIT ]; do
    if grep -q "âœ…" logs/reddit_collector.log 2>/dev/null; then
        echo "âœ… Service healthy"
        exit 0
    fi
    sleep 1
    COUNTER=$((COUNTER+1))
done

echo "âŒ Service start timeout"
exit 1
```

**å…³é”®æŠ€å·§**ï¼š

1. **PIDæ–‡ä»¶ç®¡ç†**ï¼šé¿å…é‡å¤å¯åŠ¨åŒä¸€æœåŠ¡
2. **æ—¥å¿—é‡å®šå‘**ï¼š`> logs/file.log 2>&1 &` åå°è¿è¡Œå¹¶è®°å½•æ—¥å¿—
3. **å¥åº·æ£€æŸ¥**ï¼šå¯åŠ¨åæ£€æŸ¥æ—¥å¿—ç¡®è®¤æœåŠ¡æ­£å¸¸
4. **é”™è¯¯å¤„ç†**ï¼šæ¯ä¸€æ­¥éƒ½æœ‰é”™è¯¯ç å’Œæç¤º

---

## 4.2 Docker Composeé«˜çº§æ¨¡å¼

**æ–‡ä»¶**ï¼š`docker-compose-full.yml`

### æ¨¡å¼ï¼šService Dependency + Health Check

```yaml
services:
  # ===== Kafkaä¾èµ–Zookeeperï¼Œå¿…é¡»ç­‰Zookeeperå¯åŠ¨ =====
  kafka:
    depends_on:
      zookeeper:
        condition: service_healthy  # ç­‰å¾…å¥åº·æ£€æŸ¥é€šè¿‡

  zookeeper:
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "2181"]  # æ£€æŸ¥ç«¯å£æ˜¯å¦å¼€æ”¾
      interval: 10s  # æ¯10ç§’æ£€æŸ¥ä¸€æ¬¡
      timeout: 5s
      retries: 3
      start_period: 30s  # å¯åŠ¨30ç§’åæ‰å¼€å§‹æ£€æŸ¥

  # ===== MinIOåˆå§‹åŒ–å®¹å™¨ï¼ˆä¸€æ¬¡æ€§ä»»åŠ¡ï¼‰ =====
  minio-init:
    image: minio/mc:latest
    depends_on:
      minio:
        condition: service_healthy
    restart: "no"  # åªè¿è¡Œä¸€æ¬¡ï¼Œä¸é‡å¯
    entrypoint: >
      /bin/sh -c "
      mc alias set myminio http://minio:9000 minioadmin minioadmin;
      mc mb myminio/lakehouse --ignore-existing;
      echo 'Buckets created';
      "
```

**å…³é”®æ¨¡å¼**ï¼š

1. **Health Check**ï¼šç¡®ä¿æœåŠ¡çœŸæ­£readyï¼ˆä¸æ˜¯just runningï¼‰
2. **Dependency Order**ï¼šæ­£ç¡®çš„å¯åŠ¨é¡ºåº
3. **Init Container**ï¼šä¸€æ¬¡æ€§åˆå§‹åŒ–ä»»åŠ¡
4. **Restart Policy**ï¼šåŒºåˆ†é•¿æœŸæœåŠ¡å’Œä¸€æ¬¡æ€§ä»»åŠ¡

---

## 4.3 Spark Streamingçš„å®¹é”™æœºåˆ¶

**æ ¸å¿ƒä»£ç **ï¼ˆ`streaming/spark/processor_with_minio.py`ï¼‰ï¼š

```python
# ===== Checkpointï¼šExactly-onceè¯­ä¹‰çš„å…³é”® =====
query = (
    df.writeStream
    .format("parquet")
    .outputMode("append")

    # ===== é‡ç‚¹ï¼šCheckpointä½ç½® =====
    .option("checkpointLocation", "s3a://lakehouse/checkpoints/bronze")
    # Checkpointå­˜å‚¨ï¼š
    #   - offsets/ï¼šKafkaæ¶ˆè´¹offsetï¼ˆä»å“ªé‡Œç»§ç»­ï¼‰
    #   - commits/ï¼šå·²æäº¤çš„batch ID
    #   - sources/ï¼šæ•°æ®æºçŠ¶æ€

    .option("path", "s3a://lakehouse/bronze/social_media/")
    .partitionBy("partition_date", "source")
    .trigger(processingTime='30 seconds')
    .start()
)

# ===== å®¹é”™åœºæ™¯æ¼”ç¤º =====
# T0: å¤„ç†batch 0 (offset 0-100)   â†’ æˆåŠŸï¼Œè®°å½•offset=100
# T1: å¤„ç†batch 1 (offset 101-200) â†’ æˆåŠŸï¼Œè®°å½•offset=200
# T2: å¤„ç†batch 2 (offset 201-300) â†’ å´©æºƒï¼
#
# é‡å¯åï¼š
# T3: è¯»å–checkpointï¼Œå‘ç°ä¸Šæ¬¡æˆåŠŸoffset=200
# T4: ä»offset=201ç»§ç»­å¤„ç† â†’ ä¸ä¼šé‡å¤ï¼Œä¸ä¼šä¸¢å¤±ï¼
```

**ä¸ºä»€ä¹ˆCheckpointå¾ˆé‡è¦ï¼Ÿ**

```
æ²¡æœ‰Checkpoint:
  Sparké‡å¯ â†’ ä»å¤´æ¶ˆè´¹Kafka â†’ é‡å¤å¤„ç† â†’ æ•°æ®é‡å¤ï¼

æœ‰Checkpoint:
  Sparké‡å¯ â†’ è¯»å–offset â†’ ä»æ–­ç‚¹ç»§ç»­ â†’ Exactly-onceï¼
```

---

## 4.4 Dashboardçš„çŠ¶æ€ç®¡ç†æ¨¡å¼

```python
# ===== æ¨¡å¼ï¼šSession State + Caching =====

# 1. Session Stateï¼šè·¨åˆ·æ–°ä¿å­˜çŠ¶æ€
if 'previous_count' not in st.session_state:
    st.session_state.previous_count = 0  # åˆå§‹åŒ–

# 2. è®¡ç®—å¢é‡
new_data_count = current_count - st.session_state.previous_count

# 3. æ›´æ–°çŠ¶æ€
if new_data_count != 0:
    st.session_state.previous_count = current_count
    st.success(f"ğŸ†• æ–°å¢ +{new_data_count} æ¡æ•°æ®ï¼")

# ===== æ¨¡å¼ï¼šCachingé¿å…é‡å¤è®¡ç®— =====
@st.cache_data(ttl=300)  # ç¼“å­˜5åˆ†é’Ÿ
def expensive_operation():
    # è¿™ä¸ªå‡½æ•°5åˆ†é’Ÿå†…åªæ‰§è¡Œä¸€æ¬¡
    return load_from_kafka()

# ===== æ¨¡å¼ï¼šå ä½ç¬¦åŠ¨æ€æ›´æ–° =====
countdown_placeholder = st.empty()

for remaining in range(60, 0, -1):
    # åŠ¨æ€æ›´æ–°åŒä¸€ä¸ªä½ç½®çš„å†…å®¹
    with countdown_placeholder:
        st.markdown(f"åˆ·æ–°å€’è®¡æ—¶: {remaining}ç§’")
    time.sleep(1)

st.rerun()  # åˆ·æ–°æ•´ä¸ªé¡µé¢
```

---

## 4.5 é”™è¯¯å¤„ç†å’Œé‡è¯•æ¨¡å¼

### Kafka Produceré‡è¯•

```python
from kafka import KafkaProducer
from kafka.errors import KafkaError
import time

class RobustKafkaProducer:
    def __init__(self):
        self.producer = KafkaProducer(
            bootstrap_servers='localhost:9092',
            retries=3,  # è‡ªåŠ¨é‡è¯•3æ¬¡
            acks='all',  # ç­‰å¾…æ‰€æœ‰å‰¯æœ¬ç¡®è®¤
            max_in_flight_requests_per_connection=1,  # ä¿è¯é¡ºåº
        )

    def send_with_retry(self, topic, data, max_retries=5):
        """å¸¦æŒ‡æ•°é€€é¿çš„é‡è¯•"""
        for attempt in range(max_retries):
            try:
                future = self.producer.send(topic, value=data)
                record_metadata = future.get(timeout=10)
                return True

            except KafkaError as e:
                if attempt == max_retries - 1:
                    # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼Œè®°å½•åˆ°DLQï¼ˆæ­»ä¿¡é˜Ÿåˆ—ï¼‰
                    self.send_to_dlq(data, str(e))
                    return False

                # æŒ‡æ•°é€€é¿ï¼š1ç§’ã€2ç§’ã€4ç§’ã€8ç§’...
                wait_time = 2 ** attempt
                print(f"âš ï¸ Retry {attempt+1}/{max_retries} after {wait_time}s")
                time.sleep(wait_time)

        return False

    def send_to_dlq(self, data, error):
        """æ­»ä¿¡é˜Ÿåˆ—ï¼šè®°å½•æ— æ³•å‘é€çš„æ¶ˆæ¯"""
        with open('logs/dlq.json', 'a') as f:
            f.write(json.dumps({
                'data': data,
                'error': error,
                'timestamp': datetime.now().isoformat()
            }) + '\n')
```

**å…³é”®æ¨¡å¼**ï¼š
- **è‡ªåŠ¨é‡è¯•**ï¼štransient errorsä¼šè‡ªåŠ¨æ¢å¤
- **æŒ‡æ•°é€€é¿**ï¼šé¿å…æ·¹æ²¡æœåŠ¡
- **æ­»ä¿¡é˜Ÿåˆ—**ï¼šè®°å½•æœ€ç»ˆå¤±è´¥çš„æ¶ˆæ¯ï¼Œäº‹åå¤„ç†

---

# Part 5: ç”Ÿäº§åŒ–è·¯å¾„å’Œæœªæ¥æ‰©å±•

> ä»MVPåˆ°ç”Ÿäº§çº§ç³»ç»Ÿçš„æ¼”è¿›è·¯å¾„

---

## 5.1 ç”Ÿäº§åŒ–Checklist

### Phase 1: MVPï¼ˆå½“å‰çŠ¶æ€ï¼‰

```
âœ… å·²å®Œæˆï¼š
  - å•æœºDocker Composeéƒ¨ç½²
  - åŸºæœ¬æ•°æ®æµï¼ˆé‡‡é›† â†’ Kafka â†’ Spark â†’ MinIOï¼‰
  - å®æ—¶Dashboard
  - æ—¥å¿—å’Œé”™è¯¯å¤„ç†

âŒ æœªå®Œæˆï¼ˆä½†å¯æ¥å—ï¼‰ï¼š
  - æ²¡æœ‰ç›‘æ§å‘Šè­¦
  - æ²¡æœ‰é«˜å¯ç”¨
  - æ²¡æœ‰æ•°æ®è´¨é‡æ£€æŸ¥
  - æ²¡æœ‰è‡ªåŠ¨åŒ–æµ‹è¯•
```

### Phase 2: å†…éƒ¨ç”Ÿäº§ï¼ˆ3-6ä¸ªæœˆï¼‰

```
ç›®æ ‡ï¼šç¨³å®šè¿è¡Œï¼Œå†…éƒ¨ä½¿ç”¨

å¿…é¡»å®Œæˆï¼š
  âœ… ç›‘æ§å’Œå‘Šè­¦
    - Prometheus + Grafana
    - å…³é”®æŒ‡æ ‡ï¼šæ¶ˆæ¯å»¶è¿Ÿã€å¤„ç†é€Ÿåº¦ã€é”™è¯¯ç‡

  âœ… é«˜å¯ç”¨
    - Kafkaé›†ç¾¤ï¼ˆ3èŠ‚ç‚¹ï¼‰
    - Spark on K8sï¼ˆè‡ªåŠ¨é‡å¯ï¼‰
    - MinIOå¤šèŠ‚ç‚¹ï¼ˆçº åˆ ç ï¼‰

  âœ… æ•°æ®è´¨é‡
    - SchemaéªŒè¯
    - å»é‡é€»è¾‘
    - å¼‚å¸¸æ•°æ®å‘Šè­¦

  âš ï¸ å¯é€‰ï¼š
    - CI/CD pipeline
    - è‡ªåŠ¨åŒ–æµ‹è¯•
```

### Phase 3: å¤–éƒ¨ç”Ÿäº§ï¼ˆ6-12ä¸ªæœˆï¼‰

```
ç›®æ ‡ï¼šä¼ä¸šçº§ç¨³å®šæ€§

å¿…é¡»å®Œæˆï¼š
  âœ… å®‰å…¨æ€§
    - APIè®¤è¯ï¼ˆOAuth2ï¼‰
    - æ•°æ®åŠ å¯†ï¼ˆä¼ è¾“å±‚TLSï¼Œå­˜å‚¨å±‚åŠ å¯†ï¼‰
    - å®¡è®¡æ—¥å¿—

  âœ… æ€§èƒ½ä¼˜åŒ–
    - Delta Lakeï¼ˆSilver/Goldå±‚ï¼‰
    - æŸ¥è¯¢ä¼˜åŒ–ï¼ˆZ-Orderingï¼‰
    - ç¼“å­˜ç­–ç•¥

  âœ… è¿ç»´è‡ªåŠ¨åŒ–
    - è‡ªåŠ¨æ‰©ç¼©å®¹
    - æ»šåŠ¨æ›´æ–°
    - å¤‡ä»½å’Œæ¢å¤

  âœ… SLAä¿è¯
    - 99.9%å¯ç”¨æ€§
    - <5åˆ†é’Ÿæ•°æ®å»¶è¿Ÿ
    - 24/7ç›‘æ§
```

---

## 5.2 æ‰©å±•æ–¹å‘

### 5.2.1 æ•°æ®æºæ‰©å±•

```python
# ===== æ–°å¢Blueskyæ•°æ®æºï¼ˆå·²éƒ¨åˆ†å®ç°ï¼‰ =====
# æ–‡ä»¶ï¼šdata_ingestion/bluesky/collector.py

# ===== æœªæ¥å¯æ‰©å±•ï¼š=====
# 1. HackerNews API
# 2. GitHub Trending
# 3. YouTubeè¯„è®º
# 4. Podcastè½¬å½•
# 5. Discord/Slackæ¶ˆæ¯ï¼ˆä¼ä¸šå†…éƒ¨ï¼‰

# ç»Ÿä¸€æ¥å£æ¨¡å¼ï¼š
class DataCollector(ABC):
    @abstractmethod
    def collect(self) -> List[Dict]:
        pass

    @abstractmethod
    def normalize(self, raw_data) -> Dict:
        """æ ‡å‡†åŒ–ä¸ºç»Ÿä¸€æ ¼å¼"""
        pass
```

### 5.2.2 åˆ†æå±‚æ‰©å±•ï¼ˆSilver/Goldï¼‰

```python
# ===== Silver Layer: æ•°æ®æ¸…æ´— =====
# è¾“å…¥ï¼šBronze (Parquet)
# è¾“å‡ºï¼šSilver (Delta Lake)
# é¢‘ç‡ï¼šæ¯å°æ—¶

bronze_df = spark.read.parquet("s3a://lakehouse/bronze/social_media/")

silver_df = (
    bronze_df
    .dropDuplicates(["post_id"])  # å»é‡
    .filter(col("text").isNotNull())  # è¿‡æ»¤ç©ºå€¼
    .withColumn("text_clean", clean_html_udf(col("text")))  # æ¸…æ´—HTML
    .withColumn("keywords", extract_keywords_udf(col("text_clean")))  # NLP
    .withColumn("sentiment", sentiment_analysis_udf(col("text_clean")))  # æƒ…æ„Ÿåˆ†æ
    .withColumn("language", detect_language_udf(col("text")))  # è¯­è¨€æ£€æµ‹
)

# å†™å…¥Delta Lake
(silver_df.write
 .format("delta")
 .mode("append")
 .partitionBy("partition_date", "source")
 .save("s3a://lakehouse/silver/posts"))

# ===== Gold Layer: èšåˆç»Ÿè®¡ =====
# è¾“å…¥ï¼šSilver (Delta Lake)
# è¾“å‡ºï¼šGold (Delta Lake)
# é¢‘ç‡ï¼šæ¯15åˆ†é’Ÿ

gold_df = (
    spark.read.format("delta").load("s3a://lakehouse/silver/posts")
    .groupBy(
        window(col("created_at"), "1 hour"),  # æŒ‰å°æ—¶èšåˆ
        col("source"),
        col("subreddit")
    )
    .agg(
        count("*").alias("post_count"),
        avg("sentiment").alias("avg_sentiment"),
        collect_list("keywords").alias("trending_keywords"),
        sum("engagement").alias("total_engagement")
    )
)

# å®æ—¶Dashboardå¯ä»¥ç›´æ¥æŸ¥è¯¢Goldå±‚
# SELECT * FROM gold.hourly_stats
# WHERE hour >= NOW() - INTERVAL 24 HOURS
```

### 5.2.3 ML/AIåŠŸèƒ½æ‰©å±•

```python
# ===== è¶‹åŠ¿é¢„æµ‹ï¼ˆProphetï¼‰ =====
from prophet import Prophet

# å†å²æ•°æ®
historical_data = spark.sql("""
    SELECT date, post_count
    FROM gold.hourly_stats
    WHERE source = 'reddit'
    AND date >= NOW() - INTERVAL 30 DAYS
""").toPandas()

# è®­ç»ƒProphetæ¨¡å‹
model = Prophet()
model.fit(historical_data)

# é¢„æµ‹æœªæ¥7å¤©
future = model.make_future_dataframe(periods=7*24, freq='H')
forecast = model.predict(future)

# ===== å¼‚å¸¸æ£€æµ‹ï¼ˆIsolationForestï¼‰ =====
from sklearn.ensemble import IsolationForest

# ç‰¹å¾å·¥ç¨‹
features = silver_df.select(
    "post_length",
    "engagement_rate",
    "keyword_count",
    "sentiment_score"
).toPandas()

# è®­ç»ƒå¼‚å¸¸æ£€æµ‹æ¨¡å‹
clf = IsolationForest(contamination=0.01)
anomalies = clf.fit_predict(features)

# æ ‡è®°å¼‚å¸¸å¸–å­ï¼ˆå¯èƒ½æ˜¯spamæˆ–botï¼‰
silver_df = silver_df.withColumn(
    "is_anomaly",
    when(col("anomaly_score") < -0.5, True).otherwise(False)
)
```

---

## 5.3 æ€§èƒ½ä¼˜åŒ–è·¯å¾„

### 5.3.1 Kafkaä¼˜åŒ–

```yaml
# ===== ç”Ÿäº§ç¯å¢ƒKafkaé…ç½® =====
kafka:
  # åˆ†åŒºæ•°ï¼šæ ¹æ®ååé‡è®¡ç®—
  # è§„åˆ™ï¼šåˆ†åŒºæ•° = ç›®æ ‡ååé‡ / å•åˆ†åŒºååé‡
  # ä¾‹å¦‚ï¼š10000 msg/s Ã· 100 msg/s/partition = 100 partitions
  num_partitions: 100

  # å‰¯æœ¬æ•°ï¼šè‡³å°‘3ï¼ˆä¿è¯å¯ç”¨æ€§ï¼‰
  replication_factor: 3

  # å‹ç¼©ï¼šèŠ‚çœå­˜å‚¨å’Œå¸¦å®½
  compression_type: lz4  # æ¯”gzipæ›´å¿«

  # Retentionï¼šæ ¹æ®ä¸šåŠ¡éœ€æ±‚
  retention_hours: 168  # 7å¤©
  retention_bytes: 1TB  # æˆ–æŒ‰å¤§å°é™åˆ¶

  # æ€§èƒ½è°ƒä¼˜
  log_segment_bytes: 1GB  # æ¯ä¸ªsegment 1GB
  log_flush_interval_messages: 10000  # æ¯1ä¸‡æ¡flushä¸€æ¬¡
```

### 5.3.2 Sparkä¼˜åŒ–

```python
# ===== Executoré…ç½®ä¼˜åŒ– =====
spark-submit \
  --executor-memory 4G \
  --executor-cores 4 \
  --num-executors 10 \
  --conf spark.sql.shuffle.partitions=200 \  # Shuffleåˆ†åŒºæ•°
  --conf spark.default.parallelism=200 \     # å¹¶è¡Œåº¦
  --conf spark.sql.adaptive.enabled=true \   # è‡ªé€‚åº”æŸ¥è¯¢æ‰§è¡Œ
  --conf spark.sql.adaptive.coalescePartitions.enabled=true \  # åˆå¹¶å°åˆ†åŒº
  processor.py

# ===== ä»£ç å±‚é¢ä¼˜åŒ– =====
# 1. é¿å…å®½ä¾èµ–ï¼ˆå°‘ç”¨groupByï¼‰
# 2. ä½¿ç”¨å¹¿æ’­å˜é‡ï¼ˆå°è¡¨joinï¼‰
# 3. ç¼“å­˜ä¸­é—´ç»“æœ
# 4. ä½¿ç”¨åˆ—å¼å­˜å‚¨ï¼ˆParquetï¼‰
```

### 5.3.3 MinIO/S3ä¼˜åŒ–

```
ä¼˜åŒ–ç­–ç•¥ï¼š

1. åˆ†åŒºç­–ç•¥
  âŒ åï¼špartition_date=2025-11-12/hour=12/minute=30/  ï¼ˆå¤ªç»†ï¼‰
  âœ… å¥½ï¼špartition_date=2025-11-12/source=reddit/      ï¼ˆå¹³è¡¡ï¼‰

2. æ–‡ä»¶å¤§å°
  âŒ åï¼š1000ä¸ª1MBæ–‡ä»¶  ï¼ˆåˆ—æ–‡ä»¶å¤ªå¤šï¼Œlistæ“ä½œæ…¢ï¼‰
  âœ… å¥½ï¼š10ä¸ª100MBæ–‡ä»¶  ï¼ˆå¹³è¡¡ï¼‰

3. S3 Multipart Uploadï¼ˆå¤§æ–‡ä»¶ï¼‰
  - æ–‡ä»¶ > 100MB ä½¿ç”¨multipart
  - æå‡ä¸Šä¼ é€Ÿåº¦å’Œå¯é æ€§

4. S3 Selectï¼ˆæŸ¥è¯¢ä¸‹æ¨ï¼‰
  - åœ¨S3å±‚è¿‡æ»¤æ•°æ®ï¼Œå‡å°‘ç½‘ç»œä¼ è¾“
  - æ”¯æŒParquetæ ¼å¼
```

---

## 5.4 æˆæœ¬ä¼˜åŒ–

### äº‘æˆæœ¬ä¼°ç®—ï¼ˆå‡è®¾AWSï¼‰

```
åœºæ™¯ï¼šæ¯å¤©1GBæ–°æ•°æ®ï¼Œä¿ç•™365å¤©

S3å­˜å‚¨ï¼š
  - 365GB Ã— $0.023/GB/æœˆ = $8.4/æœˆ

Kafka (MSK):
  - 3èŠ‚ç‚¹ Ã— kafka.m5.large Ã— $0.21/å°æ—¶ Ã— 24 Ã— 30 = $453/æœˆ

Spark (EMR):
  - æŒ‰éœ€è¿è¡Œï¼š4å°æ—¶/å¤© Ã— m5.xlarge Ã— $0.192/å°æ—¶ Ã— 30 = $92/æœˆ
  - Spotå®ä¾‹ï¼šå¯èŠ‚çœ70% = $27/æœˆ

æ€»æˆæœ¬ï¼š~$500/æœˆï¼ˆæŒ‰éœ€ï¼‰æˆ– ~$60/æœˆï¼ˆä¼˜åŒ–åï¼‰

ä¼˜åŒ–ç­–ç•¥ï¼š
  1. ä½¿ç”¨Spotå®ä¾‹ï¼ˆSparkï¼‰
  2. S3ç”Ÿå‘½å‘¨æœŸç­–ç•¥ï¼ˆæ—§æ•°æ®è½¬Glacierï¼‰
  3. KafkaæŒ‰éœ€æ‰©ç¼©å®¹
```

---

# Part 6: é™„å½•

---

## 6.1 ç«¯å£å’ŒURLæ¸…å•

| æœåŠ¡ | ç«¯å£ | URL | ç”¨é€” |
|------|------|-----|------|
| **Zookeeper** | 2181 | - | Kafkaåè°ƒæœåŠ¡ |
| **Kafka** | 9092 | - | æ¶ˆæ¯é˜Ÿåˆ—ï¼ˆå®¿ä¸»æœºï¼‰ |
| **Kafkaï¼ˆå®¹å™¨å†…ï¼‰** | 29092 | - | æ¶ˆæ¯é˜Ÿåˆ—ï¼ˆå®¹å™¨é—´ï¼‰ |
| **MinIO API** | 9000 | http://localhost:9000 | S3 API |
| **MinIO Console** | 9001 | http://localhost:9001 | Webç®¡ç†ç•Œé¢ |
| **Spark Master** | 8080 | http://localhost:8080 | é›†ç¾¤ç®¡ç†UI |
| **Spark Worker** | 8081 | http://localhost:8081 | WorkerçŠ¶æ€ |
| **Spark Application** | 4040 | http://localhost:4040 | ä½œä¸šç›‘æ§ |
| **Streamlit Dashboard** | 8501 | http://localhost:8501 | å®æ—¶Dashboard |

---

## 6.2 å¸¸ç”¨å‘½ä»¤é€ŸæŸ¥

### Dockerç®¡ç†

```bash
# å¯åŠ¨æ‰€æœ‰æœåŠ¡
docker-compose -f docker-compose-full.yml up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose -f docker-compose-full.yml logs -f kafka

# åœæ­¢æ‰€æœ‰æœåŠ¡
docker-compose -f docker-compose-full.yml down

# é‡å¯å•ä¸ªæœåŠ¡
docker-compose -f docker-compose-full.yml restart spark-master

# è¿›å…¥å®¹å™¨
docker exec -it kafka /bin/bash
```

### Kafkaå‘½ä»¤

```bash
# æŸ¥çœ‹topics
docker exec kafka kafka-topics --bootstrap-server localhost:9092 --list

# æŸ¥çœ‹æ¶ˆæ¯æ•°é‡
docker exec kafka kafka-run-class kafka.tools.GetOffsetShell \
  --broker-list localhost:9092 \
  --topic ai-social-raw

# æ¶ˆè´¹æ¶ˆæ¯ï¼ˆæœ€è¿‘10æ¡ï¼‰
docker exec -it kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic ai-social-raw \
  --from-beginning \
  --max-messages 10
```

### MinIOå‘½ä»¤

```bash
# åˆ—å‡ºbuckets
docker exec minio mc ls myminio/

# é€’å½’åˆ—å‡ºæ–‡ä»¶
docker exec minio mc ls --recursive myminio/lakehouse/bronze/

# æŸ¥çœ‹å¯¹è±¡è¯¦æƒ…
docker exec minio mc stat myminio/lakehouse/bronze/social_media/xxx.parquet

# ä¸‹è½½æ–‡ä»¶
docker exec minio mc cp myminio/lakehouse/bronze/social_media/xxx.parquet /tmp/
```

---

## 6.3 æ•…éšœæ’æŸ¥æŒ‡å—

### é—®é¢˜1ï¼šKafkaè¿æ¥å¤±è´¥

**ç—‡çŠ¶**ï¼š`KafkaConnectionError: Unable to bootstrap from localhost:9092`

**æ’æŸ¥æ­¥éª¤**ï¼š
```bash
# 1. æ£€æŸ¥Kafkaæ˜¯å¦è¿è¡Œ
docker ps | grep kafka

# 2. æ£€æŸ¥ç«¯å£æ˜¯å¦å¼€æ”¾
nc -zv localhost 9092

# 3. æŸ¥çœ‹Kafkaæ—¥å¿—
docker logs kafka | tail -50

# 4. æ£€æŸ¥Zookeeper
docker exec kafka kafka-broker-api-versions --bootstrap-server localhost:9092
```

**å¸¸è§åŸå› **ï¼š
- Zookeeperæœªå¯åŠ¨
- ç«¯å£è¢«å ç”¨
- é˜²ç«å¢™é˜»æŒ¡

### é—®é¢˜2ï¼šSparkä½œä¸šå¡ä½ä¸åŠ¨

**ç—‡çŠ¶**ï¼šSpark UIæ˜¾ç¤ºä½œä¸šåœ¨è¿è¡Œï¼Œä½†æ²¡æœ‰è¿›åº¦

**æ’æŸ¥æ­¥éª¤**ï¼š
```bash
# 1. æŸ¥çœ‹Sparkæ—¥å¿—
docker logs spark-master

# 2. æ£€æŸ¥Kafkaæ˜¯å¦æœ‰æ•°æ®
docker exec kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic ai-social-raw \
  --max-messages 1

# 3. æ£€æŸ¥Checkpointï¼ˆå¯èƒ½æŸåï¼‰
docker exec minio mc ls --recursive myminio/lakehouse/checkpoints/
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# åˆ é™¤Checkpointï¼Œä»å¤´å¼€å§‹ï¼ˆæ³¨æ„ï¼šä¼šé‡å¤å¤„ç†æ•°æ®ï¼‰
docker exec minio mc rm --recursive --force myminio/lakehouse/checkpoints/bronze/
```

### é—®é¢˜3ï¼šDashboardæ˜¾ç¤º"No data available"

**æ’æŸ¥æ­¥éª¤**ï¼š
```bash
# 1. æ£€æŸ¥Kafkaæ˜¯å¦æœ‰æ¶ˆæ¯
docker exec kafka kafka-run-class kafka.tools.GetOffsetShell \
  --broker-list localhost:9092 \
  --topic ai-social-raw

# 2. æ£€æŸ¥é‡‡é›†å™¨æ˜¯å¦è¿è¡Œ
cat logs/reddit.pid
ps aux | grep collector

# 3. æŸ¥çœ‹é‡‡é›†å™¨æ—¥å¿—
tail -50 logs/reddit_collector.log

# 4. æ‰‹åŠ¨æµ‹è¯•Kafkaè¯»å–
python dashboard/kafka_reader.py
```

---

## 6.4 å­¦ä¹ èµ„æº

### å®˜æ–¹æ–‡æ¡£

- **Kafka**: https://kafka.apache.org/documentation/
- **Spark**: https://spark.apache.org/docs/latest/
- **MinIO**: https://min.io/docs/minio/linux/index.html
- **Delta Lake**: https://docs.delta.io/
- **Streamlit**: https://docs.streamlit.io/

### æ¨èä¹¦ç±

1. **ã€ŠDesigning Data-Intensive Applicationsã€‹** (Martin Kleppmann)
   - æ•°æ®ç³»ç»Ÿè®¾è®¡çš„åœ£ç»
   - æ·±å…¥è®²è§£åˆ†å¸ƒå¼ç³»ç»ŸåŸç†

2. **ã€ŠStreaming Systemsã€‹** (Tyler Akidau et al.)
   - æµå¤„ç†ç³»ç»Ÿçš„æƒå¨æŒ‡å—
   - Googleå·¥ç¨‹å¸ˆå†™çš„

3. **ã€ŠLearning Sparkã€‹** (Jules S. Damji et al.)
   - Sparkå®˜æ–¹æ¨è
   - 2nd Edition (2020) æœ€æ–°

### åœ¨çº¿è¯¾ç¨‹

- **Coursera**: "Big Data Specialization" (UC San Diego)
- **Udemy**: "Apache Kafka Series - Learn Apache Kafka for Beginners"
- **DataCamp**: "Streaming Data with Apache Kafka and Spark"

---

## 6.5 é¡¹ç›®æ–‡ä»¶ç»“æ„

```
Lakehouse_ai_pipeline/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ .env                          # ç¯å¢ƒå˜é‡ï¼ˆAPI keysï¼‰
â”‚
â”œâ”€â”€ data_ingestion/
â”‚   â”œâ”€â”€ kafka_producer.py             # Kafkaç”Ÿäº§è€…å°è£…
â”‚   â”œâ”€â”€ reddit/
â”‚   â”‚   â””â”€â”€ collector.py              # Reddité‡‡é›†å™¨
â”‚   â”œâ”€â”€ twitter/
â”‚   â”‚   â””â”€â”€ collector.py              # Twitteré‡‡é›†å™¨ï¼ˆæš‚åœï¼‰
â”‚   â””â”€â”€ bluesky/
â”‚       â””â”€â”€ collector.py              # Blueskyé‡‡é›†å™¨ï¼ˆæ–°å¢ï¼‰
â”‚
â”œâ”€â”€ streaming/
â”‚   â””â”€â”€ spark/
â”‚       â”œâ”€â”€ processor_with_minio.py   # Sparkæµå¤„ç†ä¸»ç¨‹åº
â”‚       â””â”€â”€ jars/                     # Sparkä¾èµ–jaråŒ…
â”‚
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ app_realtime.py               # Streamlit Dashboard
â”‚   â””â”€â”€ kafka_reader.py               # Kafkaè¯»å–å·¥å…·
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ 00-start_all.sh               # ä¸€é”®å¯åŠ¨æ‰€æœ‰æœåŠ¡
â”‚   â”œâ”€â”€ 01-start_collectors.sh        # å¯åŠ¨é‡‡é›†å™¨
â”‚   â”œâ”€â”€ 02-start_spark_minio.sh       # å¯åŠ¨Sparkä½œä¸š
â”‚   â”œâ”€â”€ 03-start_dashboard.sh         # å¯åŠ¨Dashboard
â”‚   â””â”€â”€ 99-stop_all.sh                # åœæ­¢æ‰€æœ‰æœåŠ¡
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ TECH_ARCHITECTURE_DEEP_DIVE.md  # æœ¬æ–‡æ¡£
â”‚   â”œâ”€â”€ TECH_STACK_EXPLAINED.md       # æŠ€æœ¯æ ˆç®€ä»‹
â”‚   â””â”€â”€ DASHBOARD_IMPROVEMENTS_SUMMARY.md
â”‚
â”œâ”€â”€ logs/                             # æ—¥å¿—ç›®å½•
â”‚   â”œâ”€â”€ reddit_collector.log
â”‚   â”œâ”€â”€ bluesky_collector.log
â”‚   â””â”€â”€ spark_streaming.log
â”‚
â”œâ”€â”€ storage/                          # æœ¬åœ°å­˜å‚¨ï¼ˆåºŸå¼ƒï¼Œæ”¹ç”¨MinIOï¼‰
â”‚
â”œâ”€â”€ docker-compose-full.yml           # å®Œæ•´æœåŠ¡ç¼–æ’
â”œâ”€â”€ requirements.txt                  # Pythonä¾èµ–
â””â”€â”€ README.md                         # é¡¹ç›®è¯´æ˜
```

---

## 6.6 Glossaryï¼ˆæœ¯è¯­è¡¨ï¼‰

| æœ¯è¯­ | è§£é‡Š | ç¤ºä¾‹ |
|------|------|------|
| **Lakehouse** | æ•°æ®æ¹– + æ•°æ®ä»“åº“çš„ç»“åˆ | Delta Lake on S3 |
| **Bronze/Silver/Gold** | æ•°æ®åˆ†å±‚ï¼ˆåŸå§‹/æ¸…æ´—/èšåˆï¼‰ | Bronze = åŸå§‹Parquet |
| **ACID** | äº‹åŠ¡ç‰¹æ€§ï¼ˆåŸå­æ€§ã€ä¸€è‡´æ€§ã€éš”ç¦»æ€§ã€æŒä¹…æ€§ï¼‰ | Delta Lakeæ”¯æŒ |
| **Exactly-once** | ç²¾ç¡®ä¸€æ¬¡è¯­ä¹‰ï¼ˆä¸é‡å¤ã€ä¸ä¸¢å¤±ï¼‰ | Kafka + Spark Checkpoint |
| **Micro-batch** | å°æ‰¹é‡å¤„ç†ï¼ˆSpark Streamingæ¨¡å¼ï¼‰ | æ¯30ç§’ä¸€ä¸ªæ‰¹æ¬¡ |
| **Checkpoint** | æ£€æŸ¥ç‚¹ï¼ˆå®¹é”™æ¢å¤ï¼‰ | s3a://lakehouse/checkpoints/ |
| **Partition Pruning** | åˆ†åŒºå‰ªæï¼ˆæŸ¥è¯¢ä¼˜åŒ–ï¼‰ | åªè¯»éœ€è¦çš„åˆ†åŒº |
| **Z-Ordering** | Z-Orderæ’åºï¼ˆDelta Lakeä¼˜åŒ–ï¼‰ | æŒ‰å¸¸ç”¨åˆ—æ’åºæ•°æ® |
| **Rate Limit** | APIé™æµ | Twitter 500K/æœˆ |
| **DLQ** | æ­»ä¿¡é˜Ÿåˆ—ï¼ˆDead Letter Queueï¼‰ | å­˜å‚¨å¤±è´¥æ¶ˆæ¯ |

---

## 6.7 æ€»ç»“

æœ¬æ–‡æ¡£æ¶µç›–äº†AIè¶‹åŠ¿ç›‘æ§ç³»ç»Ÿçš„ï¼š

1. âœ… **å…¨æ™¯è§†å›¾**ï¼šæ•°æ®ç”Ÿå‘½å‘¨æœŸã€æ¶æ„åˆ†å±‚
2. âœ… **æŠ€æœ¯æ·±åº¦**ï¼šæ¯ä¸ªç»„ä»¶çš„å·¥ä½œåŸç†å’Œé…ç½®
3. âœ… **æ¶æ„å†³ç­–**ï¼šä¸ºä»€ä¹ˆé€‰æ‹©è¿™äº›æŠ€æœ¯
4. âœ… **ä»£ç è§£æ**ï¼šå…³é”®å®ç°å’Œæœ€ä½³å®è·µ
5. âœ… **ç”Ÿäº§è·¯å¾„**ï¼šMVPåˆ°ä¼ä¸šçº§çš„æ¼”è¿›
6. âœ… **å®ç”¨å·¥å…·**ï¼šå‘½ä»¤é€ŸæŸ¥ã€æ•…éšœæ’æŸ¥

**ä¸‹ä¸€æ­¥å»ºè®®**ï¼š

- **åˆå­¦è€…**ï¼šä»Part 1å¼€å§‹ï¼Œç†è§£å…¨å±€æ¶æ„
- **å¼€å‘è€…**ï¼šé‡ç‚¹çœ‹Part 2å’ŒPart 4ï¼Œç†è§£ä»£ç å®ç°
- **æ¶æ„å¸ˆ**ï¼šé‡ç‚¹çœ‹Part 3ï¼Œç†è§£æŠ€æœ¯é€‰å‹
- **è¿ç»´äººå‘˜**ï¼šé‡ç‚¹çœ‹Part 5å’ŒPart 6ï¼Œç†è§£éƒ¨ç½²å’Œè¿ç»´

**æŒç»­æ›´æ–°**ï¼š
æœ¬æ–‡æ¡£ä¼šéšç€é¡¹ç›®æ¼”è¿›æŒç»­æ›´æ–°ï¼Œæœ€æ–°ç‰ˆæœ¬è¯·æŸ¥çœ‹GitHubä»“åº“ã€‚

---

**æ–‡æ¡£å®Œæˆæ—¶é—´**: 2025-11-12
**ç‰ˆæœ¬**: 1.0
**ä½œè€…**: AI Pipeline Team
**åé¦ˆ**: æ¬¢è¿æIssueæˆ–PR

---

