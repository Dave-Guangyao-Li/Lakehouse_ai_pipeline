# AIè¶‹åŠ¿ç›‘æ§ç³»ç»Ÿ - æŠ€æœ¯æ¶æ„æ·±åº¦è§£æ

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0
**æœ€åæ›´æ–°**: 2025-11-12
**ç›®æ ‡è¯»è€…**: æœ‰ç¼–ç¨‹åŸºç¡€ï¼Œæƒ³æ·±å…¥ç†è§£æ•°æ®å·¥ç¨‹æ¶æ„é€‰å‹
**é˜…è¯»æ—¶é—´**: æ ¸å¿ƒå†…å®¹15-20åˆ†é’Ÿï¼Œç²¾è¯»å…¨æ–‡30-45åˆ†é’Ÿ

---

## ç›®å½•

- [Part 1: å…¨æ™¯è§†å›¾](#part-1-å…¨æ™¯è§†å›¾)
- [Part 2: æ ¸å¿ƒæŠ€æœ¯æ·±åº¦è§£æ](#part-2-æ ¸å¿ƒæŠ€æœ¯æ·±åº¦è§£æ)
- [Part 3: æ¶æ„å†³ç­–æ·±åº¦å‰–æ](#part-3-æ¶æ„å†³ç­–æ·±åº¦å‰–æ)
- [Part 4: å…³é”®ä»£ç æ·±åº¦è§£æ](#part-4-å…³é”®ä»£ç æ·±åº¦è§£æ)
- [Part 5: ç”Ÿäº§åŒ–è·¯å¾„å’Œæœªæ¥æ‰©å±•](#part-5-ç”Ÿäº§åŒ–è·¯å¾„å’Œæœªæ¥æ‰©å±•)
- [Part 6: é™„å½•](#part-6-é™„å½•)

---

# Part 1: å…¨æ™¯è§†å›¾

## 30ç§’ç”µæ¢¯æ¼”è®²

**è¿™ä¸ªç³»ç»Ÿæ˜¯ä»€ä¹ˆï¼Ÿ**

ä¸€ä¸ª**å®æ—¶AIè¶‹åŠ¿ç›‘æ§ç³»ç»Ÿ**ï¼Œä»Twitterå’ŒReddité‡‡é›†AIç›¸å…³è®¨è®ºï¼Œé€šè¿‡æµå¼å¤„ç†pipelineå®æ—¶åˆ†æï¼Œå­˜å‚¨åˆ°æ•°æ®æ¹–ï¼Œæœ€ååœ¨Dashboardå¯è§†åŒ–å±•ç¤ºçƒ­é—¨è¯é¢˜å’Œè¶‹åŠ¿ã€‚

**æŠ€æœ¯ç‰¹ç‚¹ï¼Ÿ**

- âš¡ **å®æ—¶å¤„ç†**ï¼šæ•°æ®ä»é‡‡é›†åˆ°å±•ç¤ºå»¶è¿Ÿ < 60ç§’
- ğŸ—ï¸ **Lakehouseæ¶æ„**ï¼šç»“åˆæ•°æ®æ¹–çš„çµæ´»æ€§å’Œæ•°æ®ä»“åº“çš„äº‹åŠ¡æ€§
- ğŸ“Š **æµæ‰¹ä¸€ä½“**ï¼šåŒæ—¶æ”¯æŒå®æ—¶æµå¤„ç†å’Œæ‰¹é‡åˆ†æ
- ğŸ”§ **å®¹å™¨åŒ–éƒ¨ç½²**ï¼šDocker Composeä¸€é”®å¯åŠ¨ï¼Œæ˜“äºå¼€å‘å’Œæµ‹è¯•
- ğŸš€ **å¯æ‰©å±•è®¾è®¡**ï¼šæ¯ä¸ªç»„ä»¶éƒ½å¯ä»¥ç‹¬ç«‹æ‰©å±•åˆ°ç”Ÿäº§ç¯å¢ƒ

**æŠ€æœ¯æ ˆç²¾åï¼Ÿ**

```
æ•°æ®é‡‡é›†: Python + Tweepy/PRAW
æ¶ˆæ¯é˜Ÿåˆ—: Kafka (é«˜åå)
æµå¤„ç†: Spark Streaming (åˆ†å¸ƒå¼)
å­˜å‚¨: MinIO (S3å…¼å®¹) + Delta Lake (ACIDäº‹åŠ¡)
å¯è§†åŒ–: Streamlit (å¿«é€ŸåŸå‹)
```

---

## æ•°æ®ç”Ÿå‘½å‘¨æœŸå¯è§†åŒ–

### å®Œæ•´æ•°æ®æ—…ç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           æ•°æ®çš„ä¸€ç”Ÿï¼ˆLife of Dataï¼‰                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ç¬¬1ç«™: æ•°æ®æº (Data Sources)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    ğŸ¦ Twitter API          ğŸ¤– Reddit API
         â”‚                        â”‚
         â”œâ”€ æ¨æ–‡å†…å®¹              â”œâ”€ å¸–å­æ ‡é¢˜
         â”œâ”€ ä½œè€…ä¿¡æ¯              â”œâ”€ å¸–å­æ­£æ–‡
         â”œâ”€ äº’åŠ¨æ•°æ®              â”œâ”€ Subreddit
         â””â”€ æ—¶é—´æˆ³                â””â”€ è¯„åˆ†æ•°æ®
              â”‚                        â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“

ç¬¬2ç«™: æ•°æ®é‡‡é›†å±‚ (Data Ingestion)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
         Pythoné‡‡é›†å™¨ (60ç§’/æ¬¡)
              â”‚
              â”œâ”€ APIè°ƒç”¨ (tweepy/praw)
              â”œâ”€ æ•°æ®æ¸…æ´— (å»é™¤HTMLæ ‡ç­¾)
              â”œâ”€ æ ¼å¼æ ‡å‡†åŒ– (JSON)
              â””â”€ æ·»åŠ å…ƒæ•°æ® (source, timestamp)
              â”‚
              â†“

ç¬¬3ç«™: æ¶ˆæ¯é˜Ÿåˆ— (Message Queue)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
         Apache Kafka
         Topic: ai-social-raw
              â”‚
              â”œâ”€ åˆ†åŒºå­˜å‚¨ (Partitioned)
              â”œâ”€ æŒä¹…åŒ–7å¤© (Retention: 7 days)
              â”œâ”€ é¡ºåºä¿è¯ (Ordering guaranteed)
              â””â”€ é«˜åå (10K+ msg/s capable)
              â”‚
              â†“

ç¬¬4ç«™: æµå¤„ç†å±‚ (Stream Processing)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
         Spark Streaming
              â”‚
              â”œâ”€ è¯»å–Kafkaæ¶ˆæ¯ (micro-batch: 30s)
              â”œâ”€ è§£æJSON (Schema validation)
              â”œâ”€ æ•°æ®è½¬æ¢ (Transform)
              â”œâ”€ åˆ†åŒº (Partition by date & source)
              â””â”€ å†™å…¥å­˜å‚¨ (Write to MinIO)
              â”‚
              â†“

ç¬¬5ç«™: å­˜å‚¨å±‚ (Storage Layer)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
         MinIO + Delta Lake
              â”‚
         Bronze Layer (Raw Data)
              â”œâ”€ æ ¼å¼: Parquet (åˆ—å¼å‹ç¼©)
              â”œâ”€ åˆ†åŒº: partition_date=YYYY-MM-DD/source=reddit|twitter
              â”œâ”€ ä¿ç•™: å®Œæ•´åŸå§‹æ•°æ®
              â””â”€ ç”¨é€”: æ•°æ®å›æº¯ã€é‡æ–°å¤„ç†
              â”‚
         [æœªæ¥] Silver Layer (Cleaned Data)
              â”œâ”€ å»é‡ (Deduplication by post_id)
              â”œâ”€ æ ‡å‡†åŒ– (Schema standardization)
              â””â”€ å…³é”®è¯æå– (NLP processing)
              â”‚
         [æœªæ¥] Gold Layer (Aggregated Data)
              â”œâ”€ æŒ‰å°æ—¶èšåˆ (Hourly aggregation)
              â””â”€ è¶‹åŠ¿è®¡ç®— (Trend scoring)
              â”‚
              â†“

ç¬¬6ç«™: åˆ†æå±‚ (Analytics Layer)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
         DuckDB / Spark SQL
              â”‚
              â”œâ”€ OLAPæŸ¥è¯¢ (Analytical queries)
              â”œâ”€ ç»Ÿè®¡åˆ†æ (Statistical analysis)
              â””â”€ æ•°æ®èšåˆ (Data aggregation)
              â”‚
              â†“

ç¬¬7ç«™: å¯è§†åŒ–å±‚ (Visualization Layer)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
         Streamlit Dashboard
              â”‚
              â”œâ”€ å®æ—¶è¯»å–Kafka (Direct Kafka read)
              â”œâ”€ NLPå…³é”®è¯æå– (spaCy)
              â”œâ”€ è¯äº‘ç”Ÿæˆ (WordCloud)
              â”œâ”€ äº¤äº’å¼å›¾è¡¨ (Plotly)
              â””â”€ è‡ªåŠ¨åˆ·æ–° (60s auto-refresh)
              â”‚
              â†“

ç»ˆç‚¹ç«™: ç”¨æˆ·ç•Œé¢ (User Interface)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    http://localhost:8501
         â”‚
         â”œâ”€ ğŸ“Š Overview: æ•°æ®æºåˆ†å¸ƒ
         â”œâ”€ ğŸ”¥ Trending Keywords: çƒ­é—¨è¯é¢˜è¯äº‘
         â””â”€ ğŸ“ Recent Posts: Reddité£æ ¼å¡ç‰‡

```

### å…³é”®æ€§èƒ½æŒ‡æ ‡

| æŒ‡æ ‡ | å½“å‰å€¼ | è¯´æ˜ |
|------|--------|------|
| **ç«¯åˆ°ç«¯å»¶è¿Ÿ** | < 60ç§’ | ä»APIé‡‡é›†åˆ°Dashboardæ˜¾ç¤º |
| **æ•°æ®é‡‡é›†é¢‘ç‡** | 60ç§’/æ¬¡ | Reddité‡‡é›†é—´éš”ï¼ˆé¿å…é™æµï¼‰ |
| **Sparkæ‰¹æ¬¡é—´éš”** | 30ç§’ | micro-batchå¤„ç†å‘¨æœŸ |
| **Dashboardåˆ·æ–°** | 60ç§’ | è‡ªåŠ¨åˆ·æ–°é—´éš” |
| **Kafkaååé‡** | ~100 msg/s (å®é™…) | 10K+ msg/s (ç†è®º) |
| **æ•°æ®ä¿ç•™** | Kafka: 7å¤©<br>MinIO: æ— é™ | å¯é…ç½® |

---

## æ¶æ„åˆ†å±‚å›¾

### 6å±‚æ¶æ„è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 6: å±•ç¤ºå±‚ (Presentation Layer)                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Streamlit Dashboard (http://localhost:8501)               â”‚ â”‚
â”‚  â”‚  - å®æ—¶æ•°æ®å±•ç¤º                                              â”‚ â”‚
â”‚  â”‚  - NLPå…³é”®è¯æå–                                             â”‚ â”‚
â”‚  â”‚  - äº¤äº’å¼å¯è§†åŒ–                                              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†‘ â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 5: åˆ†æå±‚ (Analytics Layer)                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  DuckDB (OLAP)                 Spark SQL                    â”‚ â”‚
â”‚  â”‚  - Ad-hocæŸ¥è¯¢                   - å¤§è§„æ¨¡æ•°æ®åˆ†æ              â”‚ â”‚
â”‚  â”‚  - è½»é‡çº§æœ¬åœ°åˆ†æ               - åˆ†å¸ƒå¼è®¡ç®—                 â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†‘ â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 4: å­˜å‚¨å±‚ (Storage Layer)                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  MinIO (S3-compatible Object Storage)                       â”‚ â”‚
â”‚  â”‚  + Delta Lake (ACID Transactions)                           â”‚ â”‚
â”‚  â”‚                                                              â”‚ â”‚
â”‚  â”‚  Bronze â”€â”€â†’ [Raw Data]       (Parquet)                      â”‚ â”‚
â”‚  â”‚  Silver â”€â”€â†’ [Cleaned Data]   (Delta) [æœªæ¥]                 â”‚ â”‚
â”‚  â”‚  Gold   â”€â”€â†’ [Aggregated]     (Delta) [æœªæ¥]                 â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†‘ â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 3: æµå¤„ç†å±‚ (Stream Processing Layer)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Apache Spark Streaming                                     â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚ â”‚
â”‚  â”‚  â”‚ Master   â”‚  â”‚ Worker-1 â”‚  â”‚ Worker-N â”‚                 â”‚ â”‚
â”‚  â”‚  â”‚ :8080    â”‚  â”‚ :8081    â”‚  â”‚          â”‚                 â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚ â”‚
â”‚  â”‚  - Structured Streaming API                                 â”‚ â”‚
â”‚  â”‚  - Micro-batch Processing (30s)                             â”‚ â”‚
â”‚  â”‚  - Exactly-once Semantics                                   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†‘ â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 2: æ¶ˆæ¯é˜Ÿåˆ—å±‚ (Message Queue Layer)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Apache Kafka (:9092)                                       â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚  â”‚  â”‚  Topic: ai-social-raw                                 â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  Partitions: 1 (å•èŠ‚ç‚¹)                               â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  Replication: 1                                       â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  Retention: 7 days                                    â”‚  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â”‚  â”‚                                                              â”‚ â”‚
â”‚  â”‚  Zookeeper (:2181) - Kafkaåè°ƒæœåŠ¡                          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†‘ â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 1: æ•°æ®é‡‡é›†å±‚ (Data Ingestion Layer)                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Python Collectors (æ¯60ç§’é‡‡é›†ä¸€æ¬¡)                          â”‚ â”‚
â”‚  â”‚                                                              â”‚ â”‚
â”‚  â”‚  twitter/collector.py  â¸ï¸ (æš‚åœ)                            â”‚ â”‚
â”‚  â”‚  â”œâ”€ Tweepy (Twitter API v2)                                â”‚ â”‚
â”‚  â”‚  â””â”€ KafkaProducer                                           â”‚ â”‚
â”‚  â”‚                                                              â”‚ â”‚
â”‚  â”‚  reddit/collector.py  âœ… (è¿è¡Œä¸­)                            â”‚ â”‚
â”‚  â”‚  â”œâ”€ PRAW (Reddit API)                                      â”‚ â”‚
â”‚  â”‚  â”œâ”€ Target: r/MachineLearning, r/LocalLLaMA               â”‚ â”‚
â”‚  â”‚  â””â”€ KafkaProducer                                           â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 0: æ•°æ®æº (Data Sources)                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  ğŸ¦ Twitter API v2        ğŸ¤– Reddit API                     â”‚ â”‚
â”‚  â”‚  - Bearer Tokenè®¤è¯       - OAuth2 (script mode)            â”‚ â”‚
â”‚  â”‚  - Essential access       - Read-only                       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å®¹å™¨åŒ–åŸºç¡€è®¾æ–½

æ‰€æœ‰æœåŠ¡è¿è¡Œåœ¨Dockerå®¹å™¨ä¸­ï¼Œé€šè¿‡`docker-compose-full.yml`ç¼–æ’ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Docker Network: lakehouse-network (bridge)                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  zookeeper:2181                                       â”‚  â”‚
â”‚  â”‚  â”œâ”€ Volume: zookeeper-data                           â”‚  â”‚
â”‚  â”‚  â””â”€ Volume: zookeeper-logs                           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  kafka:9092, :29092                                   â”‚  â”‚
â”‚  â”‚  â”œâ”€ Depends on: zookeeper                            â”‚  â”‚
â”‚  â”‚  â””â”€ Volume: kafka-data                               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  minio:9000 (API), :9001 (Console)                    â”‚  â”‚
â”‚  â”‚  â”œâ”€ Volume: minio-data                               â”‚  â”‚
â”‚  â”‚  â””â”€ Buckets: lakehouse, bronze, silver, gold        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  spark-master:8080 (UI), :7077 (Port)                â”‚  â”‚
â”‚  â”‚  â””â”€ Volume: spark-logs                               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  spark-worker:8081                                    â”‚  â”‚
â”‚  â”‚  â”œâ”€ Depends on: spark-master                         â”‚  â”‚
â”‚  â”‚  â”œâ”€ Cores: 2                                         â”‚  â”‚
â”‚  â”‚  â”œâ”€ Memory: 2G                                       â”‚  â”‚
â”‚  â”‚  â””â”€ Volume: spark-worker-logs                        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å®¿ä¸»æœºPythonè¿›ç¨‹:
â”œâ”€ data_ingestion/reddit/collector.py (PID saved to logs/reddit.pid)
â”œâ”€ streaming/spark/processor_with_minio.py (Spark Submit)
â””â”€ dashboard/app_realtime.py (Streamlit)
```

---

## ä¸ºä»€ä¹ˆé€‰æ‹©è¿™ä¸ªæ¶æ„ï¼Ÿ

### æ ¸å¿ƒè®¾è®¡åŸåˆ™

#### 1. **Lambdaæ¶æ„å˜ä½“**ï¼ˆæµæ‰¹ä¸€ä½“ï¼‰

ä¼ ç»ŸLambdaæ¶æ„æœ‰ä¸¤æ¡è·¯å¾„ï¼š
- **Batch Layer**ï¼šå¤„ç†å†å²å…¨é‡æ•°æ®ï¼ˆæ…¢ä½†å‡†ç¡®ï¼‰
- **Speed Layer**ï¼šå¤„ç†å®æ—¶å¢é‡æ•°æ®ï¼ˆå¿«ä½†å¯èƒ½ä¸å®Œæ•´ï¼‰

æˆ‘ä»¬çš„ç®€åŒ–ç‰ˆæœ¬ï¼š
- **å®æ—¶è·¯å¾„**ï¼šKafka â†’ Spark Streaming â†’ MinIOï¼ˆBronzeï¼‰
- **æ‰¹å¤„ç†è·¯å¾„**ï¼ˆæœªæ¥ï¼‰ï¼šMinIO Bronze â†’ Spark Batch â†’ Silver/Gold

**ä¼˜åŠ¿**ï¼š
- âœ… æ—¢èƒ½å®æ—¶çœ‹åˆ°æœ€æ–°æ•°æ®ï¼ˆDashboardè¯»Kafkaï¼‰
- âœ… åˆèƒ½å­˜å‚¨å†å²æ•°æ®è¿›è¡Œå¤æ‚åˆ†æï¼ˆMinIOæŒä¹…åŒ–ï¼‰
- âœ… å¦‚æœå®æ—¶å¤„ç†å‡ºé—®é¢˜ï¼Œå¯ä»¥ä»Bronzeå±‚é‡æ–°å¤„ç†

#### 2. **Lakehouseæ¶æ„**ï¼ˆæ•°æ®æ¹– + æ•°æ®ä»“åº“ï¼‰

**ä¼ ç»Ÿé€‰æ‹©**ï¼š
- **æ•°æ®æ¹–**ï¼ˆå¦‚HDFSï¼‰ï¼šå­˜å‚¨çµæ´»ï¼Œä½†ç¼ºä¹äº‹åŠ¡ä¿è¯
- **æ•°æ®ä»“åº“**ï¼ˆå¦‚Snowflakeï¼‰ï¼šäº‹åŠ¡æ€§å¼ºï¼Œä½†å­˜å‚¨æ˜‚è´µä¸”ä¸çµæ´»

**Lakehouseç»“åˆä¸¤è€…ä¼˜åŠ¿**ï¼š
- ğŸ“¦ MinIOï¼šæä¾›ä¾¿å®œçš„å¯¹è±¡å­˜å‚¨ï¼ˆç±»ä¼¼S3ï¼‰
- ğŸ”’ Delta Lakeï¼šåœ¨å¯¹è±¡å­˜å‚¨ä¹‹ä¸Šæä¾›ACIDäº‹åŠ¡
- ğŸ“Š Bronze/Silver/Goldï¼šæ•°æ®åˆ†å±‚ç®¡ç†ï¼Œé€æ­¥æå‡æ•°æ®è´¨é‡

#### 3. **å®¹å™¨åŒ–å¼€å‘ç¯å¢ƒ**

**ä¸ºä»€ä¹ˆç”¨Docker Composeè€Œä¸æ˜¯Kubernetesï¼Ÿ**

**å¼€å‘é˜¶æ®µéœ€æ±‚**ï¼š
- âœ… å¿«é€Ÿå¯åŠ¨/åœæ­¢
- âœ… æœ¬åœ°è¿è¡Œä¸éœ€è¦äº‘èµ„æº
- âœ… æ˜“äºè°ƒè¯•å’Œä¿®æ”¹é…ç½®
- âœ… èµ„æºå ç”¨å¯æ§ï¼ˆç¬”è®°æœ¬ç”µè„‘èƒ½è·‘ï¼‰

Docker Composeå®Œç¾æ»¡è¶³è¿™äº›éœ€æ±‚ï¼Œè€ŒKuberneteså¯¹äºå•æœºå¼€å‘æ¥è¯´è¿‡äºé‡é‡çº§ã€‚

**ç”Ÿäº§ç¯å¢ƒå†è¿ç§»åˆ°K8s**æ—¶ï¼Œå®¹å™¨åŒ–çš„å¥½å¤„ï¼š
- é…ç½®å·²ç»å®¹å™¨åŒ–ï¼Œåªéœ€ç¼–å†™K8s manifest
- åº”ç”¨ä»£ç æ— éœ€ä¿®æ”¹
- å¯ä»¥é€ä¸ªæœåŠ¡è¿ç§»

#### 4. **è§£è€¦è®¾è®¡**ï¼ˆæ¾è€¦åˆæ¶æ„ï¼‰

æ¯ä¸ªç»„ä»¶éƒ½å¯ä»¥ç‹¬ç«‹æ›¿æ¢ï¼š
- é‡‡é›†å™¨ï¼šå¯ä»¥æ¢æˆå…¶ä»–æ•°æ®æºï¼ˆHackerNewsã€Podcastç­‰ï¼‰
- Kafkaï¼šå¯ä»¥æ¢æˆPulsarã€Redis Streams
- Sparkï¼šå¯ä»¥æ¢æˆFlinkã€Storm
- MinIOï¼šå¯ä»¥æ¢æˆAWS S3ã€Azure Blob
- Dashboardï¼šå¯ä»¥æ¢æˆGrafanaã€Tableau

è¿™ç§è®¾è®¡è®©ç³»ç»Ÿéå¸¸çµæ´»ï¼Œæ–¹ä¾¿å­¦ä¹ å’Œè¿­ä»£ã€‚

---

**æ¥ä¸‹æ¥**ï¼š[Part 2: æ ¸å¿ƒæŠ€æœ¯æ·±åº¦è§£æ](#part-2-æ ¸å¿ƒæŠ€æœ¯æ·±åº¦è§£æ) å°†è¯¦ç»†è§£é‡Šæ¯ä¸ªæŠ€æœ¯çš„å·¥ä½œåŸç†ã€æ¶æ„é€‰å‹åŸå› ã€ä»¥åŠåœ¨é¡¹ç›®ä¸­çš„å…·ä½“ä½¿ç”¨ã€‚

---

# Part 2: æ ¸å¿ƒæŠ€æœ¯æ·±åº¦è§£æ

> æŒ‰ç…§æ•°æ®æµçš„é¡ºåºï¼Œé€å±‚æ·±å…¥è®²è§£æ¯ä¸ªæŠ€æœ¯

---

## 2.1 æ•°æ®é‡‡é›†å±‚ (Data Ingestion Layer)

### Pythoné‡‡é›†å™¨æ¡†æ¶

#### ä¸€å¥è¯ä»‹ç»
**ç”¨Pythonè„šæœ¬å®šæœŸè°ƒç”¨ç¤¾äº¤åª’ä½“APIï¼Œå°†æ•°æ®æ ‡å‡†åŒ–åå‘é€åˆ°Kafkaã€‚**

#### ä¸ºä»€ä¹ˆéœ€è¦é‡‡é›†å±‚ï¼Ÿ

**é—®é¢˜**ï¼šç¤¾äº¤åª’ä½“APIå„æœ‰å„çš„æ ¼å¼ã€é™æµè§„åˆ™ã€è®¤è¯æ–¹å¼ã€‚

**è§£å†³**ï¼šç»Ÿä¸€çš„é‡‡é›†å™¨æ¡†æ¶ï¼š
- ğŸ”„ å®šæœŸé‡‡é›†ï¼ˆscheduleåº“ï¼‰
- ğŸ”Œ ç»Ÿä¸€æ¥å£ï¼ˆKafka Producerï¼‰
- ğŸ›¡ï¸ é”™è¯¯å¤„ç†ï¼ˆé‡è¯•ã€æ—¥å¿—ï¼‰
- â±ï¸ é™æµæ§åˆ¶ï¼ˆé¿å…API banï¼‰

#### æŠ€æœ¯é€‰å‹ï¼šTweepy (Twitter) vs PRAW (Reddit)

| ç‰¹æ€§ | Tweepy | PRAW |
|------|--------|------|
| **APIç‰ˆæœ¬** | Twitter API v2 | Reddit API (OAuth2) |
| **è®¤è¯æ–¹å¼** | Bearer Token | Client ID + Secret |
| **é€Ÿç‡é™åˆ¶** | ä¸¥æ ¼ï¼ˆ500K tweets/æœˆï¼‰ | å®½æ¾ï¼ˆ60æ¬¡/åˆ†é’Ÿï¼‰ |
| **æ•°æ®ç»“æ„** | å¤æ‚çš„åµŒå¥—JSON | æ‰å¹³åŒ–å¯¹è±¡ |
| **æ˜“ç”¨æ€§** | â­â­â­ | â­â­â­â­â­ |
| **æ–‡æ¡£è´¨é‡** | å®˜æ–¹è¯¦ç»† | ç¤¾åŒºä¼˜ç§€ |

**ä¸ºä»€ä¹ˆåŒæ—¶ç”¨ä¸¤ä¸ªï¼Ÿ**
- Twitterï¼šè¦†ç›–ä¸»æµç¤¾äº¤åª’ä½“
- Redditï¼šæŠ€æœ¯è®¨è®ºç¤¾åŒºï¼ˆæ›´æ·±å…¥ï¼‰

---

### Tweepyæ·±åº¦è§£æ

#### å·¥ä½œåŸç†ï¼ˆç™½è¯ï¼‰

Tweepyæ˜¯Twitter APIçš„Pythonå°è£…ï¼Œæƒ³è±¡æˆï¼š
- **ä½ **ï¼šPythonè„šæœ¬
- **Tweepy**ï¼šç¿»è¯‘å™¨
- **Twitter**ï¼šæ•°æ®åº“

ä½ ç”¨Pythonè¯´"ç»™æˆ‘æœ€è¿‘çš„AIç›¸å…³æ¨æ–‡"ï¼ŒTweepyç¿»è¯‘æˆTwitter APIèƒ½ç†è§£çš„HTTPè¯·æ±‚ï¼Œç„¶åæŠŠè¿”å›çš„JSONç¿»è¯‘å›Pythonå¯¹è±¡ã€‚

#### é¡¹ç›®ä¸­çš„å…·ä½“ä½¿ç”¨

**æ–‡ä»¶**ï¼š`data_ingestion/twitter/collector.py`

```python
# ç¬¬1æ­¥ï¼šåˆå§‹åŒ–å®¢æˆ·ç«¯
client = tweepy.Client(bearer_token=TWITTER_BEARER_TOKEN)

# ç¬¬2æ­¥ï¼šå®šä¹‰æœç´¢æŸ¥è¯¢
query = '(AI OR "artificial intelligence" OR "machine learning") lang:en -is:retweet'
# è§£é‡Šï¼š
# - AIç›¸å…³å…³é”®è¯
# - lang:enï¼šåªè¦è‹±æ–‡
# - -is:retweetï¼šæ’é™¤è½¬å‘

# ç¬¬3æ­¥ï¼šè°ƒç”¨API
tweets = client.search_recent_tweets(
    query=query,
    max_results=100,  # æ¯æ¬¡æœ€å¤š100æ¡
    tweet_fields=['created_at', 'public_metrics', 'author_id'],
    expansions=['author_id'],
    user_fields=['username']
)

# ç¬¬4æ­¥ï¼šè§£ææ•°æ®
for tweet in tweets.data:
    data = {
        'id': tweet.id,
        'text': tweet.text,
        'created_at': tweet.created_at.isoformat(),
        'author_username': get_author_username(tweet, tweets.includes),
        'metrics': {
            'likes': tweet.public_metrics['like_count'],
            'retweets': tweet.public_metrics['retweet_count']
        }
    }

    # ç¬¬5æ­¥ï¼šå‘é€åˆ°Kafka
    producer.send_tweet(data)
```

#### é…ç½®è¦ç‚¹

**ç¯å¢ƒå˜é‡**ï¼ˆ`config/.env`ï¼‰ï¼š
```bash
TWITTER_BEARER_TOKEN=AAAAAAAA...  # ä»Twitter Developer Portalè·å–
```

**é‡è¦å‚æ•°**ï¼š
- `max_results=100`ï¼šAPIé™åˆ¶çš„å•æ¬¡æœ€å¤§è¿”å›æ•°
- `query`å¤æ‚åº¦ï¼šå¤ªå¤æ‚ä¼šé™ä½åŒ¹é…ç‡ï¼Œå¤ªç®€å•ä¼šæœ‰å™ªéŸ³

#### å¸¸è§å‘ç‚¹

1. **Rate Limitï¼ˆé™æµï¼‰**
   - **ç—‡çŠ¶**ï¼š`429 Too Many Requests`
   - **åŸå› **ï¼šè¶…è¿‡APIé…é¢ï¼ˆEssential access: 500K tweets/æœˆï¼‰
   - **è§£å†³**ï¼š
     - å¢åŠ é‡‡é›†é—´éš”ï¼ˆ600ç§’ â†’ 3600ç§’ï¼‰
     - å‡çº§åˆ°Elevated accessï¼ˆ2M tweets/æœˆï¼‰
     - æœ¬é¡¹ç›®æš‚æ—¶ç¦ç”¨Twitterï¼Œåªç”¨Reddit

2. **Tokenæƒé™ä¸è¶³**
   - **ç—‡çŠ¶**ï¼š`401 Unauthorized`
   - **åŸå› **ï¼šä½¿ç”¨äº†API Keyè€Œä¸æ˜¯Bearer Token
   - **è§£å†³**ï¼šåœ¨Twitter Developer Portalæ£€æŸ¥"Keys and Tokens"ï¼Œä½¿ç”¨Bearer Token

---

### PRAWæ·±åº¦è§£æ

#### å·¥ä½œåŸç†ï¼ˆç™½è¯ï¼‰

PRAW = **P**ython **R**eddit **A**PI **W**rapper

æƒ³è±¡Redditæ˜¯ä¸€ä¸ªå·¨å¤§çš„å›¾ä¹¦é¦†ï¼š
- **Subreddit**ï¼šä¸åŒä¸»é¢˜çš„ä¹¦æ¶ï¼ˆr/MachineLearningã€r/LocalLLaMAï¼‰
- **Post**ï¼šä¹¦æ¶ä¸Šçš„ä¹¦
- **PRAW**ï¼šå›¾ä¹¦ç®¡ç†å‘˜ï¼Œå¸®ä½ æ‰¾ä¹¦

#### é¡¹ç›®ä¸­çš„å…·ä½“ä½¿ç”¨

**æ–‡ä»¶**ï¼š`data_ingestion/reddit/collector.py`

```python
# ç¬¬1æ­¥ï¼šåˆå§‹åŒ–PRAWå®¢æˆ·ç«¯
reddit = praw.Reddit(
    client_id=REDDIT_CLIENT_ID,        # åº”ç”¨ID
    client_secret=REDDIT_CLIENT_SECRET, # åº”ç”¨å¯†é’¥
    user_agent=REDDIT_USER_AGENT       # æ ‡è¯†ä½ çš„åº”ç”¨ï¼ˆå¦‚"AI_Trend_Monitor/1.0"ï¼‰
)

# ç¬¬2æ­¥ï¼šå®šä¹‰ç›®æ ‡Subreddits
TARGET_SUBREDDITS = [
    'MachineLearning',  # å­¦æœ¯è®¨è®º
    'LocalLLaMA',       # æœ¬åœ°LLMè¿è¡Œ
    'artificial',       # AIé€šç”¨è®¨è®º
]

# ç¬¬3æ­¥ï¼šé‡‡é›†æ–°å¸–å­
for subreddit_name in TARGET_SUBREDDITS:
    subreddit = reddit.subreddit(subreddit_name)

    # è·å–æœ€æ–°çš„10ç¯‡å¸–å­
    for post in subreddit.new(limit=10):
        data = {
            'id': post.id,
            'title': post.title,
            'text': post.selftext,  # å¸–å­æ­£æ–‡
            'author': str(post.author),
            'subreddit': subreddit_name,
            'created_utc': post.created_utc,
            'metrics': {
                'score': post.score,      # èµ-è¸©
                'comments': post.num_comments
            }
        }

        # ç¬¬4æ­¥ï¼šå‘é€åˆ°Kafka
        producer.send_reddit_post(data)
```

#### å…³é”®æ¦‚å¿µè§£é‡Š

**1. OAuth2 Script Mode**

Reddit APIæœ‰å¤šç§è®¤è¯æ–¹å¼ï¼Œæˆ‘ä»¬ç”¨çš„æ˜¯**Scriptæ¨¡å¼**ï¼š
- âœ… é€‚åˆï¼šä¸ªäººè„šæœ¬ã€åå°ä»»åŠ¡
- âœ… æ— éœ€ç”¨æˆ·äº¤äº’ï¼ˆä¸éœ€è¦æµè§ˆå™¨æˆæƒï¼‰
- âŒ é™åˆ¶ï¼šåªèƒ½è¯»å–å…¬å¼€æ•°æ®ï¼ˆå¤Ÿç”¨äº†ï¼‰

**2. User Agent**

Redditè¦æ±‚æ¯ä¸ªåº”ç”¨æä¾›User Agentæ ‡è¯†ï¼š
```python
user_agent = "AI_Trend_Monitor/1.0 by /u/your_reddit_username"
```

**æ ¼å¼**ï¼š`<App Name>/<Version> by /u/<Reddit Username>`

**ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ**
- Redditç”¨æ¥è¯†åˆ«æµé‡æ¥æº
- å¦‚æœUser Agentä¸è§„èŒƒï¼Œå¯èƒ½è¢«é™æµ

**3. Rate Limitï¼ˆé™æµï¼‰**

Reddit APIé™åˆ¶ï¼š
- **60æ¬¡è¯·æ±‚/åˆ†é’Ÿ**ï¼ˆæ¯”Twitterå®½æ¾å¾ˆå¤šï¼‰
- æˆ‘ä»¬æ¯60ç§’é‡‡é›†ä¸€æ¬¡ï¼Œå®Œå…¨å®‰å…¨

#### é…ç½®è¦ç‚¹

**ç¯å¢ƒå˜é‡**ï¼ˆ`config/.env`ï¼‰ï¼š
```bash
REDDIT_CLIENT_ID=abcd1234efgh          # åº”ç”¨ID
REDDIT_CLIENT_SECRET=xyz789secret      # åº”ç”¨å¯†é’¥
REDDIT_USER_AGENT=AI_Trend_Monitor/1.0 # è‡ªå®šä¹‰æ ‡è¯†
```

**å¦‚ä½•è·å–ï¼Ÿ**
1. è®¿é—® https://www.reddit.com/prefs/apps
2. ç‚¹å‡»"Create App" â†’ é€‰æ‹©**"script"**ç±»å‹
3. redirect_uriå¡«`http://localhost:8080`ï¼ˆå¿…å¡«ä½†ä¸ä¼šç”¨ï¼‰
4. è®°å½•Client IDï¼ˆappåç§°ä¸‹æ–¹ï¼‰å’ŒSecret

---

### Kafka Producerï¼ˆå‘é€æ•°æ®åˆ°Kafkaï¼‰

#### ä¸€å¥è¯ä»‹ç»
**Pythonç¨‹åºé€šè¿‡kafka-pythonåº“ï¼Œå°†é‡‡é›†çš„æ•°æ®å‘é€åˆ°Kafka topicã€‚**

#### å·¥ä½œåŸç†ï¼ˆç™½è¯ï¼‰

æƒ³è±¡Kafkaæ˜¯ä¸€ä¸ª**é‚®å±€**ï¼š
- **Topic**ï¼šä¸åŒçš„é‚®ç®±ï¼ˆæˆ‘ä»¬çš„æ˜¯`ai-social-raw`ï¼‰
- **Producer**ï¼šå¯„ä¿¡äººï¼ˆæˆ‘ä»¬çš„é‡‡é›†å™¨ï¼‰
- **Message**ï¼šä¿¡ä»¶ï¼ˆJSONæ ¼å¼çš„æ•°æ®ï¼‰
- **Key**ï¼šä¿¡å°ä¸Šçš„ç¼–å·ï¼ˆç”¨äºåˆ†åŒºï¼Œæˆ‘ä»¬ç”¨post_idï¼‰

ProduceræŠŠæ•°æ®å°è£…æˆä¿¡ä»¶ï¼Œæ‰”åˆ°é‚®ç®±é‡Œï¼Œç„¶åå°±ä¸ç®¡äº†ï¼ˆå¼‚æ­¥å‘é€ï¼‰ã€‚

#### é¡¹ç›®ä¸­çš„å…·ä½“ä½¿ç”¨

**æ–‡ä»¶**ï¼š`data_ingestion/kafka_producer.py`

```python
from kafka import KafkaProducer
import json

class SocialMediaProducer:
    def __init__(self):
        # åˆ›å»ºç”Ÿäº§è€…
        self.producer = KafkaProducer(
            # ===== è¿æ¥é…ç½® =====
            bootstrap_servers='localhost:9092',  # Kafkaåœ°å€

            # ===== åºåˆ—åŒ–é…ç½® =====
            # å°†Python dictè½¬æ¢ä¸ºJSON bytes
            value_serializer=lambda v: json.dumps(v).encode('utf-8'),

            # å°†post_idè½¬æ¢ä¸ºbytesï¼ˆç”¨äºåˆ†åŒºï¼‰
            key_serializer=lambda k: k.encode('utf-8') if k else None,

            # ===== å¯é æ€§é…ç½® =====
            acks='all',  # ç­‰å¾…æ‰€æœ‰å‰¯æœ¬ç¡®è®¤ï¼ˆæœ€é«˜å¯é æ€§ï¼‰
            # acks=0  # ä¸ç­‰å¾…ç¡®è®¤ï¼ˆæœ€å¿«ï¼Œä½†å¯èƒ½ä¸¢æ•°æ®ï¼‰
            # acks=1  # åªç­‰å¾…leaderç¡®è®¤ï¼ˆæŠ˜ä¸­ï¼‰

            retries=3,  # å¤±è´¥é‡è¯•3æ¬¡

            # ===== é¡ºåºä¿è¯é…ç½® =====
            max_in_flight_requests_per_connection=1,
            # åŒæ—¶åªå‘é€ä¸€ä¸ªè¯·æ±‚ï¼Œä¿è¯æ¶ˆæ¯é¡ºåº

            # ===== å‹ç¼©é…ç½® =====
            compression_type='gzip'  # å‹ç¼©æ•°æ®ï¼ŒèŠ‚çœå¸¦å®½
        )

    def send_reddit_post(self, post_data: dict):
        """å‘é€Redditå¸–å­åˆ°Kafka"""

        # æ„é€ æ¶ˆæ¯æ ¼å¼
        message = {
            'source': 'reddit',             # æ•°æ®æ¥æº
            'data': post_data,              # åŸå§‹æ•°æ®
            'timestamp': post_data.get('created_utc')  # æ—¶é—´æˆ³
        }

        # ä½¿ç”¨post_idä½œä¸ºkeyï¼ˆä¿è¯åŒä¸€å¸–å­çš„æ›´æ–°æœ‰åºï¼‰
        post_id = post_data.get('id')

        # å‘é€åˆ°Kafka
        future = self.producer.send(
            'ai-social-raw',  # topicåç§°
            value=message,    # æ¶ˆæ¯å†…å®¹
            key=str(post_id)  # åˆ†åŒºkey
        )

        # ç­‰å¾…å‘é€å®Œæˆï¼ˆåŒæ­¥ç¡®è®¤ï¼‰
        record_metadata = future.get(timeout=10)

        print(f"âœ… Sent to partition {record_metadata.partition}, "
              f"offset {record_metadata.offset}")
```

#### å…³é”®é…ç½®è§£æ

##### 1. `acks='all'`ï¼ˆå¯é æ€§vsæ€§èƒ½æƒè¡¡ï¼‰

| é…ç½® | å«ä¹‰ | å»¶è¿Ÿ | å¯é æ€§ | ä½¿ç”¨åœºæ™¯ |
|------|------|------|--------|----------|
| `acks=0` | ä¸ç­‰ç¡®è®¤ | æœ€ä½ | âŒ å¯èƒ½ä¸¢æ•°æ® | æ—¥å¿—é‡‡é›†ã€ç‚¹å‡»æµ |
| `acks=1` | Leaderç¡®è®¤ | ä¸­ç­‰ | âš ï¸ LeaderæŒ‚äº†å¯èƒ½ä¸¢ | ä¸€èˆ¬ä¸šåŠ¡æ•°æ® |
| `acks='all'` | æ‰€æœ‰å‰¯æœ¬ç¡®è®¤ | æœ€é«˜ | âœ… æœ€å¯é  | é‡‘èäº¤æ˜“ã€å…³é”®æ•°æ® |

**æˆ‘ä»¬é€‰æ‹©`acks='all'`**ï¼š
- ç¤¾äº¤åª’ä½“æ•°æ®è™½ç„¶ä¸æ˜¯é‡‘èçº§ï¼Œä½†ä¸¢äº†å¾ˆéš¾æ¢å¤
- æˆ‘ä»¬é‡‡é›†é¢‘ç‡æ˜¯60ç§’ï¼Œå»¶è¿Ÿä¸æ•æ„Ÿ

##### 2. `max_in_flight_requests_per_connection=1`ï¼ˆé¡ºåºä¿è¯ï¼‰

**é—®é¢˜åœºæ™¯**ï¼š
```
T1: å‘é€æ¶ˆæ¯Aï¼ˆoffset=100ï¼‰
T2: å‘é€æ¶ˆæ¯Bï¼ˆoffset=101ï¼‰
T3: Aå¤±è´¥ï¼Œé‡è¯•
T4: BæˆåŠŸå†™å…¥offset=101
T5: Aé‡è¯•æˆåŠŸå†™å…¥offset=102

ç»“æœï¼šBåœ¨Aå‰é¢ï¼é¡ºåºé”™ä¹±ï¼
```

**è§£å†³**ï¼šè®¾ç½®=1ï¼ŒåŒä¸€æ—¶é—´åªæœ‰ä¸€ä¸ªè¯·æ±‚åœ¨é€”ï¼š
```
T1: å‘é€æ¶ˆæ¯A
T2: ç­‰å¾…Aå®Œæˆ...
T3: AæˆåŠŸ
T4: å‘é€æ¶ˆæ¯B
ç»“æœï¼šé¡ºåºæ­£ç¡®ï¼
```

**ä»£ä»·**ï¼šååé‡é™ä½ï¼ˆä½†æˆ‘ä»¬ä¸éœ€è¦é«˜ååï¼‰

##### 3. `compression_type='gzip'`ï¼ˆå‹ç¼©ï¼‰

ç¤¾äº¤åª’ä½“æ–‡æœ¬æ•°æ®å‹ç¼©æ¯”å¾ˆé«˜ï¼š
- åŸå§‹JSONï¼š~2KB/æ¡
- Gzipå‹ç¼©ï¼š~500B/æ¡ï¼ˆå‹ç¼©æ¯”75%ï¼‰

**æƒè¡¡**ï¼š
- âœ… èŠ‚çœç½‘ç»œå¸¦å®½
- âœ… èŠ‚çœKafkaå­˜å‚¨ç©ºé—´
- âŒ CPUå¼€é”€å¢åŠ ï¼ˆä½†Pythoné‡‡é›†å™¨CPUé—²ç€ï¼‰

---

## 2.2 æ¶ˆæ¯é˜Ÿåˆ—å±‚ (Message Queue Layer)

### Apache Kafkaæ·±åº¦è§£æ

#### ä¸€å¥è¯ä»‹ç»
**Kafkaæ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼çš„ã€é«˜ååé‡çš„æ¶ˆæ¯é˜Ÿåˆ—ï¼Œä¸“ä¸ºæµå¼æ•°æ®è®¾è®¡ã€‚**

#### ä¸ºä»€ä¹ˆéœ€è¦æ¶ˆæ¯é˜Ÿåˆ—ï¼Ÿ

**æ²¡æœ‰æ¶ˆæ¯é˜Ÿåˆ—çš„é—®é¢˜**ï¼š

```
é‡‡é›†å™¨ â”€â”€â”€â”€â”€â”€ç›´æ¥å†™å…¥â”€â”€â”€â”€â”€â”€â†’ Spark Streaming
   â”‚                            â”‚
   â”œâ”€â”€ å¦‚æœSparkæŒ‚äº†ï¼Ÿ          â”œâ”€â”€ æ•°æ®ä¸¢å¤±ï¼
   â”œâ”€â”€ Sparkå¤„ç†æ…¢ï¼Ÿ            â”œâ”€â”€ é‡‡é›†å™¨è¢«é˜»å¡ï¼
   â””â”€â”€ å¤šä¸ªæ¶ˆè´¹è€…ï¼Ÿ             â””â”€â”€ æ— æ³•å®ç°ï¼
```

**æœ‰Kafkaçš„æ¶æ„**ï¼š

```
é‡‡é›†å™¨ â†’ Kafkaï¼ˆç¼“å†²ï¼‰ â†’ Spark
   â”‚        â”‚            â”‚
   âœ…        âœ…            âœ…
   å¿«é€Ÿ      æŒä¹…åŒ–        æ…¢æ…¢å¤„ç†
   å®Œæˆ      7å¤©          å¯é‡å¯
```

**Kafkaè§£å†³çš„æ ¸å¿ƒé—®é¢˜**ï¼š
1. **è§£è€¦**ï¼šç”Ÿäº§è€…å’Œæ¶ˆè´¹è€…äº’ä¸å½±å“
2. **ç¼“å†²**ï¼šç”Ÿäº§é€Ÿåº¦ â‰  æ¶ˆè´¹é€Ÿåº¦
3. **æŒä¹…åŒ–**ï¼šæ•°æ®å†™å…¥ç£ç›˜ï¼Œä¸æ€•ä¸¢
4. **é‡æ”¾**ï¼šå¯ä»¥ä»ä»»æ„offseté‡æ–°æ¶ˆè´¹
5. **æ‰©å±•**ï¼šæ”¯æŒå¤šä¸ªæ¶ˆè´¹è€…å¹¶è¡Œå¤„ç†

#### Kafkaæ ¸å¿ƒæ¦‚å¿µï¼ˆç™½è¯è§£é‡Šï¼‰

##### 1. Topicï¼ˆä¸»é¢˜ï¼‰= æ•°æ®çš„åˆ†ç±»

æƒ³è±¡Kafkaæ˜¯ä¸€ä¸ªå›¾ä¹¦é¦†ï¼š
- **Topic**ï¼šä¹¦æ¶çš„æ ‡ç­¾ï¼ˆ"AIç›¸å…³"ã€"è´¢ç»æ–°é—»"ï¼‰
- æˆ‘ä»¬çš„topicï¼š`ai-social-raw`ï¼ˆAIç¤¾äº¤åª’ä½“åŸå§‹æ•°æ®ï¼‰

##### 2. Partitionï¼ˆåˆ†åŒºï¼‰= ä¹¦æ¶çš„æ ¼å­

ä¸€ä¸ªtopicå¯ä»¥åˆ†æˆå¤šä¸ªpartitionï¼š
```
Topic: ai-social-raw
â”œâ”€ Partition 0: [msg1, msg2, msg5, ...]
â”œâ”€ Partition 1: [msg3, msg6, msg7, ...]
â””â”€ Partition 2: [msg4, msg8, msg9, ...]
```

**ä¸ºä»€ä¹ˆåˆ†åŒºï¼Ÿ**
- **å¹¶è¡Œå¤„ç†**ï¼šå¤šä¸ªæ¶ˆè´¹è€…å¯ä»¥åŒæ—¶è¯»ä¸åŒåˆ†åŒº
- **æ‰©å±•æ€§**ï¼šåˆ†åŒºå¯ä»¥åˆ†å¸ƒåœ¨ä¸åŒæœºå™¨
- **é¡ºåºä¿è¯**ï¼šåŒä¸€åˆ†åŒºå†…æ¶ˆæ¯æœ‰åº

**åˆ†åŒºè§„åˆ™**ï¼š
```python
# å¦‚æœæŒ‡å®šäº†keyï¼ˆå¦‚post_idï¼‰
partition = hash(key) % num_partitions

# å¦‚æœæ²¡æœ‰key
partition = round_robin  # è½®è¯¢åˆ†é…
```

**æˆ‘ä»¬çš„é…ç½®**ï¼šå•åˆ†åŒºï¼ˆå› ä¸ºæ˜¯å•æœºç¯å¢ƒï¼‰

##### 3. Offsetï¼ˆåç§»é‡ï¼‰= ä¹¦çš„é¡µç 

æ¯æ¡æ¶ˆæ¯åœ¨åˆ†åŒºä¸­æœ‰å”¯ä¸€çš„offsetï¼š
```
Partition 0:
[offset=0] â†’ æ¶ˆæ¯A
[offset=1] â†’ æ¶ˆæ¯B
[offset=2] â†’ æ¶ˆæ¯C
```

**æ¶ˆè´¹è€…è®°ä½offset**ï¼Œä¸‹æ¬¡ä»æ–­ç‚¹ç»§ç»­è¯»ï¼š
```python
# æ¶ˆè´¹è€…Aè¯»åˆ°offset=1000
# æ¶ˆè´¹è€…é‡å¯åï¼Œä»offset=1001ç»§ç»­
```

##### 4. Consumer Groupï¼ˆæ¶ˆè´¹è€…ç»„ï¼‰= è¯»ä¹¦å°ç»„

```
Topic: ai-social-raw (3ä¸ªåˆ†åŒº)
â”œâ”€ Partition 0 â†’ Consumer 1 (Group: spark-processor)
â”œâ”€ Partition 1 â†’ Consumer 2 (Group: spark-processor)
â””â”€ Partition 2 â†’ Consumer 3 (Group: spark-processor)

åŒæ—¶ï¼š
â”œâ”€ Partition 0 â†’ Consumer A (Group: dashboard)
â”œâ”€ Partition 1 â†’ Consumer B (Group: dashboard)
â””â”€ Partition 2 â†’ Consumer C (Group: dashboard)
```

**åŒä¸€Groupå†…**ï¼š
- æ¯ä¸ªåˆ†åŒºåªè¢«ä¸€ä¸ªæ¶ˆè´¹è€…è¯»ï¼ˆé¿å…é‡å¤ï¼‰
- æ¶ˆè´¹è€…æ•° â‰¤ åˆ†åŒºæ•°ï¼ˆå¤šäº†ä¹Ÿæ²¡ç”¨ï¼‰

**ä¸åŒGroupé—´**ï¼š
- å„è‡ªç‹¬ç«‹æ¶ˆè´¹ï¼ˆäº’ä¸å½±å“ï¼‰
- Dashboardå’ŒSparkå¯ä»¥åŒæ—¶è¯»åŒä¸€æ•°æ®

#### é¡¹ç›®ä¸­çš„Kafkaé…ç½®

**Docker Composeé…ç½®**ï¼ˆ`docker-compose-full.yml:19-45`ï¼‰ï¼š

```yaml
kafka:
  image: confluentinc/cp-kafka:7.5.0
  container_name: kafka
  hostname: kafka
  depends_on:
    - zookeeper
  ports:
    - "9092:9092"    # å®¿ä¸»æœºè®¿é—®ç«¯å£
    - "29092:29092"  # å®¹å™¨é—´è®¿é—®ç«¯å£
  environment:
    KAFKA_BROKER_ID: 1  # Brokerå”¯ä¸€ID

    # Zookeeperè¿æ¥
    KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181

    # ç›‘å¬å™¨é…ç½®ï¼ˆé‡è¦ï¼ï¼‰
    KAFKA_ADVERTISED_LISTENERS: >
      PLAINTEXT://kafka:29092,        # å®¹å™¨é—´é€šä¿¡
      PLAINTEXT_HOST://localhost:9092 # å®¿ä¸»æœºé€šä¿¡

    KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: >
      PLAINTEXT:PLAINTEXT,
      PLAINTEXT_HOST:PLAINTEXT

    KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT

    # Topicé…ç½®
    KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1  # å•èŠ‚ç‚¹ï¼Œå‰¯æœ¬æ•°=1
    KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"    # è‡ªåŠ¨åˆ›å»ºtopic

    # æ•°æ®ä¿ç•™
    KAFKA_LOG_RETENTION_HOURS: 168  # 7å¤©ï¼ˆ7*24=168å°æ—¶ï¼‰
```

#### ç›‘å¬å™¨é…ç½®è¯¦è§£ï¼ˆé‡è¦å‘ç‚¹ï¼‰

**é—®é¢˜**ï¼šä¸ºä»€ä¹ˆéœ€è¦ä¸¤ä¸ªç›‘å¬å™¨ï¼Ÿ

```
åœºæ™¯1: Sparkå®¹å™¨è®¿é—®Kafka
  Spark â†’ kafka:29092 âœ… (å®¹å™¨åè§£æ)

åœºæ™¯2: å®¿ä¸»æœºPythonè„šæœ¬è®¿é—®Kafka
  Python â†’ localhost:9092 âœ… (ç«¯å£æ˜ å°„)

å¦‚æœåªé…ç½®ä¸€ä¸ªï¼Ÿ
  Spark â†’ localhost:9092 âŒ (localhostæŒ‡å‘Sparkè‡ªå·±)
  Python â†’ kafka:29092 âŒ (æ— æ³•è§£ækafkaå®¹å™¨å)
```

**è§£å†³**ï¼šä¸¤ä¸ªç›‘å¬å™¨ï¼Œå„å¸å…¶èŒï¼š
- `PLAINTEXT://kafka:29092`ï¼šå®¹å™¨é—´é€šä¿¡
- `PLAINTEXT_HOST://localhost:9092`ï¼šå®¿ä¸»æœºé€šä¿¡

#### Kafkaå¸¸ç”¨å‘½ä»¤

```bash
# 1. æŸ¥çœ‹æ‰€æœ‰topics
docker exec kafka kafka-topics --bootstrap-server localhost:9092 --list

# 2. æŸ¥çœ‹topicè¯¦æƒ…
docker exec kafka kafka-topics --bootstrap-server localhost:9092 \
  --describe --topic ai-social-raw

# 3. æ¶ˆè´¹æ¶ˆæ¯ï¼ˆä»å¤´å¼€å§‹ï¼‰
docker exec -it kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic ai-social-raw \
  --from-beginning \
  --max-messages 10

# 4. æŸ¥çœ‹æ¶ˆæ¯æ•°é‡
docker exec kafka kafka-run-class kafka.tools.GetOffsetShell \
  --broker-list localhost:9092 \
  --topic ai-social-raw
```

---

### Zookeeperæ·±åº¦è§£æ

#### ä¸€å¥è¯ä»‹ç»
**Zookeeperæ˜¯Kafkaçš„åè°ƒæœåŠ¡ï¼Œç®¡ç†é›†ç¾¤å…ƒæ•°æ®å’Œleaderé€‰ä¸¾ã€‚**

#### ä¸ºä»€ä¹ˆKafkaéœ€è¦Zookeeperï¼Ÿ

Kafkaæ˜¯åˆ†å¸ƒå¼ç³»ç»Ÿï¼Œéœ€è¦è§£å†³ï¼š
1. **è°æ˜¯Leaderï¼Ÿ**ï¼ˆæ¯ä¸ªåˆ†åŒºçš„ä¸»å‰¯æœ¬ï¼‰
2. **å“ªäº›Brokeræ´»ç€ï¼Ÿ**ï¼ˆå¥åº·æ£€æŸ¥ï¼‰
3. **Topicé…ç½®å­˜å“ªï¼Ÿ**ï¼ˆå…ƒæ•°æ®ç®¡ç†ï¼‰
4. **Consumer GroupçŠ¶æ€ï¼Ÿ**ï¼ˆoffsetè®°å½•ï¼‰

Zookeeperå°±æ˜¯é‚£ä¸ª**è®°äº‹æœ¬**ï¼Œè®°å½•è¿™äº›ä¿¡æ¯ã€‚

#### Zookeeperå­˜å‚¨çš„Kafkaå…ƒæ•°æ®

```
/brokers
  /ids
    /1 â†’ {"host":"kafka", "port":9092}  # Brokerä¿¡æ¯
  /topics
    /ai-social-raw
      /partitions
        /0
          /state â†’ {"leader":1, "isr":[1]}  # Leaderå’Œå‰¯æœ¬
```

#### é¡¹ç›®ä¸­çš„Zookeeperé…ç½®

**Docker Composeé…ç½®**ï¼ˆ`docker-compose-full.yml:3-16`ï¼‰ï¼š

```yaml
zookeeper:
  image: confluentinc/cp-zookeeper:7.5.0
  container_name: zookeeper
  hostname: zookeeper
  ports:
    - "2181:2181"  # Clientç«¯å£
  environment:
    ZOOKEEPER_CLIENT_PORT: 2181      # å®¢æˆ·ç«¯è¿æ¥ç«¯å£
    ZOOKEEPER_TICK_TIME: 2000        # å¿ƒè·³é—´éš”ï¼ˆæ¯«ç§’ï¼‰
  volumes:
    - zookeeper-data:/var/lib/zookeeper/data   # æ•°æ®æŒä¹…åŒ–
    - zookeeper-logs:/var/lib/zookeeper/log    # æ—¥å¿—æŒä¹…åŒ–
```

#### Kafka Raftæ¨¡å¼ï¼ˆKRaftï¼‰- æœªæ¥è¶‹åŠ¿

**é‡å¤§å˜åŒ–**ï¼šKafka 3.xå¼€å§‹å¯ä»¥ä¸ä¾èµ–Zookeeperï¼

```
ä¼ ç»Ÿæ¨¡å¼:
Kafka â†’ Zookeeper (ç®¡ç†å…ƒæ•°æ®)

KRaftæ¨¡å¼:
Kafka (å†…ç½®Raftåè®®ï¼Œè‡ªå·±ç®¡ç†å…ƒæ•°æ®)
```

**ä¼˜åŠ¿**ï¼š
- âœ… ç®€åŒ–æ¶æ„ï¼ˆå°‘ä¸€ä¸ªç»„ä»¶ï¼‰
- âœ… æ›´å¿«çš„å…ƒæ•°æ®æ“ä½œ
- âœ… æ”¯æŒæ›´å¤šåˆ†åŒºï¼ˆç™¾ä¸‡çº§ï¼‰

**æˆ‘ä»¬ä¸ºä»€ä¹ˆè¿˜ç”¨Zookeeperï¼Ÿ**
- Confluentå®˜æ–¹é•œåƒé»˜è®¤é…ç½®
- ç”Ÿäº§ç¯å¢ƒè¿˜åœ¨è¿‡æ¸¡æœŸ
- å­¦ä¹ ä¼ ç»Ÿæ¶æ„æœ‰ä»·å€¼

**æœªæ¥è¿ç§»**ï¼šå¾ˆç®€å•ï¼Œæ”¹ä¸€ä¸‹é…ç½®å³å¯ã€‚

---

## 2.3 æµå¤„ç†å±‚ (Stream Processing Layer)

### Apache Spark Streamingæ·±åº¦è§£æ

#### ä¸€å¥è¯ä»‹ç»
**Spark Streamingæ˜¯Apache Sparkçš„æµå¤„ç†æ¨¡å—ï¼Œå°†å®æ—¶æ•°æ®æµåˆ†æˆå°æ‰¹æ¬¡ï¼ˆmicro-batchï¼‰è¿›è¡Œå¤„ç†ã€‚**

#### ä¸ºä»€ä¹ˆéœ€è¦æµå¤„ç†ï¼Ÿ

**ä¼ ç»Ÿæ‰¹å¤„ç†çš„é—®é¢˜**ï¼š
```
æ¯å¤©å‡Œæ™¨3ç‚¹ï¼š
  1. è¯»å–æ˜¨å¤©å…¨éƒ¨æ•°æ®ï¼ˆå¯èƒ½TBçº§ï¼‰
  2. å¤„ç†æ•°å°æ—¶
  3. å†™å…¥ç»“æœ

ç”¨æˆ·ï¼šæˆ‘è¦çœ‹å®æ—¶æ•°æ®ï¼
ç³»ç»Ÿï¼šè¯·ç­‰åˆ°æ˜å¤©å‡Œæ™¨4ç‚¹...
```

**æµå¤„ç†çš„ä¼˜åŠ¿**ï¼š
```
æ¯30ç§’ï¼š
  1. è¯»å–è¿™30ç§’çš„æ–°æ•°æ®ï¼ˆKBçº§ï¼‰
  2. å¤„ç†å‡ ç§’é’Ÿ
  3. å†™å…¥ç»“æœ

ç”¨æˆ·ï¼šæˆ‘è¦çœ‹å®æ—¶æ•°æ®ï¼
ç³»ç»Ÿï¼šå¥½çš„ï¼Œå»¶è¿Ÿ<1åˆ†é’Ÿï¼
```

#### Spark Streaming vs Spark Batch

| ç‰¹æ€§ | Spark Batch | Spark Streaming |
|------|-------------|-----------------|
| **å¤„ç†æ¨¡å¼** | ä¸€æ¬¡æ€§å¤„ç†å®Œæ•´æ•°æ®é›† | æŒç»­å¤„ç†æ— ç•Œæ•°æ®æµ |
| **å»¶è¿Ÿ** | åˆ†é’Ÿ-å°æ—¶çº§ | ç§’-åˆ†é’Ÿçº§ |
| **è¾“å…¥** | é™æ€æ–‡ä»¶ï¼ˆHDFS, S3ï¼‰ | åŠ¨æ€æµï¼ˆKafka, Socketï¼‰ |
| **API** | DataFrame/RDD | Structured Streaming |
| **å®¹é”™** | Taské‡è¯• | Checkpoint + WAL |
| **é€‚ç”¨åœºæ™¯** | æŠ¥è¡¨ã€å†å²åˆ†æ | å®æ—¶ç›‘æ§ã€é¢„è­¦ |

**æˆ‘ä»¬çš„é€‰æ‹©**ï¼šSpark Streamingï¼ˆStructured Streaming APIï¼‰

#### æ ¸å¿ƒæ¦‚å¿µï¼šMicro-batch Processing

**ç™½è¯è§£é‡Š**ï¼š

æƒ³è±¡æ•°æ®æµæ˜¯ä¸€æ¡æ²³ï¼š
- **çœŸæ­£çš„æµå¤„ç†**ï¼ˆFlinkï¼‰ï¼šæ¯ä¸€æ»´æ°´ï¼ˆæ¶ˆæ¯ï¼‰æ¥äº†å°±å¤„ç†
- **Micro-batch**ï¼ˆSparkï¼‰ï¼šæ¯éš”30ç§’ï¼Œç”¨æ¡¶æ¥ä¸€æ¡¶æ°´ï¼Œç„¶åæ‰¹é‡å¤„ç†

```
Timeline:
T0-T30ç§’:  æ”¶é›†æ•°æ® â†’ Batch 1 (100æ¡æ¶ˆæ¯)
T30-T60ç§’: æ”¶é›†æ•°æ® â†’ Batch 2 (120æ¡æ¶ˆæ¯)
           åŒæ—¶å¤„ç† Batch 1
T60-T90ç§’: æ”¶é›†æ•°æ® â†’ Batch 3 (95æ¡æ¶ˆæ¯)
           åŒæ—¶å¤„ç† Batch 2
```

**ä¼˜åŠ¿**ï¼š
- âœ… å¯ä»¥ç”¨æ‰¹å¤„ç†çš„ä¼˜åŒ–æŠ€æœ¯
- âœ… ä»£ç å…¼å®¹æ€§å¥½ï¼ˆDataFrame APIç»Ÿä¸€ï¼‰
- âœ… ååé‡é«˜

**åŠ£åŠ¿**ï¼š
- âŒ å»¶è¿Ÿæ¯”çœŸæµå¤„ç†é«˜ï¼ˆä½†ç§’çº§å·²ç»å¤Ÿç”¨ï¼‰

#### é¡¹ç›®ä¸­çš„Sparké…ç½®

**Docker Composeé…ç½®**ï¼ˆ`docker-compose-full.yml:89-134`ï¼‰ï¼š

```yaml
# Spark Master
spark-master:
  image: apache/spark:3.5.0-python3  # å®˜æ–¹é•œåƒï¼Œå†…ç½®Pythonæ”¯æŒ
  container_name: spark-master
  hostname: spark-master
  ports:
    - "8080:8080"  # Web UI
    - "7077:7077"  # Masterç«¯å£ï¼ˆWorkerè¿æ¥ï¼‰
    - "4040:4040"  # Application UIï¼ˆä½œä¸šè¿è¡Œæ—¶ï¼‰
  environment:
    - SPARK_MODE=master
    - SPARK_MASTER_HOST=spark-master
    - SPARK_MASTER_PORT=7077
    - SPARK_MASTER_WEBUI_PORT=8080
  command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
  volumes:
    - ./streaming/spark:/opt/spark-apps  # æŒ‚è½½ä»£ç 
    - ./storage:/opt/storage
    - spark-logs:/opt/spark/logs

# Spark Worker
spark-worker:
  image: apache/spark:3.5.0-python3
  container_name: spark-worker
  hostname: spark-worker
  user: root  # é‡è¦ï¼é¿å…æƒé™é—®é¢˜
  depends_on:
    - spark-master
  ports:
    - "8081:8081"  # Worker Web UI
  environment:
    - SPARK_MODE=worker
    - SPARK_MASTER_URL=spark://spark-master:7077
    - SPARK_WORKER_CORES=2       # ä½¿ç”¨2ä¸ªCPUæ ¸å¿ƒ
    - SPARK_WORKER_MEMORY=2G     # ä½¿ç”¨2GBå†…å­˜
    - SPARK_WORKER_WEBUI_PORT=8081
    - SPARK_WORKER_DIR=/tmp/spark-work  # å·¥ä½œç›®å½•
  command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
  volumes:
    - ./streaming/spark:/opt/spark-apps
    - ./storage:/opt/storage
    - spark-worker-logs:/tmp/spark-work
```

**æ¶æ„æ¨¡å¼**ï¼šStandaloneæ¨¡å¼

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Spark Master (:8080)                â”‚
â”‚  - ç®¡ç†é›†ç¾¤èµ„æº                       â”‚
â”‚  - è°ƒåº¦ä½œä¸š                          â”‚
â”‚  - ç›‘æ§WorkerçŠ¶æ€                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
          spark://spark-master:7077
                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                           â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚ Worker 1     â”‚      â”‚ Worker N    â”‚
â”‚ :8081        â”‚      â”‚ (å¯æ‰©å±•)     â”‚
â”‚ 2 Cores      â”‚      â”‚             â”‚
â”‚ 2G Memory    â”‚      â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å…¶ä»–æ¨¡å¼å¯¹æ¯”**ï¼š

| æ¨¡å¼ | ç®¡ç†å™¨ | é€‚ç”¨åœºæ™¯ | å¤æ‚åº¦ |
|------|--------|----------|--------|
| **Standalone** | Sparkè‡ªå¸¦ | å¼€å‘ã€å°è§„æ¨¡ | â­ ç®€å• |
| **YARN** | Hadoop YARN | å…±äº«Hadoopé›†ç¾¤ | â­â­â­ ä¸­ç­‰ |
| **Kubernetes** | K8s | äº‘åŸç”Ÿéƒ¨ç½² | â­â­â­â­ å¤æ‚ |
| **Mesos** | Apache Mesos | å¤šæ¡†æ¶å…±äº« | â­â­â­â­ å¤æ‚ |

#### Structured Streamingæ ¸å¿ƒAPI

**æ–‡ä»¶**ï¼š`streaming/spark/processor_with_minio.py`

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json, current_timestamp, to_date
from pyspark.sql.types import StructType, StructField, StringType, MapType

# ========== ç¬¬1æ­¥ï¼šåˆ›å»ºSpark Session ==========
spark = (
    SparkSession.builder
    .appName("AI_Trend_Monitor_MinIO")
    .master("spark://spark-master:7077")  # è¿æ¥åˆ°Master

    # Kafkaä¾èµ–
    .config("spark.jars.packages",
            "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,"
            "org.apache.hadoop:hadoop-aws:3.3.4")  # S3Aæ”¯æŒ

    # MinIOé…ç½®ï¼ˆS3å…¼å®¹ï¼‰
    .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000")
    .config("spark.hadoop.fs.s3a.access.key", "minioadmin")
    .config("spark.hadoop.fs.s3a.secret.key", "minioadmin")
    .config("spark.hadoop.fs.s3a.path.style.access", "true")  # è·¯å¾„é£æ ¼è®¿é—®
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
    .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false")  # å…³é—­SSL

    .getOrCreate()
)

# ========== ç¬¬2æ­¥ï¼šä»Kafkaè¯»å–æµ ==========
kafka_df = (
    spark.readStream  # æ³¨æ„ï¼šreadStreamï¼Œä¸æ˜¯read
    .format("kafka")
    .option("kafka.bootstrap.servers", "kafka:29092")  # å®¹å™¨å†…åœ°å€
    .option("subscribe", "ai-social-raw")  # è®¢é˜…topic
    .option("startingOffsets", "earliest")  # ä»å¤´è¯»ï¼ˆé¦–æ¬¡è¿è¡Œï¼‰
    .option("failOnDataLoss", "false")  # å®¹å¿æ•°æ®ä¸¢å¤±ï¼ˆå¼€å‘ç¯å¢ƒï¼‰
    .load()
)

# Kafkaè¿”å›çš„DataFrame Schema:
# |-- key: binary (å¯ç©º)
# |-- value: binary (æ¶ˆæ¯å†…å®¹ï¼ŒJSON bytes)
# |-- topic: string
# |-- partition: integer
# |-- offset: long
# |-- timestamp: timestamp

# ========== ç¬¬3æ­¥ï¼šè§£æJSONæ•°æ® ==========

# å®šä¹‰JSON Schema
KAFKA_MESSAGE_SCHEMA = StructType([
    StructField("source", StringType(), True),
    StructField("timestamp", StringType(), True),
    StructField("data", MapType(StringType(), StringType()), True)
])

parsed_df = (
    kafka_df
    .selectExpr("CAST(value AS STRING) as json_value")  # bytes â†’ string
    .select(from_json(col("json_value"), KAFKA_MESSAGE_SCHEMA).alias("data"))
    .select(
        col("data.source").alias("source"),
        col("data.timestamp").alias("event_timestamp"),
        col("data.data").alias("raw_data"),
        current_timestamp().alias("processed_at")  # å¤„ç†æ—¶é—´æˆ³
    )
    .withColumn("partition_date", to_date(col("processed_at")))  # æ—¥æœŸåˆ†åŒº
)

# ç»“æœDataFrame Schema:
# |-- source: string (reddit/twitter)
# |-- event_timestamp: string (åŸå§‹äº‹ä»¶æ—¶é—´)
# |-- raw_data: map<string,string> (åŸå§‹æ•°æ®)
# |-- processed_at: timestamp (å¤„ç†æ—¶é—´)
# |-- partition_date: date (åˆ†åŒºå­—æ®µï¼Œå¦‚2025-11-12)

# ========== ç¬¬4æ­¥ï¼šå†™å…¥MinIO ==========
query = (
    parsed_df
    .writeStream  # æ³¨æ„ï¼šwriteStreamï¼Œä¸æ˜¯write
    .format("parquet")  # è¾“å‡ºæ ¼å¼
    .outputMode("append")  # è¿½åŠ æ¨¡å¼ï¼ˆvs complete/updateï¼‰
    .option("checkpointLocation", "s3a://lakehouse/checkpoints/bronze")  # å®¹é”™æ£€æŸ¥ç‚¹
    .option("path", "s3a://lakehouse/bronze/social_media")  # è¾“å‡ºè·¯å¾„
    .partitionBy("partition_date", "source")  # æŒ‰æ—¥æœŸå’Œæ¥æºåˆ†åŒº
    .trigger(processingTime='30 seconds')  # æ¯30ç§’ä¸€ä¸ªæ‰¹æ¬¡
    .start()
)

# è¾“å‡ºç›®å½•ç»“æ„:
# s3a://lakehouse/bronze/social_media/
#   partition_date=2025-11-12/
#     source=reddit/
#       part-00000-xxx.snappy.parquet
#       part-00001-xxx.snappy.parquet
#     source=twitter/
#       part-00000-xxx.snappy.parquet

# ========== ç¬¬5æ­¥ï¼šç­‰å¾…ç»ˆæ­¢ ==========
query.awaitTermination()  # é˜»å¡ï¼ŒæŒç»­è¿è¡Œ
```

#### å…³é”®æ¦‚å¿µæ·±åº¦è§£æ

##### 1. Output Modeï¼ˆè¾“å‡ºæ¨¡å¼ï¼‰

| æ¨¡å¼ | å«ä¹‰ | é€‚ç”¨åœºæ™¯ | æˆ‘ä»¬çš„é€‰æ‹© |
|------|------|----------|------------|
| **append** | åªå†™æ–°å¢è¡Œ | åªè¿½åŠ æ•°æ®ï¼Œä¸ä¿®æ”¹å†å² | âœ… åŸå§‹æ•°æ®é‡‡é›† |
| **complete** | æ¯æ¬¡è¾“å‡ºå…¨éƒ¨ç»“æœ | èšåˆç»“æœï¼Œéœ€è¦å…¨é‡ | è¶‹åŠ¿ç»Ÿè®¡è¡¨ |
| **update** | åªè¾“å‡ºæ›´æ–°çš„è¡Œ | å¢é‡æ›´æ–°ï¼Œæœ‰ä¸»é”® | ç”¨æˆ·ç”»åƒè¡¨ |

**ä¸ºä»€ä¹ˆé€‰appendï¼Ÿ**
- æˆ‘ä»¬æ˜¯åŸå§‹æ•°æ®é‡‡é›†ï¼Œæ¯æ¡æ¶ˆæ¯éƒ½æ˜¯æ–°çš„
- ä¸éœ€è¦ä¿®æ”¹å†å²æ•°æ®
- æ€§èƒ½æœ€å¥½ï¼ˆåªå†™å¢é‡ï¼‰

##### 2. Checkpointï¼ˆæ£€æŸ¥ç‚¹ï¼‰

**é—®é¢˜**ï¼šSparkä½œä¸šå´©æºƒé‡å¯ï¼Œå¦‚ä½•é¿å…é‡å¤å¤„ç†ï¼Ÿ

```
T1: å¤„ç†offset 0-100
T2: å¤„ç†offset 101-200
T3: å´©æºƒï¼

é‡å¯åï¼š
  å¦‚æœæ²¡æœ‰Checkpoint â†’ ä»offset 0é‡æ–°å¼€å§‹ï¼ˆé‡å¤ï¼ï¼‰
  å¦‚æœæœ‰Checkpoint â†’ ä»offset 201ç»§ç»­ï¼ˆå®Œç¾ï¼ï¼‰
```

**Checkpointå­˜å‚¨ä»€ä¹ˆï¼Ÿ**
```
s3a://lakehouse/checkpoints/bronze/
  commits/  # å·²æäº¤çš„batch ID
    0, 1, 2, ...
  offsets/  # Kafka offsetè®°å½•
    0: {"ai-social-raw":{"0":123}}
    1: {"ai-social-raw":{"0":456}}
  sources/  # SourceçŠ¶æ€
  state/    # èšåˆçŠ¶æ€ï¼ˆå¦‚æœæœ‰ï¼‰
```

**å®¹é”™ä¿è¯**ï¼š
- âœ… Exactly-onceè¯­ä¹‰ï¼ˆç»“åˆKafka offsetå’ŒCheckpointï¼‰
- âœ… å´©æºƒåè‡ªåŠ¨æ¢å¤
- âœ… ä¸é‡å¤ã€ä¸ä¸¢å¤±

##### 3. Triggerï¼ˆè§¦å‘å™¨ï¼‰

| Triggerç±»å‹ | è¯´æ˜ | å»¶è¿Ÿ | ååé‡ |
|------------|------|------|--------|
| `processingTime='30 seconds'` | å›ºå®šé—´éš” | 30ç§’+ | é«˜ |
| `once=True` | è¿è¡Œä¸€æ¬¡å°±åœ | N/A | - |
| `continuous='1 second'` | è¿ç»­å¤„ç†ï¼ˆå®éªŒæ€§ï¼‰ | 1ç§’+ | æœ€é«˜ |
| é»˜è®¤ï¼ˆmicro-batchï¼‰ | å°½å¿«å¤„ç† | æœ€ä½ | ä¸­ |

**æˆ‘ä»¬é€‰æ‹©30ç§’é—´éš”**ï¼š
- å¹³è¡¡å»¶è¿Ÿå’Œåå
- é¿å…äº§ç”Ÿå¤ªå¤šå°æ–‡ä»¶
- Kafkaæ•°æ®é‡ä¸å¤§ï¼Œä¸éœ€è¦æ›´é¢‘ç¹

##### 4. Partitioningï¼ˆåˆ†åŒºï¼‰

```python
.partitionBy("partition_date", "source")
```

**æ•ˆæœ**ï¼š
```
bronze/social_media/
  partition_date=2025-11-12/source=reddit/part-00000.parquet
  partition_date=2025-11-12/source=twitter/part-00000.parquet
  partition_date=2025-11-13/source=reddit/part-00000.parquet
  partition_date=2025-11-13/source=twitter/part-00000.parquet
```

**å¥½å¤„**ï¼š
- âœ… **æŸ¥è¯¢åŠ é€Ÿ**ï¼šåªè¯»éœ€è¦çš„åˆ†åŒºï¼ˆPartition Pruningï¼‰
  ```sql
  SELECT * FROM bronze
  WHERE partition_date = '2025-11-12'  -- åªæ‰«æ1å¤©çš„æ•°æ®ï¼Œè·³è¿‡å…¶ä»–
  AND source = 'reddit'                 -- åªæ‰«æredditåˆ†åŒº
  ```
- âœ… **æ•°æ®ç®¡ç†**ï¼šæ–¹ä¾¿åˆ é™¤æ—§æ•°æ®
  ```bash
  # åˆ é™¤30å¤©å‰çš„æ•°æ®
  aws s3 rm s3://lakehouse/bronze/social_media/partition_date=2025-10-12/ --recursive
  ```

#### Spark Submitå¯åŠ¨è„šæœ¬

**æ–‡ä»¶**ï¼š`scripts/02-start_spark_minio.sh`

```bash
#!/bin/bash

echo "ğŸš€ Starting Spark Streaming with MinIO..."

# æ¿€æ´»Pythonç¯å¢ƒ
source venv/bin/activate

# Spark Submit
spark-submit \
  --master spark://localhost:7077 \
  --deploy-mode client \
  --name "AI_Trend_Monitor_MinIO" \
  \
  # ===== èµ„æºé…ç½® =====
  --executor-memory 1G \
  --executor-cores 1 \
  --total-executor-cores 2 \
  \
  # ===== Jarä¾èµ–ï¼ˆæœ¬åœ°è·¯å¾„ï¼‰ =====
  --jars streaming/spark/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar,\
streaming/spark/jars/kafka-clients-3.4.1.jar,\
streaming/spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar,\
streaming/spark/jars/commons-pool2-2.11.1.jar,\
streaming/spark/jars/hadoop-aws-3.3.4.jar,\
streaming/spark/jars/aws-java-sdk-bundle-1.12.262.jar \
  \
  # ===== MinIO/S3Aé…ç½® =====
  --conf spark.hadoop.fs.s3a.endpoint=http://localhost:9000 \
  --conf spark.hadoop.fs.s3a.access.key=minioadmin \
  --conf spark.hadoop.fs.s3a.secret.key=minioadmin \
  --conf spark.hadoop.fs.s3a.path.style.access=true \
  --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \
  --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false \
  \
  # ===== Pythonæ–‡ä»¶ =====
  streaming/spark/processor_with_minio.py
```

**å…³é”®å‚æ•°è§£é‡Š**ï¼š

- `--master spark://localhost:7077`ï¼šè¿æ¥åˆ°Spark Master
- `--deploy-mode client`ï¼šDriverè¿è¡Œåœ¨å®¢æˆ·ç«¯ï¼ˆå¯ä»¥çœ‹åˆ°æ—¥å¿—ï¼‰
  - `client`ï¼šDriveråœ¨æäº¤æœºå™¨ï¼ˆé€‚åˆå¼€å‘ï¼‰
  - `cluster`ï¼šDriveråœ¨Workerä¸Šï¼ˆé€‚åˆç”Ÿäº§ï¼‰

- `--executor-memory 1G`ï¼šæ¯ä¸ªExecutorä½¿ç”¨1GBå†…å­˜
- `--total-executor-cores 2`ï¼šæ€»å…±ä½¿ç”¨2ä¸ªCPUæ ¸å¿ƒ

- `--jars`ï¼šä¾èµ–çš„jaræ–‡ä»¶ï¼ˆKafka + S3Aï¼‰
  - ä¸ºä»€ä¹ˆä¸ç”¨`--packages`è‡ªåŠ¨ä¸‹è½½ï¼ŸMavenä¸‹è½½å¯èƒ½å¤±è´¥æˆ–æ…¢

#### Spark Web UIæ·±åº¦è§£è¯»

è®¿é—® http://localhost:8080 å¯ä»¥çœ‹åˆ°ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Spark Master UI (http://localhost:8080)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                        â”‚
â”‚  Workers (1)                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Worker ID: worker-20251112-spark-worker          â”‚  â”‚
â”‚  â”‚ State: ALIVE                                     â”‚  â”‚
â”‚  â”‚ Cores: 2 (0 Used)                                â”‚  â”‚
â”‚  â”‚ Memory: 2.0 GB (0.0 B Used)                      â”‚  â”‚
â”‚  â”‚ UI: http://spark-worker:8081                     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                        â”‚
â”‚  Running Applications (1)                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ App ID: app-20251112120000-0001                  â”‚  â”‚
â”‚  â”‚ Name: AI_Trend_Monitor_MinIO                     â”‚  â”‚
â”‚  â”‚ Cores: 2                                         â”‚  â”‚
â”‚  â”‚ Memory: 2.0 GB                                   â”‚  â”‚
â”‚  â”‚ Submit Date: 2025-11-12 12:00:00                 â”‚  â”‚
â”‚  â”‚ Duration: 1.5 hours                              â”‚  â”‚
â”‚  â”‚ UI: http://localhost:4040                        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Application UI** (http://localhost:4040) æ›´è¯¦ç»†ï¼š

```
Tabs:
â”œâ”€ Jobs: æ¯ä¸ªmicro-batchæ˜¯ä¸€ä¸ªjob
â”œâ”€ Stages: æ¯ä¸ªjobçš„æ‰§è¡Œé˜¶æ®µ
â”œâ”€ Storage: ç¼“å­˜çš„æ•°æ®
â”œâ”€ Environment: Sparké…ç½®
â”œâ”€ Executors: Executorè¯¦æƒ…
â””â”€ Streaming: æµå¤„ç†ä¸“å±
     â”œâ”€ Input Rate: æ¯ç§’å¤„ç†å¤šå°‘æ¡æ¶ˆæ¯
     â”œâ”€ Processing Time: æ¯ä¸ªbatchå¤„ç†è€—æ—¶
     â”œâ”€ Scheduling Delay: æ’é˜Ÿç­‰å¾…æ—¶é—´
     â””â”€ Total Delay: ç«¯åˆ°ç«¯å»¶è¿Ÿ
```

**æ€§èƒ½ç›‘æ§å…³é”®æŒ‡æ ‡**ï¼š
- **Processing Time < Batch Interval**ï¼šå¥åº·ï¼ˆå¦‚15ç§’ < 30ç§’ï¼‰
- **Processing Time â‰ˆ Batch Interval**ï¼šä¸´ç•Œï¼ˆå¦‚28ç§’ < 30ç§’ï¼‰
- **Processing Time > Batch Interval**ï¼šè­¦å‘Šï¼ç§¯å‹ï¼ï¼ˆå¦‚35ç§’ > 30ç§’ï¼‰

---

## 2.4 å­˜å‚¨å±‚ (Storage Layer)

### MinIOæ·±åº¦è§£æ

#### ä¸€å¥è¯ä»‹ç»
**MinIOæ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„å¯¹è±¡å­˜å‚¨æœåŠ¡ï¼Œ100%å…¼å®¹AWS S3 APIã€‚**

#### ä¸ºä»€ä¹ˆéœ€è¦å¯¹è±¡å­˜å‚¨ï¼Ÿ

**å¯¹æ¯”ä¼ ç»Ÿæ–‡ä»¶ç³»ç»Ÿ**ï¼š

| ç‰¹æ€§ | æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ | HDFS | MinIO/S3 |
|------|-------------|------|----------|
| **æ‰©å±•æ€§** | âŒ å•æœºé™åˆ¶ | âœ… åˆ†å¸ƒå¼ï¼ŒPBçº§ | âœ… åˆ†å¸ƒå¼ï¼ŒEBçº§ |
| **å¯é æ€§** | âŒ å•ç‚¹æ•…éšœ | âœ… å‰¯æœ¬æœºåˆ¶ | âœ… çº åˆ ç /å‰¯æœ¬ |
| **API** | æ–‡ä»¶è·¯å¾„ | HDFS CLI | âœ… HTTP REST API |
| **äº‘å…¼å®¹** | âŒ | âŒ | âœ… S3å…¼å®¹ |
| **æˆæœ¬** | ç¡¬ç›˜æˆæœ¬ | æœåŠ¡å™¨æˆæœ¬ | âœ… æŒ‰éœ€ä»˜è´¹ |
| **è¿ç»´å¤æ‚åº¦** | ä½ | é«˜ï¼ˆNameNode/DataNodeï¼‰ | âœ… ä½ï¼ˆæ— çŠ¶æ€ï¼‰ |

**ä¸ºä»€ä¹ˆé€‰MinIOè€Œä¸æ˜¯çœŸæ­£çš„AWS S3ï¼Ÿ**

| åœºæ™¯ | MinIO | AWS S3 |
|------|-------|--------|
| **å¼€å‘/æµ‹è¯•** | âœ… æœ¬åœ°è¿è¡Œï¼Œå…è´¹ | âŒ éœ€è¦ç½‘ç»œï¼Œæœ‰è´¹ç”¨ |
| **æ•°æ®ä¸»æƒ** | âœ… æ•°æ®åœ¨æœ¬åœ° | âŒ æ•°æ®åœ¨AWS |
| **å»¶è¿Ÿ** | âœ… æœ¬åœ°<1ms | âŒ ç½‘ç»œ50-200ms |
| **ç”Ÿäº§ç¯å¢ƒ** | éœ€è¦è‡ªå·±è¿ç»´ | âœ… æ‰˜ç®¡æœåŠ¡ |

**ç­–ç•¥**ï¼š
- å¼€å‘ï¼šMinIOï¼ˆæœ¬åœ°ï¼‰
- ç”Ÿäº§ï¼šè¿ç§»åˆ°S3ï¼ˆæ”¹é…ç½®å³å¯ï¼ŒAPIå…¼å®¹ï¼‰

#### MinIOæ ¸å¿ƒæ¦‚å¿µ

##### 1. Bucketï¼ˆæ¡¶ï¼‰= é¡¶å±‚å‘½åç©ºé—´

```
MinIO Server
â”œâ”€ Bucket: lakehouse
â”‚  â”œâ”€ bronze/social_media/...
â”‚  â”œâ”€ silver/...
â”‚  â””â”€ gold/...
â”œâ”€ Bucket: backups
â””â”€ Bucket: logs
```

**ç±»æ¯”**ï¼š
- S3 Bucket = ç¡¬ç›˜ä¸Šçš„æ ¹ç›®å½•
- Object = æ–‡ä»¶

##### 2. Objectï¼ˆå¯¹è±¡ï¼‰= æ–‡ä»¶ + å…ƒæ•°æ®

```
Object Key: bronze/social_media/partition_date=2025-11-12/source=reddit/part-00000.parquet

Object:
  â”œâ”€ Data: [ParquetäºŒè¿›åˆ¶æ•°æ®]
  â”œâ”€ Metadata:
  â”‚  â”œâ”€ Content-Type: application/octet-stream
  â”‚  â”œâ”€ Content-Length: 2048576 (2MB)
  â”‚  â”œâ”€ ETag: "abc123..."  # å†…å®¹å“ˆå¸Œ
  â”‚  â””â”€ Last-Modified: 2025-11-12 12:30:00
  â””â”€ User Metadata: (å¯è‡ªå®šä¹‰)
     â”œâ”€ x-amz-meta-source: kafka
     â””â”€ x-amz-meta-batch-id: 12345
```

**æ³¨æ„**ï¼š
- Object KeyåŒ…å«"/"ï¼Œä½†ä¸æ˜¯çœŸæ­£çš„ç›®å½•ï¼ˆæ‰å¹³å­˜å‚¨ï¼‰
- MinIO Console UIä¼šæ¨¡æ‹Ÿç›®å½•ç»“æ„

##### 3. S3A FileSystemï¼ˆHadoopé›†æˆï¼‰

Sparké€šè¿‡S3Aåè®®è®¿é—®MinIOï¼š

```
Spark â”€â”€S3A://â”€â”€â†’ MinIO

s3a://lakehouse/bronze/social_media/
  â†“
HTTP API:
  GET http://minio:9000/lakehouse/bronze/social_media/...
```

**é…ç½®**ï¼š
```python
.config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000")
.config("spark.hadoop.fs.s3a.access.key", "minioadmin")
.config("spark.hadoop.fs.s3a.secret.key", "minioadmin")
.config("spark.hadoop.fs.s3a.path.style.access", "true")  # é‡è¦ï¼
```

**`path.style.access=true`æ˜¯ä»€ä¹ˆï¼Ÿ**

```
è™šæ‹Ÿä¸»æœºé£æ ¼ (false):
  http://lakehouse.minio:9000/bronze/social_media/...
  éœ€è¦DNSè§£æ lakehouse.minio

è·¯å¾„é£æ ¼ (true):
  http://minio:9000/lakehouse/bronze/social_media/...
  ä¸éœ€è¦DNSï¼Œç›´æ¥è·¯å¾„
```

æœ¬åœ°å¼€å‘ç”¨è·¯å¾„é£æ ¼æ›´ç®€å•ã€‚

#### é¡¹ç›®ä¸­çš„MinIOé…ç½®

**Docker Composeé…ç½®**ï¼ˆ`docker-compose-full.yml:48-86`ï¼‰ï¼š

```yaml
# MinIOæœåŠ¡
minio:
  image: minio/minio:latest
  container_name: minio
  hostname: minio
  ports:
    - "9000:9000"  # S3 APIç«¯å£
    - "9001:9001"  # Web Consoleç«¯å£
  environment:
    MINIO_ROOT_USER: minioadmin      # è®¿é—®å¯†é’¥
    MINIO_ROOT_PASSWORD: minioadmin  # ç§˜å¯†å¯†é’¥
  command: server /data --console-address ":9001"
  volumes:
    - minio-data:/data  # æ•°æ®æŒä¹…åŒ–
  networks:
    - lakehouse-network

# MinIOåˆå§‹åŒ–ï¼ˆåˆ›å»ºbucketsï¼‰
minio-init:
  image: minio/mc:latest  # MinIO Client
  container_name: minio-init
  depends_on:
    minio:
      condition: service_healthy
  entrypoint: >
    /bin/sh -c "
    mc alias set myminio http://minio:9000 minioadmin minioadmin;
    mc mb myminio/lakehouse --ignore-existing;
    mc mb myminio/bronze --ignore-existing;
    mc mb myminio/silver --ignore-existing;
    mc mb myminio/gold --ignore-existing;
    mc anonymous set download myminio/lakehouse;
    echo 'MinIO buckets created successfully';
    "
  networks:
    - lakehouse-network
```

**MinIO Client (mc) å¸¸ç”¨å‘½ä»¤**ï¼š

```bash
# 1. é…ç½®alias
docker exec minio mc alias set myminio http://localhost:9000 minioadmin minioadmin

# 2. åˆ—å‡ºæ‰€æœ‰buckets
docker exec minio mc ls myminio/

# 3. é€’å½’åˆ—å‡ºlakehouse bucketçš„å†…å®¹
docker exec minio mc ls --recursive myminio/lakehouse/bronze/social_media/

# 4. æŸ¥çœ‹å¯¹è±¡è¯¦æƒ…
docker exec minio mc stat myminio/lakehouse/bronze/social_media/partition_date=2025-11-12/source=reddit/part-00000.parquet

# 5. ä¸‹è½½æ–‡ä»¶åˆ°æœ¬åœ°
docker exec minio mc cp myminio/lakehouse/bronze/social_media/part-00000.parquet /tmp/

# 6. åˆ é™¤æ—§æ•°æ®
docker exec minio mc rm --recursive --force myminio/lakehouse/bronze/social_media/partition_date=2025-10-01/
```

#### MinIO Console Web UI

è®¿é—® http://localhost:9001ï¼š

```
Login:
  Username: minioadmin
  Password: minioadmin

ç•Œé¢:
â”œâ”€ Buckets
â”‚  â”œâ”€ lakehouse (ä¸»æ•°æ®)
â”‚  â”‚  â”œâ”€ bronze/
â”‚  â”‚  â”‚  â””â”€ social_media/
â”‚  â”‚  â”‚     â””â”€ partition_date=2025-11-12/
â”‚  â”‚  â”‚        â”œâ”€ source=reddit/
â”‚  â”‚  â”‚        â”‚  â””â”€ part-00000.parquet (2.1 MB)
â”‚  â”‚  â”‚        â””â”€ source=twitter/
â”‚  â”‚  â”œâ”€ checkpoints/
â”‚  â”‚  â”œâ”€ silver/  (æœªæ¥)
â”‚  â”‚  â””â”€ gold/    (æœªæ¥)
â”‚  â”œâ”€ bronze
â”‚  â”œâ”€ silver
â”‚  â””â”€ gold
â”‚
â”œâ”€ Object Browser (æµè§ˆæ–‡ä»¶)
â”œâ”€ Access Keys (è®¿é—®å¯†é’¥ç®¡ç†)
â”œâ”€ Monitoring (ç›‘æ§)
â””â”€ Settings (è®¾ç½®)
```

---

### Delta Lakeæ·±åº¦è§£æ

#### ä¸€å¥è¯ä»‹ç»
**Delta Lakeæ˜¯æ„å»ºåœ¨Parquetä¹‹ä¸Šçš„å¼€æºå­˜å‚¨å±‚ï¼Œæä¾›ACIDäº‹åŠ¡ã€æ—¶é—´æ—…è¡Œã€Schemaæ¼”åŒ–ç­‰ç‰¹æ€§ã€‚**

#### ä¸ºä»€ä¹ˆéœ€è¦Delta Lakeï¼Ÿ

**Parquetçš„é—®é¢˜**ï¼š

```
åœºæ™¯ï¼šä¿®æ”¹å·²å†™å…¥çš„æ•°æ®

Parquet:
  1. å†™å…¥ part-00000.parquet (1000è¡Œ)
  2. å‘ç°æœ‰é”™ï¼Œæƒ³åˆ é™¤ID=500çš„é‚£è¡Œ
  3. âŒ æ— æ³•ä¿®æ”¹ï¼Parquetæ˜¯ä¸å¯å˜çš„ï¼
  4. åªèƒ½ï¼šé‡å†™æ•´ä¸ªæ–‡ä»¶ï¼ˆä½æ•ˆï¼‰æˆ–æ–°å¢æ ‡è®°æ–‡ä»¶ï¼ˆå¤æ‚ï¼‰

Delta Lake:
  1. å†™å…¥ part-00000.parquet (1000è¡Œ)
  2. æ‰§è¡Œ DELETE WHERE id=500
  3. âœ… åªæ›´æ–°_delta_log/ï¼Œä¸é‡å†™æ•°æ®æ–‡ä»¶
  4. è¯»å–æ—¶è‡ªåŠ¨è¿‡æ»¤
```

**æ ¸å¿ƒä¼˜åŠ¿**ï¼š

| ç‰¹æ€§ | Parquet | Delta Lake |
|------|---------|------------|
| **ACIDäº‹åŠ¡** | âŒ | âœ… å¤šå†™å…¥è€…éš”ç¦» |
| **æ—¶é—´æ—…è¡Œ** | âŒ | âœ… æŸ¥è¯¢å†å²ç‰ˆæœ¬ |
| **Schemaæ¼”åŒ–** | âŒ éœ€è¦é‡å†™ | âœ… è‡ªåŠ¨åˆå¹¶ |
| **UPSERT** | âŒ | âœ… Merge Into |
| **DELETE/UPDATE** | âŒ | âœ… æ”¯æŒ |
| **æ–‡ä»¶å‹ç¼©** | æ‰‹åŠ¨ | âœ… è‡ªåŠ¨Optimize |
| **æ•°æ®æ ¡éªŒ** | âŒ | âœ… SchemaéªŒè¯ |

#### Delta Lakeå·¥ä½œåŸç†

**æ ¸å¿ƒï¼šTransaction Logï¼ˆäº‹åŠ¡æ—¥å¿—ï¼‰**

```
Delta Tableç›®å½•ç»“æ„:

delta_table/
â”œâ”€ _delta_log/
â”‚  â”œâ”€ 00000000000000000000.json  # Version 0
â”‚  â”œâ”€ 00000000000000000001.json  # Version 1
â”‚  â”œâ”€ 00000000000000000002.json  # Version 2
â”‚  â””â”€ ...
â”œâ”€ part-00000-xxx.snappy.parquet  # æ•°æ®æ–‡ä»¶
â”œâ”€ part-00001-xxx.snappy.parquet
â””â”€ ...
```

**äº‹åŠ¡æ—¥å¿—ç¤ºä¾‹**ï¼ˆ`00000000000000000000.json`ï¼‰ï¼š

```json
{
  "commitInfo": {
    "timestamp": 1699785600000,
    "operation": "WRITE",
    "operationParameters": {"mode": "Append"}
  }
}
{
  "add": {
    "path": "part-00000-xxx.snappy.parquet",
    "size": 2048576,
    "modificationTime": 1699785600000,
    "dataChange": true,
    "stats": "{\"numRecords\":1000,\"minValues\":{\"id\":1},\"maxValues\":{\"id\":1000}}"
  }
}
```

**DELETEæ“ä½œ**ï¼ˆ`00000000000000000001.json`ï¼‰ï¼š

```json
{
  "remove": {
    "path": "part-00000-xxx.snappy.parquet",
    "deletionTimestamp": 1699785700000
  }
}
{
  "add": {
    "path": "part-00001-yyy.snappy.parquet",  # æ–°æ–‡ä»¶ï¼Œä¸åŒ…å«id=500
    "size": 2040000,
    "dataChange": true
  }
}
```

**æ—¶é—´æ—…è¡Œ**ï¼š

```python
# æŸ¥è¯¢Version 0ï¼ˆç¬¬ä¸€æ¬¡å†™å…¥çš„æ•°æ®ï¼‰
df_v0 = spark.read.format("delta").option("versionAsOf", 0).load("s3a://lakehouse/silver/posts")

# æŸ¥è¯¢æ˜¨å¤©çš„æ•°æ®
df_yesterday = spark.read.format("delta").option("timestampAsOf", "2025-11-11").load("...")
```

#### é¡¹ç›®ä¸­çš„Delta Lakeä½¿ç”¨ï¼ˆæœªæ¥è®¡åˆ’ï¼‰

**å½“å‰çŠ¶æ€**ï¼šBronzeå±‚ä½¿ç”¨Parquetï¼ˆç®€å•ï¼‰

**Phase 3è®¡åˆ’**ï¼šè¿ç§»åˆ°Delta Lake

```python
# Bronze â†’ Silverè½¬æ¢ï¼ˆæ¯å°æ—¶è¿è¡Œï¼‰
from delta.tables import DeltaTable

# è¯»å–Bronzeï¼ˆParquetï¼‰
bronze_df = spark.read.parquet("s3a://lakehouse/bronze/social_media/partition_date=2025-11-12/")

# æ¸…æ´—æ•°æ®
silver_df = (
    bronze_df
    .dropDuplicates(["post_id"])  # å»é‡
    .filter(col("text").isNotNull())  # è¿‡æ»¤ç©ºå€¼
    .withColumn("text_length", length(col("text")))
    .withColumn("keywords", extract_keywords_udf(col("text")))  # NLPå¤„ç†
)

# å†™å…¥Silverï¼ˆDeltaï¼‰
(silver_df
 .write
 .format("delta")
 .mode("append")
 .partitionBy("partition_date", "source")
 .save("s3a://lakehouse/silver/posts"))

# æ›´æ–°æ“ä½œï¼ˆUPSERTï¼‰
deltaTable = DeltaTable.forPath(spark, "s3a://lakehouse/silver/posts")

deltaTable.alias("target").merge(
    new_data.alias("source"),
    "target.post_id = source.post_id"
).whenMatchedUpdateAll(  # åŒ¹é…æ—¶æ›´æ–°
).whenNotMatchedInsertAll(  # ä¸åŒ¹é…æ—¶æ’å…¥
).execute()
```

**ä¼˜åŒ–æ“ä½œ**ï¼š

```python
# Optimize: åˆå¹¶å°æ–‡ä»¶
deltaTable.optimize().executeCompaction()

# Z-Order: æŒ‰å¸¸ç”¨æŸ¥è¯¢åˆ—æ’åºï¼ŒåŠ é€ŸæŸ¥è¯¢
deltaTable.optimize().executeZOrderBy("post_id", "created_at")

# Vacuum: æ¸…ç†æ—§ç‰ˆæœ¬æ–‡ä»¶ï¼ˆä¿ç•™7å¤©ï¼‰
deltaTable.vacuum(retentionHours=168)
```

---

**Part 2ç»§ç»­ï¼ˆè¿˜éœ€æ·»åŠ Streamlitå’ŒNLPéƒ¨åˆ†ï¼‰**...

