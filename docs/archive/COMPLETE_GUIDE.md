# AIè¶‹åŠ¿ç›‘æ§ç³»ç»Ÿ - å®Œæ•´æŒ‡å—

**ç‰ˆæœ¬**: v1.0 | **æ›´æ–°æ—¥æœŸ**: 2025-11-11 | **çŠ¶æ€**: Phase 2B å®Œæˆ

---

## ğŸ“– ç›®å½•

1. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
2. [ç³»ç»Ÿæ¶æ„](#ç³»ç»Ÿæ¶æ„)
3. [å®Œæ•´ä½¿ç”¨æµç¨‹](#å®Œæ•´ä½¿ç”¨æµç¨‹)
4. [æ•…éšœæ’æŸ¥](#æ•…éšœæ’æŸ¥)
5. [ä¸‹ä¸€æ­¥æ‰©å±•](#ä¸‹ä¸€æ­¥æ‰©å±•)

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å‰ç½®è¦æ±‚

- Docker Desktop (è¿è¡Œä¸­)
- Python 3.9+
- Twitter APIå¯†é’¥
- Reddit APIå¯†é’¥

### 5åˆ†é’Ÿå¯åŠ¨

```bash
# 1. é…ç½®APIå¯†é’¥
cp config/env.example config/.env
# ç¼–è¾‘ config/.env å¡«å…¥å¯†é’¥

# 2. å¯åŠ¨åŸºç¡€è®¾æ–½
./scripts/start_full_infrastructure.sh

# 3. å®‰è£…Pythonä¾èµ–
python3 -m venv venv && source venv/bin/activate
pip install -r requirements.txt

# 4. å¯åŠ¨æ•°æ®é‡‡é›†
./scripts/start_collectors.sh

# 5. ä¸‹è½½Sparkä¾èµ–ï¼ˆé¦–æ¬¡è¿è¡Œï¼‰
./scripts/prepare_spark_jars.sh

# 6. å¯åŠ¨Sparkå¤„ç†
./scripts/start_spark_streaming_fixed.sh
```

**è®¿é—®ç‚¹ï¼š**
- Spark UI: http://localhost:8080
- MinIO: http://localhost:9001 (minioadmin/minioadmin)
- Dashboard: è¿è¡Œ `./scripts/start_dashboard_realtime.sh`

---

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

### æ•°æ®æµ

```
Twitter/Reddit API
       â†“
Pythoné‡‡é›†å™¨ (æ¯5åˆ†é’Ÿ)
       â†“
Apache Kafka (ai-social-raw topic)
       â†“
Spark Streaming (å®æ—¶å¤„ç†)
       â†“
MinIO + Delta Lake (Bronze/Silver/Gold)
       â†“
Streamlit Dashboard
```

### æŠ€æœ¯æ ˆ

| å±‚çº§ | æŠ€æœ¯ | ç”¨é€” |
|------|------|------|
| é‡‡é›† | Tweepy + PRAW | APIæ•°æ®é‡‡é›† |
| é˜Ÿåˆ— | Kafka | æ¶ˆæ¯ç¼“å†² |
| å¤„ç† | Spark Streaming | å®æ—¶ETL |
| å­˜å‚¨ | MinIO + Delta Lake | Lakehouse |
| å¯è§†åŒ– | Streamlit | å®æ—¶Dashboard |

### ç›®å½•ç»“æ„

```
lakehouse_ai_pipeline/
â”œâ”€â”€ config/              # é…ç½®æ–‡ä»¶
â”‚   â””â”€â”€ .env            # APIå¯†é’¥ï¼ˆéœ€åˆ›å»ºï¼‰
â”œâ”€â”€ data_ingestion/      # æ•°æ®é‡‡é›†
â”‚   â”œâ”€â”€ twitter/
â”‚   â”œâ”€â”€ reddit/
â”‚   â””â”€â”€ kafka_producer.py
â”œâ”€â”€ streaming/spark/     # Sparkä½œä¸š
â”‚   â”œâ”€â”€ simple_processor.py
â”‚   â””â”€â”€ jars/           # Kafkaè¿æ¥å™¨
â”œâ”€â”€ dashboard/           # å¯è§†åŒ–
â”‚   â”œâ”€â”€ app_realtime.py
â”‚   â””â”€â”€ kafka_reader.py
â”œâ”€â”€ scripts/             # å¯åŠ¨è„šæœ¬
â””â”€â”€ docs/                # æ–‡æ¡£
```

---

## ğŸ“‹ å®Œæ•´ä½¿ç”¨æµç¨‹

### åœºæ™¯1ï¼šé¦–æ¬¡å¯åŠ¨ç³»ç»Ÿ

**æ­¥éª¤ï¼š**

```bash
# 1. é…ç½®ç¯å¢ƒ
cd ~/Documents/Lakehouse_ai_pipeline
cp config/env.example config/.env
nano config/.env  # å¡«å…¥Twitterå’ŒReddit APIå¯†é’¥

# 2. å¯åŠ¨åŸºç¡€è®¾æ–½
./scripts/start_full_infrastructure.sh
# ç­‰å¾…30ç§’ï¼Œæ‰€æœ‰æœåŠ¡å¯åŠ¨

# 3. éªŒè¯æœåŠ¡
docker ps  # åº”è¯¥çœ‹åˆ°5ä¸ªå®¹å™¨è¿è¡Œ
curl http://localhost:8080  # Spark Master UIå¯è®¿é—®

# 4. å®‰è£…Pythonç¯å¢ƒ
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# 5. å¯åŠ¨æ•°æ®é‡‡é›†
./scripts/start_collectors.sh
# æŸ¥çœ‹æ—¥å¿—ç¡®è®¤æ­£å¸¸ï¼štail -f logs/twitter_collector.log

# 6. å‡†å¤‡Sparkä¾èµ–ï¼ˆé¦–æ¬¡éœ€è¦ï¼‰
./scripts/prepare_spark_jars.sh
# ç­‰å¾…jaræ–‡ä»¶ä¸‹è½½å®Œæˆ

# 7. å¯åŠ¨Spark Streaming
./scripts/start_spark_streaming_fixed.sh
```

**é¢„æœŸè¾“å‡ºï¼š**

Sparkåº”æ˜¾ç¤ºç±»ä¼¼ä»¥ä¸‹çš„è¾“å‡ºï¼š
```
âœ… Spark session created
ğŸ“Š Connecting to Kafka...
âœ… Connected to Kafka topic: ai-social-raw
-------------------------------------------
Batch: 0
-------------------------------------------
+-------+-----+
|source |count|
+-------+-----+
|twitter|193  |
|reddit |373  |
+-------+-----+
```

---

### åœºæ™¯2ï¼šæ—¥å¸¸å¯åŠ¨ï¼ˆå·²é…ç½®è¿‡ï¼‰

```bash
# 1. å¯åŠ¨åŸºç¡€è®¾æ–½
./scripts/start_full_infrastructure.sh

# 2. å¯åŠ¨é‡‡é›†å™¨
source venv/bin/activate
./scripts/start_collectors.sh

# 3. å¯åŠ¨Sparkå¤„ç†
./scripts/start_spark_streaming_fixed.sh
```

---

### åœºæ™¯3ï¼šæŸ¥çœ‹å®æ—¶Dashboard

```bash
# æ–°ç»ˆç«¯çª—å£
cd ~/Documents/Lakehouse_ai_pipeline
source venv/bin/activate
./scripts/start_dashboard_realtime.sh

# æµè§ˆå™¨è‡ªåŠ¨æ‰“å¼€ http://localhost:8501
```

**DashboardåŠŸèƒ½ï¼š**
- å®æ—¶æ•°æ®ç»Ÿè®¡ï¼ˆTwitter/Redditå¸–å­æ•°ï¼‰
- çƒ­é—¨è¯é¢˜åˆ†æï¼ˆåŸºäºå…³é”®è¯ï¼‰
- æœ€æ´»è·ƒä½œè€…
- æœ€è¿‘å¸–å­åˆ—è¡¨
- è‡ªåŠ¨åˆ·æ–°ï¼ˆå¯é…ç½®ï¼‰

---

### åœºæ™¯4ï¼šåœæ­¢æ‰€æœ‰æœåŠ¡

```bash
# 1. åœæ­¢Spark Streamingï¼ˆåœ¨è¿è¡Œçš„ç»ˆç«¯æŒ‰ Ctrl+Cï¼‰

# 2. åœæ­¢é‡‡é›†å™¨
./scripts/stop_collectors.sh

# 3. åœæ­¢Dashboardï¼ˆæŒ‰ Ctrl+Cï¼‰

# 4. åœæ­¢åŸºç¡€è®¾æ–½
docker-compose -f docker-compose-full.yml down
```

---

### åœºæ™¯5ï¼šæŸ¥çœ‹æ•°æ®å’Œæ—¥å¿—

**æŸ¥çœ‹Kafkaæ¶ˆæ¯ï¼š**
```bash
docker exec -it kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic ai-social-raw \
  --from-beginning \
  --max-messages 10
```

**æŸ¥çœ‹é‡‡é›†å™¨æ—¥å¿—ï¼š**
```bash
# å®æ—¶æŸ¥çœ‹
tail -f logs/twitter_collector.log
tail -f logs/reddit_collector.log

# æŸ¥çœ‹æœ€è¿‘100è¡Œ
tail -100 logs/twitter_collector.log
```

**æŸ¥çœ‹MinIOæ•°æ®ï¼š**
1. è®¿é—® http://localhost:9001
2. ç™»å½• (minioadmin/minioadmin)
3. æŸ¥çœ‹buckets: lakehouse, bronze, silver, gold

**æŸ¥çœ‹Sparkä½œä¸šè¯¦æƒ…ï¼š**
- Master UI: http://localhost:8080
- Application UI: http://localhost:4040
- Worker UI: http://localhost:8081

---

## ğŸ”§ æ•…éšœæ’æŸ¥

### é—®é¢˜1: Dockeræ— æ³•å¯åŠ¨

**ç—‡çŠ¶ï¼š**
```
ERROR: Cannot connect to the Docker daemon
```

**è§£å†³ï¼š**
1. æ‰“å¼€Docker Desktop
2. ç­‰å¾…Dockerå®Œå…¨å¯åŠ¨
3. éªŒè¯ï¼š`docker ps`

---

### é—®é¢˜2: Spark Workerå¯åŠ¨å¤±è´¥

**ç—‡çŠ¶ï¼š**
```
java.io.IOException: Failed to create directory /opt/spark/work/...
```

**è§£å†³ï¼š**

å·²åœ¨ `docker-compose-full.yml` ä¸­ä¿®å¤ï¼š
- Workerä»¥rootç”¨æˆ·è¿è¡Œ
- ä½¿ç”¨ `/tmp/spark-work` ä½œä¸ºå·¥ä½œç›®å½•

å¦‚ä»æœ‰é—®é¢˜ï¼š
```bash
# é‡å¯æœåŠ¡
docker-compose -f docker-compose-full.yml restart spark-worker

# æŸ¥çœ‹æ—¥å¿—
docker-compose -f docker-compose-full.yml logs spark-worker
```

---

### é—®é¢˜3: Sparkæ— æ³•ä¸‹è½½Kafkaè¿æ¥å™¨

**ç—‡çŠ¶ï¼š**
```
FileNotFoundException: /home/spark/.ivy2/cache/resolved-...
```

**è§£å†³ï¼š**

ä½¿ç”¨ä¿®å¤åçš„è„šæœ¬ï¼ˆå·²é¢„ä¸‹è½½jarï¼‰ï¼š
```bash
./scripts/prepare_spark_jars.sh
./scripts/start_spark_streaming_fixed.sh
```

ä¸è¦ä½¿ç”¨ `start_spark_streaming.sh`ï¼ˆå·²å¼ƒç”¨ï¼‰ã€‚

---

### é—®é¢˜4: Twitter API 401é”™è¯¯

**ç—‡çŠ¶ï¼š**
```
401 Unauthorized
```

**è§£å†³ï¼š**
1. æ£€æŸ¥ `config/.env` ä¸­çš„å¯†é’¥æ˜¯å¦æ­£ç¡®
2. ç¡®è®¤ä½¿ç”¨çš„æ˜¯ `TWITTER_BEARER_TOKEN`ï¼ˆä¸æ˜¯API Keyï¼‰
3. éªŒè¯Twitter Developerè´¦å·çŠ¶æ€

**æµ‹è¯•è¿æ¥ï¼š**
```bash
source venv/bin/activate
python -c "
import os
from dotenv import load_dotenv
load_dotenv('config/.env')
print('Bearer Token:', os.getenv('TWITTER_BEARER_TOKEN')[:20] + '...')
"
```

---

### é—®é¢˜5: Redditè¿æ¥å¤±è´¥

**ç—‡çŠ¶ï¼š**
```
received 404 HTTP response
```

**å¯èƒ½åŸå› ï¼š**
- Subredditåç§°é”™è¯¯æˆ–ä¸å­˜åœ¨
- APIé™æµ

**è§£å†³ï¼š**

ç¼–è¾‘ `data_ingestion/reddit/collector.py`ï¼Œæ³¨é‡Šæ‰é—®é¢˜subredditï¼š
```python
TARGET_SUBREDDITS = [
    'MachineLearning',
    'LocalLLaMA',
    # 'ArtificialIntelligence',  # æš‚æ—¶ç¦ç”¨
]
```

---

### é—®é¢˜6: Dashboardæ— æ•°æ®

**ç—‡çŠ¶ï¼š**
Dashboardæ˜¾ç¤º "No data available"

**æ£€æŸ¥æ¸…å•ï¼š**

```bash
# 1. Kafkaæ˜¯å¦æœ‰æ•°æ®ï¼Ÿ
docker exec -it kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic ai-social-raw \
  --from-beginning \
  --max-messages 5

# 2. é‡‡é›†å™¨æ˜¯å¦è¿è¡Œï¼Ÿ
ps aux | grep collector

# 3. æŸ¥çœ‹é‡‡é›†å™¨æ—¥å¿—
tail -20 logs/twitter_collector.log
```

---

### é—®é¢˜7: ç«¯å£å ç”¨

**ç—‡çŠ¶ï¼š**
```
Port 8080 is already in use
```

**è§£å†³ï¼š**
```bash
# æŸ¥æ‰¾å ç”¨è¿›ç¨‹
lsof -i :8080

# åœæ­¢å†²çªæœåŠ¡
docker-compose -f docker-compose-full.yml down

# æˆ–æ€æ‰è¿›ç¨‹
kill -9 <PID>
```

---

## ğŸ¯ ä¸‹ä¸€æ­¥æ‰©å±•

### Phase 3: æ·»åŠ Delta LakeæŒä¹…åŒ–

**ç›®æ ‡ï¼š** å°†å¤„ç†åçš„æ•°æ®å†™å…¥Delta Lake

**å®æ–½ï¼š**
1. ä¿®æ”¹ `streaming/spark/processor.py`
2. æ·»åŠ Delta Lakeå†™å…¥é€»è¾‘
3. å®ç°Bronze/Silver/Goldä¸‰å±‚
4. Dashboardä»Delta Lakeè¯»å–

**é¢„è®¡æ—¶é—´ï¼š** 1-2å‘¨

---

### Phase 4: å‘é‡æœç´¢å’ŒRAG

**ç›®æ ‡ï¼š** è¯­ä¹‰æœç´¢å’Œæ™ºèƒ½é—®ç­”

**æŠ€æœ¯æ ˆï¼š**
- Milvusï¼ˆå‘é‡æ•°æ®åº“ï¼‰
- Sentence Transformersï¼ˆæ–‡æœ¬embeddingï¼‰
- LangChain + OpenAI/Claude API

**åŠŸèƒ½ï¼š**
- è¯­ä¹‰ç›¸ä¼¼æœç´¢
- è‡ªç„¶è¯­è¨€é—®ç­”
- è¶‹åŠ¿é¢„æµ‹

**é¢„è®¡æ—¶é—´ï¼š** 2-3å‘¨

---

### Phase 5: ç”Ÿäº§åŒ–éƒ¨ç½²

**ç›®æ ‡ï¼š** éƒ¨ç½²åˆ°äº‘ç«¯ï¼Œé«˜å¯ç”¨

**æ”¹è¿›ï¼š**
- è¿ç§»åˆ°AWS/Azure
- Kafkaé›†ç¾¤ï¼ˆ3èŠ‚ç‚¹ï¼‰
- Spark HAé…ç½®
- ç›‘æ§å‘Šè­¦ï¼ˆPrometheus + Grafanaï¼‰
- CI/CDæµæ°´çº¿

---

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

### å½“å‰ç³»ç»Ÿå®¹é‡

| æŒ‡æ ‡ | å½“å‰å€¼ | ç†è®ºä¸Šé™ |
|------|--------|----------|
| é‡‡é›†é¢‘ç‡ | 5åˆ†é’Ÿ/æ¬¡ | APIé™åˆ¶ |
| Kafkaåå | ~100msg/s | 10K msg/s |
| Sparkå¤„ç†å»¶è¿Ÿ | <1ç§’ | <100ms |
| å­˜å‚¨ | MinIO | æ— é™ |
| æ•°æ®ä¿ç•™ | Kafka 7å¤© | å¯é…ç½® |

### èµ„æºä½¿ç”¨

- **Dockerå†…å­˜**: å»ºè®®4GB+
- **ç£ç›˜ç©ºé—´**: 10GB+ï¼ˆæ•°æ®å¢é•¿ï¼‰
- **CPU**: 2æ ¸å¿ƒ+

---

## ğŸ“š å¿«é€Ÿå‘½ä»¤å‚è€ƒ

```bash
# === å¯åŠ¨ç›¸å…³ ===
./scripts/start_full_infrastructure.sh    # å¯åŠ¨åŸºç¡€è®¾æ–½
./scripts/start_collectors.sh             # å¯åŠ¨é‡‡é›†å™¨
./scripts/start_spark_streaming_fixed.sh  # å¯åŠ¨Spark
./scripts/start_dashboard_realtime.sh     # å¯åŠ¨Dashboard

# === åœæ­¢ç›¸å…³ ===
./scripts/stop_collectors.sh              # åœæ­¢é‡‡é›†å™¨
docker-compose -f docker-compose-full.yml down  # åœæ­¢åŸºç¡€è®¾æ–½

# === æŸ¥çœ‹ç›¸å…³ ===
docker ps                                 # æŸ¥çœ‹å®¹å™¨çŠ¶æ€
docker-compose -f docker-compose-full.yml logs -f  # æŸ¥çœ‹æ‰€æœ‰æ—¥å¿—
tail -f logs/twitter_collector.log        # æŸ¥çœ‹Twitteræ—¥å¿—
tail -f logs/reddit_collector.log         # æŸ¥çœ‹Redditæ—¥å¿—

# === æµ‹è¯•ç›¸å…³ ===
docker exec -it kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic ai-social-raw \
  --from-beginning \
  --max-messages 10                       # æŸ¥çœ‹Kafkaæ¶ˆæ¯

curl http://localhost:8080                # æµ‹è¯•Spark Master
curl http://localhost:9000/minio/health/live  # æµ‹è¯•MinIO
```

---

## ğŸ†˜ è·å–å¸®åŠ©

### æŸ¥çœ‹æ—¥å¿—ä½ç½®

- é‡‡é›†å™¨æ—¥å¿—: `logs/*.log`
- Sparkæ—¥å¿—: `docker-compose -f docker-compose-full.yml logs spark-master`
- Kafkaæ—¥å¿—: `docker-compose -f docker-compose-full.yml logs kafka`

### é‡ç½®ç³»ç»Ÿ

```bash
# å®Œå…¨é‡ç½®ï¼ˆåˆ é™¤æ‰€æœ‰æ•°æ®ï¼‰
docker-compose -f docker-compose-full.yml down -v
rm -rf logs/*.log
# ç„¶åé‡æ–°å¯åŠ¨
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0
**æœ€åæ›´æ–°**: 2025-11-11
**ç»´æŠ¤è€…**: Guangyao Li
