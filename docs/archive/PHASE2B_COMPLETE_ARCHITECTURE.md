# Phase 2B: å®Œæ•´Lakehouseæ¶æ„å®æ–½æŒ‡å—

## ğŸ¯ ç›®æ ‡

å®ç°å®Œæ•´çš„æ•°æ®å¤„ç†pipelineï¼š
```
Twitter/Reddit â†’ é‡‡é›†å™¨ â†’ Kafka â†’ Spark Streaming â†’ MinIO â†’ Dashboard
```

---

## ğŸš€ å®æ–½æ­¥éª¤

### Step 1: åœæ­¢å½“å‰çš„minimalåŸºç¡€è®¾æ–½

```bash
cd ~/Documents/Lakehouse_ai_pipeline

# åœæ­¢minimalç‰ˆæœ¬
docker-compose -f docker-compose-minimal.yml down
```

### Step 2: å¯åŠ¨å®Œæ•´åŸºç¡€è®¾æ–½ï¼ˆåŒ…å«Sparkï¼‰

```bash
# å¯åŠ¨å®Œæ•´ç‰ˆï¼ˆKafka + MinIO + Sparkï¼‰
./scripts/start_full_infrastructure.sh
```

**ç­‰å¾…çº¦30ç§’è®©æ‰€æœ‰æœåŠ¡å¯åŠ¨å®Œæˆã€‚**

ä½ åº”è¯¥çœ‹åˆ°ï¼š
```
âœ… Kafka is ready
âœ… MinIO is ready
âœ… Spark Master is ready
âœ… Spark Worker is ready
```

### Step 3: éªŒè¯Sparké›†ç¾¤

**è®¿é—®Spark Master UI:**

æµè§ˆå™¨æ‰“å¼€ï¼šhttp://localhost:8080

ä½ åº”è¯¥çœ‹åˆ°ï¼š
- **Workers**: 1 (Alive)
- **Cores**: 2
- **Memory**: 2.0 GB

è¿™è¯´æ˜Spark Masterå’ŒWorkerå·²æˆåŠŸè¿æ¥ï¼

### Step 4: ç¡®ä¿é‡‡é›†å™¨æ­£åœ¨è¿è¡Œ

```bash
# æ£€æŸ¥é‡‡é›†å™¨çŠ¶æ€
ps aux | grep collector

# å¦‚æœæ²¡è¿è¡Œï¼Œå¯åŠ¨å®ƒä»¬
./scripts/start_collectors.sh

# æŸ¥çœ‹æ—¥å¿—
tail -f logs/twitter_collector.log
```

### Step 5: è¿è¡ŒSpark Streamingä½œä¸š

```bash
# å¯åŠ¨Sparkå¤„ç†ä½œä¸š
./scripts/start_spark_streaming.sh
```

**ä½ åº”è¯¥çœ‹åˆ°ï¼š**

```
ğŸš€ Starting Spark Streaming Job...
âœ… Spark Master is running
ğŸ“¦ Copying streaming script to Spark Master...
ğŸ”¥ Submitting Spark job...
```

ç„¶åSparkä¼šå¼€å§‹è¾“å‡ºå¤„ç†çš„æ•°æ®ï¼š

```
+--------+-------------------+-------------------+
|source  |event_timestamp    |processed_at       |
+--------+-------------------+-------------------+
|twitter |2025-11-11T16:47:44|2025-11-11 17:00:00|
|reddit  |2025-11-11T16:49:06|2025-11-11 17:00:00|
+--------+-------------------+-------------------+

+--------+-----+
|source  |count|
+--------+-----+
|twitter |96   |
|reddit  |2    |
+--------+-----+
```

è¿™è¯´æ˜Sparkæ­£åœ¨**å®æ—¶å¤„ç†Kafkaä¸­çš„æ•°æ®**ï¼

### Step 6: éªŒè¯å®Œæ•´æ•°æ®æµ

ç°åœ¨ä½ çš„å®Œæ•´pipelineæ­£åœ¨è¿è¡Œï¼š

```
Twitter API â†’ Pythoné‡‡é›†å™¨ âœ…
      â†“
   Kafka âœ…
      â†“
Spark Streaming âœ… â† æ­£åœ¨å®æ—¶å¤„ç†
      â†“
   Consoleè¾“å‡º âœ…
```

---

## ğŸ” ç›‘æ§å’ŒéªŒè¯

### æŸ¥çœ‹Sparkä½œä¸šçŠ¶æ€

**Spark Master UI:**
- http://localhost:8080
- æŸ¥çœ‹ "Running Applications"

**Spark Application UI:**
- http://localhost:4040
- æŸ¥çœ‹æµå¤„ç†ç»Ÿè®¡ã€Stageã€Executorä¿¡æ¯

### æŸ¥çœ‹Kafkaæ¶ˆæ¯

```bash
# æŸ¥çœ‹topicä¸­çš„æ¶ˆæ¯æ•°é‡
docker exec kafka kafka-run-class kafka.tools.GetOffsetShell \
  --broker-list localhost:9092 \
  --topic ai-social-raw
```

### æŸ¥çœ‹å®¹å™¨çŠ¶æ€

```bash
# æŸ¥çœ‹æ‰€æœ‰å®¹å™¨
docker-compose -f docker-compose-full.yml ps

# æŸ¥çœ‹Spark Masteræ—¥å¿—
docker-compose -f docker-compose-full.yml logs -f spark-master

# æŸ¥çœ‹Spark Workeræ—¥å¿—
docker-compose -f docker-compose-full.yml logs -f spark-worker
```

---

## ğŸ“Š å½“å‰ vs ç›®æ ‡æ¶æ„

### å½“å‰å®ç°ï¼ˆPhase 2B - Step 1ï¼‰

```
Twitter/Reddit API
       â†“
   é‡‡é›†å™¨ âœ…
       â†“
    Kafka âœ…
       â†“
 Spark Streaming âœ… (è¾“å‡ºåˆ°Console)
```

### ä¸‹ä¸€æ­¥ï¼ˆPhase 2B - Step 2ï¼‰

æ·»åŠ Delta Lakeå­˜å‚¨ï¼š

```
Twitter/Reddit API
       â†“
   é‡‡é›†å™¨ âœ…
       â†“
    Kafka âœ…
       â†“
 Spark Streaming âœ…
       â†“
  Delta Lake (MinIO) â† å¾…å®ç°
  (Bronze/Silver/Gold)
       â†“
   Dashboard
```

---

## ğŸ¯ ä¸‹ä¸€æ­¥ä»»åŠ¡

### ä»»åŠ¡1: å®ç°Delta Lakeå†™å…¥

ä¿®æ”¹ `streaming/spark/processor.py` æ·»åŠ Delta Lakeæ”¯æŒï¼š

1. å®‰è£…Delta LakeåŒ…
2. é…ç½®MinIOè¿æ¥
3. å®ç°Bronzeå±‚å†™å…¥ï¼ˆåŸå§‹æ•°æ®ï¼‰
4. å®ç°Silverå±‚å†™å…¥ï¼ˆæ¸…æ´—åï¼‰
5. å®ç°Goldå±‚å†™å…¥ï¼ˆèšåˆæ•°æ®ï¼‰

### ä»»åŠ¡2: Dashboardè¿æ¥Delta Lake

ä¿®æ”¹Dashboardä»Delta Lakeè¯»å–æ•°æ®è€Œä¸æ˜¯ç›´æ¥ä»Kafkaã€‚

### ä»»åŠ¡3: å®ç°çƒ­ç‚¹æ£€æµ‹ç®—æ³•

åœ¨Sparkä¸­å®ç°çœŸæ­£çš„è¶‹åŠ¿æ£€æµ‹ï¼š
- æ»‘åŠ¨çª—å£èšåˆ
- å…³é”®è¯é¢‘ç‡ç»Ÿè®¡
- è¯é¢˜çªå‘æ£€æµ‹

---

## ğŸ› ï¸ æ•…éšœæ’æŸ¥

### Sparkå¯åŠ¨å¤±è´¥

**æ£€æŸ¥Dockerèµ„æºï¼š**
- Docker Desktop â†’ Settings â†’ Resources
- æ¨èï¼šè‡³å°‘4GBå†…å­˜

**æŸ¥çœ‹é”™è¯¯æ—¥å¿—ï¼š**
```bash
docker-compose -f docker-compose-full.yml logs spark-master
docker-compose -f docker-compose-full.yml logs spark-worker
```

### Spark Workeræœªè¿æ¥åˆ°Master

**æ£€æŸ¥ç½‘ç»œï¼š**
```bash
docker exec spark-worker ping spark-master
```

**é‡å¯Workerï¼š**
```bash
docker-compose -f docker-compose-full.yml restart spark-worker
```

### Sparkæ— æ³•è¯»å–Kafka

**æ£€æŸ¥Kafkaåœ°å€ï¼š**
- åœ¨Sparkä¸­ä½¿ç”¨ï¼š`kafka:29092`ï¼ˆå®¹å™¨å†…éƒ¨ï¼‰
- åœ¨ä¸»æœºä½¿ç”¨ï¼š`localhost:9092`

**éªŒè¯Kafkaè¿æ¥ï¼š**
```bash
docker exec spark-master nc -zv kafka 29092
```

### é•œåƒä¸‹è½½æ…¢

**ä½¿ç”¨Dockeré•œåƒåŠ é€Ÿï¼š**

ç¼–è¾‘ `~/.docker/daemon.json`ï¼š
```json
{
  "registry-mirrors": [
    "https://docker.mirrors.ustc.edu.cn"
  ]
}
```

é‡å¯Docker Desktopã€‚

---

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### Sparké…ç½®è°ƒä¼˜

ç¼–è¾‘ `scripts/start_spark_streaming.sh`ï¼š

```bash
--conf spark.executor.memory=2g \
--conf spark.driver.memory=2g \
--conf spark.executor.cores=2 \
--conf spark.sql.shuffle.partitions=4
```

### Kafkaè°ƒä¼˜

ç¼–è¾‘ `docker-compose-full.yml` Kafkaç¯å¢ƒå˜é‡ï¼š

```yaml
KAFKA_NUM_PARTITIONS: 4  # å¢åŠ åˆ†åŒºæ•°
KAFKA_LOG_RETENTION_HOURS: 24  # å‡å°‘ä¿ç•™æ—¶é—´
```

---

## ğŸ‰ æˆå°±è§£é”

å®Œæˆè¿™ä¸ªé˜¶æ®µåï¼Œä½ å°†æ‹¥æœ‰ï¼š

âœ… **å®Œæ•´çš„LakehouseåŸºç¡€è®¾æ–½**
- Kafkaæ¶ˆæ¯é˜Ÿåˆ—
- MinIOå¯¹è±¡å­˜å‚¨
- Sparkåˆ†å¸ƒå¼å¤„ç†

âœ… **å®æ—¶æ•°æ®å¤„ç†pipeline**
- æ•°æ®é‡‡é›† â†’ é˜Ÿåˆ— â†’ å¤„ç†

âœ… **å¯æ‰©å±•çš„æ¶æ„**
- å¯ä»¥æ·»åŠ æ›´å¤šWorker
- å¯ä»¥å¤„ç†æ›´å¤§æ•°æ®é‡

---

## ğŸ“š å­¦ä¹ èµ„æº

### Spark Streaming
- [å®˜æ–¹æ–‡æ¡£](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)
- [Spark + Kafkaé›†æˆ](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html)

### Delta Lake
- [Delta Lakeå®˜æ–¹æ–‡æ¡£](https://docs.delta.io/)
- [Delta Lake with Spark](https://docs.delta.io/latest/quick-start.html)

### MinIO
- [MinIO Pythonå®¢æˆ·ç«¯](https://min.io/docs/minio/linux/developers/python/minio-py.html)
- [S3Aé…ç½®](https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html)

---

**å‡†å¤‡å¥½äº†å—ï¼Ÿå¼€å§‹å®æ–½ï¼** ğŸš€

```bash
# 1. åœæ­¢minimalåŸºç¡€è®¾æ–½
docker-compose -f docker-compose-minimal.yml down

# 2. å¯åŠ¨å®Œæ•´åŸºç¡€è®¾æ–½
./scripts/start_full_infrastructure.sh

# 3. éªŒè¯Spark UI
# æµè§ˆå™¨æ‰“å¼€: http://localhost:8080

# 4. å¯åŠ¨Spark Streaming
./scripts/start_spark_streaming.sh
```

æœ‰ä»»ä½•é—®é¢˜éšæ—¶å‘Šè¯‰æˆ‘ï¼ğŸ’ª
