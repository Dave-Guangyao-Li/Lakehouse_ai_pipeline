"""
Streamlit Dashboard for AI Trend Monitoring - å®Œå…¨é‡æ„ç‰ˆæœ¬
- çœŸæ­£çš„5ç§’å€’è®¡æ—¶
- NLPæå–çœŸå®å…³é”®è¯
- è¯äº‘å¯è§†åŒ–
- Redditå¡ç‰‡æ ·å¼
- ç°ä»£åŒ–UIè®¾è®¡
"""
import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import time
import sys
import os
from collections import Counter
import re

# NLP imports
try:
    import spacy
    from wordcloud import WordCloud
    import matplotlib.pyplot as plt
    import nltk
    from nltk.corpus import stopwords

    NLP_AVAILABLE = True

    # åŠ è½½spacyæ¨¡å‹
    try:
        nlp = spacy.load('en_core_web_sm')
    except:
        NLP_AVAILABLE = False

    # ä¸‹è½½stopwordsï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
    try:
        STOP_WORDS = set(stopwords.words('english'))
    except LookupError:
        try:
            nltk.download('stopwords', quiet=True)
            STOP_WORDS = set(stopwords.words('english'))
        except:
            STOP_WORDS = set()

    # è‡ªå®šä¹‰æ‰©å±•åœç”¨è¯ï¼ˆè™šè¯ã€ä»£è¯ã€å¸¸è§æ— æ„ä¹‰è¯ï¼‰
    CUSTOM_STOP_WORDS = {
        'that', 'this', 'these', 'those', 'them', 'they', 'their', 'theirs',
        'people', 'anyone', 'someone', 'something', 'anything', 'everything',
        'which', 'what', 'where', 'when', 'who', 'whom', 'whose',
        'the us', 'the model', 'the future', 'the world', 'the company',
        'a lot', 'lot', 'thing', 'things', 'stuff', 'way', 'ways',
        'time', 'times', 'question', 'thanks', 'thank',
        # å¸¸è§Reddit/ç¤¾äº¤åª’ä½“è¯
        'post', 'comment', 'thread', 'subreddit', 'user', 'upvote',
        # é€šç”¨ä»£è¯å’Œé™å®šè¯
        'anyone', 'everyone', 'someone', 'nobody', 'anybody', 'somebody',
        'all', 'some', 'many', 'few', 'much', 'none',
    }
    STOP_WORDS.update(CUSTOM_STOP_WORDS)

except:
    NLP_AVAILABLE = False
    STOP_WORDS = set()

# Add parent directory to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    from kafka_reader import KafkaDataReader
    KAFKA_AVAILABLE = True
except Exception as e:
    KAFKA_AVAILABLE = False

# Page configuration
st.set_page_config(
    page_title="AI Trend Monitor - Live",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# å¢å¼ºCSS - ç°ä»£åŒ–è®¾è®¡
st.markdown("""
<style>
    /* æ•´ä½“èƒŒæ™¯ */
    .main {
        background: linear-gradient(135deg, #f5f7fa 0%, #e8eef5 100%);
    }

    /* ä¸»æ ‡é¢˜ */
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1E88E5;
        text-align: center;
        padding: 1rem 0;
        text-shadow: 2px 2px 4px rgba(0,0,0,0.1);
    }

    /* å®æ—¶çŠ¶æ€ç¯ */
    @keyframes blink {
        0%, 100% { opacity: 1; }
        50% { opacity: 0.3; }
    }

    .live-indicator {
        display: inline-block;
        width: 12px;
        height: 12px;
        background-color: #FF5252;
        border-radius: 50%;
        margin-right: 8px;
        animation: blink 1.5s infinite;
        box-shadow: 0 0 10px rgba(255, 82, 82, 0.5);
    }

    /* å€’è®¡æ—¶æ ·å¼ */
    .countdown-container {
        background: white;
        border-radius: 8px;
        padding: 12px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }

    .countdown {
        font-size: 1.1rem;
        color: #FF9800;
        font-weight: bold;
    }

    /* å®æ—¶æ—¶é’Ÿ */
    .realtime-clock {
        font-size: 1.1rem;
        font-weight: 500;
        color: #424242;
        background: white;
        border-radius: 8px;
        padding: 12px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }

    /* Metricå¡ç‰‡ç¾åŒ– */
    div[data-testid="stMetric"] {
        background: white;
        padding: 20px;
        border-radius: 12px;
        box-shadow: 0 4px 6px rgba(0,0,0,0.07);
        transition: transform 0.2s, box-shadow 0.2s;
    }

    div[data-testid="stMetric"]:hover {
        transform: translateY(-2px);
        box-shadow: 0 6px 12px rgba(0,0,0,0.12);
    }

    /* è¿›åº¦æ¡ç¾åŒ– */
    .stProgress > div > div > div > div {
        background: linear-gradient(90deg, #1E88E5, #42A5F5);
    }

    /* Redditå¡ç‰‡æ ·å¼ */
    .reddit-card {
        background: white;
        border: 1px solid #e0e0e0;
        border-radius: 12px;
        padding: 16px;
        margin-bottom: 12px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.08);
        transition: all 0.2s;
    }

    .reddit-card:hover {
        box-shadow: 0 4px 12px rgba(0,0,0,0.15);
        transform: translateY(-2px);
        border-color: #FF4500;
    }

    .card-header {
        display: flex;
        align-items: center;
        margin-bottom: 10px;
        gap: 8px;
    }

    .source-badge {
        padding: 4px 10px;
        border-radius: 12px;
        font-size: 0.75rem;
        font-weight: bold;
        color: white;
    }

    .badge-reddit {
        background: #FF4500;
    }

    .badge-twitter {
        background: #1DA1F2;
    }

    .card-meta {
        color: #787C7E;
        font-size: 0.875rem;
    }

    .card-title {
        font-size: 1.05rem;
        font-weight: 600;
        color: #1c1c1c;
        margin: 10px 0;
        line-height: 1.4;
    }

    .card-footer {
        display: flex;
        gap: 16px;
        color: #787C7E;
        font-size: 0.875rem;
        margin-top: 10px;
        padding-top: 10px;
        border-top: 1px solid #f0f0f0;
    }

    /* Tabæ ·å¼ç¾åŒ– */
    .stTabs [data-baseweb="tab-list"] {
        gap: 8px;
        background-color: white;
        border-radius: 8px;
        padding: 4px;
    }

    .stTabs [data-baseweb="tab"] {
        border-radius: 4px;
        padding: 8px 16px;
    }
</style>
""", unsafe_allow_html=True)


# åˆå§‹åŒ– session_state
if 'previous_count' not in st.session_state:
    st.session_state.previous_count = 0
if 'refresh_count' not in st.session_state:
    st.session_state.refresh_count = 0


@st.cache_data(ttl=300)  # ç¼“å­˜5åˆ†é’Ÿ
def load_real_data():
    """ä»KafkaåŠ è½½å®æ—¶æ•°æ®"""
    if not KAFKA_AVAILABLE:
        return None, 0

    try:
        reader = KafkaDataReader(
            bootstrap_servers='localhost:9092',
            topic='ai-social-raw'
        )

        # è·å–æ€»æ¶ˆæ¯æ•°
        total_count = reader.get_message_count()

        # è·å–æ‰€æœ‰æ¶ˆæ¯
        messages = reader.get_all_messages()

        if not messages:
            return None, total_count

        df = reader.parse_to_dataframe(messages)

        if df.empty:
            return None, total_count

        return df, total_count

    except Exception as e:
        st.error(f"Error loading data: {e}")
        return None, 0


def extract_real_keywords(df, top_n=20):
    """ä½¿ç”¨NLPæå–çœŸå®çš„AIä¸»é¢˜è¯ï¼ˆè¿‡æ»¤è™šè¯å’Œåœç”¨è¯ï¼‰"""
    if df is None or df.empty or not NLP_AVAILABLE:
        return pd.DataFrame()

    try:
        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬
        all_text = ' '.join(df['text'].astype(str).tolist())

        # æ¸…ç†æ–‡æœ¬
        all_text = re.sub(r'http\S+|www\S+|https\S+', '', all_text)  # ç§»é™¤URL
        all_text = re.sub(r'[^\w\s]', ' ', all_text)  # ç§»é™¤æ ‡ç‚¹

        # ä½¿ç”¨spacyå¤„ç†ï¼ˆé™åˆ¶é•¿åº¦é¿å…è¶…æ—¶ï¼‰
        doc = nlp(all_text[:200000])

        # æå–æœ‰æ„ä¹‰çš„è¯ï¼ˆåªä¿ç•™åè¯å’Œä¸“æœ‰åè¯ï¼‰
        keywords = []

        for token in doc:
            # è¯æ€§è¿‡æ»¤ï¼šåªä¿ç•™åè¯(NOUN)å’Œä¸“æœ‰åè¯(PROPN)
            if token.pos_ in ['NOUN', 'PROPN']:
                # è¯å½¢è¿˜åŸï¼ˆå°†å¤æ•°å˜å•æ•°ã€åŠ¨è¯å˜åŸå½¢ç­‰ï¼‰
                word = token.lemma_.lower()

                # ä¸¥æ ¼è¿‡æ»¤æ¡ä»¶
                if (len(word) >= 3  # è‡³å°‘3ä¸ªå­—ç¬¦
                    and word not in STOP_WORDS  # ä¸åœ¨åœç”¨è¯è¡¨
                    and not word.isdigit()  # ä¸æ˜¯çº¯æ•°å­—
                    and token.is_alpha  # åªåŒ…å«å­—æ¯
                    and word not in ['artificial intelligence', 'machine learning', 'deep learning']  # è¿‡æ»¤é€šç”¨AIè¯
                    and not word.startswith('ai')  # è¿‡æ»¤aiå¼€å¤´çš„è¯
                    ):
                    keywords.append(word)

        # ç»Ÿè®¡é¢‘ç‡
        keyword_counts = Counter(keywords).most_common(top_n)

        if not keyword_counts:
            return pd.DataFrame()

        return pd.DataFrame(keyword_counts, columns=['keyword', 'mentions'])

    except Exception as e:
        print(f"âŒ å…³é”®è¯æå–é”™è¯¯: {e}")
        return pd.DataFrame()


def check_reddit_collector_status():
    """æ£€æŸ¥Reddité‡‡é›†å™¨è¿è¡ŒçŠ¶æ€"""
    log_file = 'logs/reddit_collector.log'

    try:
        # æ£€æŸ¥æ—¥å¿—æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(log_file):
            return {
                'status': 'unknown',
                'message': 'æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨',
                'rate_limited': False,
                'last_success': False
            }

        # è¯»å–æœ€å100è¡Œæ—¥å¿—
        with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
            lines = f.readlines()[-100:]

        if not lines:
            return {'status': 'unknown', 'message': 'æ—¥å¿—ä¸ºç©º', 'rate_limited': False, 'last_success': False}

        # æ£€æŸ¥rate limitï¼ˆæœ€è¿‘çš„æ—¥å¿—ï¼‰
        recent_lines = lines[-20:]
        has_rate_limit = any(
            'rate limit' in line.lower()
            or '429' in line
            or 'too many requests' in line.lower()
            for line in recent_lines
        )

        # æ£€æŸ¥æœ€è¿‘æ˜¯å¦æœ‰æˆåŠŸé‡‡é›†
        has_success = any(
            'successfully' in line.lower()
            or 'âœ…' in line
            or 'sent to kafka' in line.lower()
            for line in recent_lines
        )

        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
        has_error = any(
            'error' in line.lower()
            or 'âŒ' in line
            or 'failed' in line.lower()
            for line in recent_lines
        )

        # åˆ¤æ–­çŠ¶æ€
        if has_rate_limit:
            status = 'limited'
            message = 'APIè°ƒç”¨å—é™ï¼Œç­‰å¾…æ¢å¤ä¸­...'
        elif has_success:
            status = 'running'
            message = 'æ­£å¸¸è¿è¡Œ'
        elif has_error:
            status = 'error'
            message = 'é‡‡é›†å‡ºç°é”™è¯¯'
        else:
            status = 'idle'
            message = 'ç©ºé—²æˆ–ç­‰å¾…ä¸­'

        return {
            'status': status,
            'message': message,
            'rate_limited': has_rate_limit,
            'last_success': has_success
        }

    except Exception as e:
        return {
            'status': 'unknown',
            'message': f'æ— æ³•è¯»å–çŠ¶æ€: {str(e)}',
            'rate_limited': False,
            'last_success': False
        }


def create_word_cloud(df):
    """ç”Ÿæˆè¯äº‘"""
    if df is None or df.empty or not NLP_AVAILABLE:
        st.info("è¯äº‘åŠŸèƒ½éœ€è¦NLPåº“æ”¯æŒ")
        return

    try:
        keywords_df = extract_real_keywords(df, top_n=100)

        if keywords_df.empty:
            st.warning("æ— æ³•ç”Ÿæˆè¯äº‘ï¼šæ²¡æœ‰æå–åˆ°å…³é”®è¯")
            return

        word_freq = dict(zip(keywords_df['keyword'], keywords_df['mentions']))

        wordcloud = WordCloud(
            width=800,
            height=400,
            background_color='white',
            colormap='Blues',
            max_words=50,
            relative_scaling=0.5,
            min_font_size=12,
            collocations=False
        ).generate_from_frequencies(word_freq)

        fig, ax = plt.subplots(figsize=(10, 5))
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis('off')
        st.pyplot(fig)

    except Exception as e:
        st.error(f"è¯äº‘ç”Ÿæˆé”™è¯¯: {e}")


def render_reddit_card(row):
    """æ¸²æŸ“Reddité£æ ¼å¡ç‰‡"""
    source = row.get('source', 'Unknown')
    author = row.get('author', 'Unknown')
    text = str(row.get('text', ''))[:200]
    engagement = row.get('engagement', 0)
    subreddit = row.get('subreddit', '')

    badge_class = 'badge-reddit' if source == 'Reddit' else 'badge-twitter'

    card_html = f"""
    <div class="reddit-card">
        <div class="card-header">
            <span class="source-badge {badge_class}">{source}</span>
            <span class="card-meta">
                {'r/' + subreddit if subreddit else ''} {'â€¢ ' if subreddit else ''}by u/{author}
            </span>
        </div>

        <div class="card-title">{text}...</div>

        <div class="card-footer">
            <span>ğŸ‘ {engagement:,}</span>
            <span>ğŸ’¬ è¯„è®º</span>
            <span>ğŸ”— åˆ†äº«</span>
        </div>
    </div>
    """

    return card_html


def main():
    """ä¸»åº”ç”¨"""

    # === é¡¶éƒ¨çŠ¶æ€æ  ===
    col_time, col_countdown, col_btn = st.columns([2, 2, 1])

    with col_time:
        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        st.markdown(f'<div class="realtime-clock">â° <b>å½“å‰æ—¶é—´</b>: {current_time}</div>',
                   unsafe_allow_html=True)

    # åˆ›å»ºå€’è®¡æ—¶å ä½ç¬¦
    countdown_placeholder = col_countdown.empty()
    progress_placeholder = col_countdown.empty()

    with col_btn:
        if st.button("ğŸ”„ ç«‹å³åˆ·æ–°", use_container_width=True):
            st.cache_data.clear()
            st.session_state.refresh_count += 1
            st.rerun()

    st.markdown("---")

    # === ä¸»æ ‡é¢˜ ===
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.markdown('<div class="main-header">ğŸ¤– AI Trend Monitor</div>', unsafe_allow_html=True)
        st.markdown("""
            <center>
                <span class="live-indicator"></span>
                <span style="font-size: 1.1rem; font-weight: 600; color: #FF5252;">LIVE DATA</span>
            </center>
        """, unsafe_allow_html=True)

    st.markdown("Real-time monitoring of AI discussions from Twitter and Reddit")

    # === ä¾§è¾¹æ  ===
    with st.sidebar:
        st.header("âš™ï¸ Settings")

        auto_refresh = st.checkbox("Auto-refresh", value=True)

        st.markdown("---")

        st.subheader("ğŸ“Š Data Sources")
        if KAFKA_AVAILABLE:
            st.success("âœ… Kafka Connected")
        else:
            st.error("âŒ Kafka Unavailable")

        st.markdown("---")

        st.subheader("ğŸ¤– Collectors Status")

        # TwitterçŠ¶æ€ï¼ˆæš‚åœï¼‰
        st.markdown("- ğŸ¦ **Twitter**: â¸ï¸ æš‚åœ")

        # RedditçŠ¶æ€ï¼ˆåŠ¨æ€æ£€æŸ¥ï¼‰
        reddit_status = check_reddit_collector_status()

        if reddit_status['status'] == 'limited':
            st.markdown("- ğŸ¤– **Reddit**: âš ï¸ Rate Limited")
            st.caption(f"   {reddit_status['message']}")
        elif reddit_status['status'] == 'running':
            st.markdown("- ğŸ¤– **Reddit**: âœ… æ­£å¸¸è¿è¡Œ")
            st.caption("   ğŸ“¡ é‡‡é›†é¢‘ç‡: 60ç§’/æ¬¡")
        elif reddit_status['status'] == 'error':
            st.markdown("- ğŸ¤– **Reddit**: âŒ å‡ºç°é”™è¯¯")
            st.caption(f"   {reddit_status['message']}")
        elif reddit_status['status'] == 'idle':
            st.markdown("- ğŸ¤– **Reddit**: â¸ï¸ ç©ºé—²ä¸­")
            st.caption(f"   {reddit_status['message']}")
        else:
            st.markdown("- ğŸ¤– **Reddit**: â“ çŠ¶æ€æœªçŸ¥")
            st.caption(f"   {reddit_status['message']}")

        if NLP_AVAILABLE:
            st.success("âœ… NLPåŠŸèƒ½å¯ç”¨")
        else:
            st.warning("âš ï¸ NLPåŠŸèƒ½ä¸å¯ç”¨")

    # === åŠ è½½æ•°æ® ===
    with st.spinner("ğŸ“Š Loading real-time data from Kafka..."):
        df, total_count = load_real_data()

    # === è®¡ç®—æ–°å¢æ•°æ® ===
    new_data_count = total_count - st.session_state.previous_count
    if new_data_count != 0:
        st.session_state.previous_count = total_count

    # === æ˜¾ç¤ºæ–°å¢æç¤º ===
    if new_data_count > 0:
        st.success(f"ğŸ†• æ–°å¢ +{new_data_count} æ¡æ•°æ®ï¼")

    # === æ•°æ®æ£€æŸ¥ ===
    if df is None or df.empty:
        st.error("âš ï¸ No data available")
        st.info("ğŸ’¡ æ•°æ®æ­£åœ¨é‡‡é›†ä¸­ï¼Œè¯·ç¨ç­‰...")
        st.markdown(f"**Kafkaæ€»æ¶ˆæ¯æ•°**: {total_count}")

        # å€’è®¡æ—¶å¾ªç¯ï¼ˆå³ä½¿æ²¡æ•°æ®ä¹Ÿæ˜¾ç¤ºï¼‰
        if auto_refresh:
            for remaining in range(60, 0, -1):
                # æ ¼å¼åŒ–æ˜¾ç¤ºæ—¶é—´
                if remaining >= 60:
                    time_str = f"{remaining // 60}åˆ†{remaining % 60}ç§’"
                else:
                    time_str = f"{remaining}ç§’"

                with countdown_placeholder:
                    st.markdown(f'<div class="countdown-container"><div class="countdown">â³ <b>ä¸‹æ¬¡åˆ·æ–°</b>: {time_str}</div></div>',
                               unsafe_allow_html=True)
                with progress_placeholder:
                    progress = (60 - remaining) / 60
                    st.progress(progress)
                time.sleep(1)

            st.rerun()
        return

    # === æ•°æ®ç»Ÿè®¡ ===
    total_posts = len(df)
    twitter_count = len(df[df['source'] == 'Twitter'])
    reddit_count = len(df[df['source'] == 'Reddit'])
    total_engagement = df['engagement'].sum()

    # === æ ¸å¿ƒæŒ‡æ ‡ ===
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        delta_str = f"+{new_data_count}" if new_data_count > 0 else None
        st.metric(
            label="ğŸ“Š Total Posts",
            value=f"{total_posts:,}",
            delta=delta_str
        )

    with col2:
        pct = f"{twitter_count/total_posts*100:.1f}%" if total_posts > 0 else "0%"
        st.metric(
            label="ğŸ¦ Twitter Posts",
            value=f"{twitter_count:,}",
            delta=pct
        )

    with col3:
        pct = f"{reddit_count/total_posts*100:.1f}%" if total_posts > 0 else "0%"
        st.metric(
            label="ğŸ¤– Reddit Posts",
            value=f"{reddit_count:,}",
            delta=pct
        )

    with col4:
        st.metric(
            label="ğŸ’¬ Total Engagement",
            value=f"{total_engagement:,}",
            delta="Live"
        )

    st.markdown("---")

    # === Tabsç»„ç»‡å†…å®¹ ===
    tab1, tab2, tab3 = st.tabs(["ğŸ“Š Overview", "ğŸ”¥ Trending Keywords", "ğŸ“ Recent Posts"])

    with tab1:
        st.subheader("ğŸ“Š Data Source Distribution")

        # é¥¼å›¾
        source_counts = df['source'].value_counts()

        fig = px.pie(
            values=source_counts.values,
            names=source_counts.index,
            title='Twitter vs Reddit',
            color_discrete_map={'Twitter': '#1DA1F2', 'Reddit': '#FF4500'}
        )

        fig.update_traces(textposition='inside', textinfo='percent+label')
        fig.update_layout(height=400)

        st.plotly_chart(fig, use_container_width=True)

    with tab2:
        col_cloud, col_list = st.columns([2, 1])

        with col_cloud:
            st.subheader("â˜ï¸ Topic Word Cloud")
            create_word_cloud(df)

        with col_list:
            st.subheader("ğŸ”¥ Top 10 Keywords")
            keywords_df = extract_real_keywords(df, 10)

            if not keywords_df.empty:
                # æŸ±çŠ¶å›¾
                fig = px.bar(
                    keywords_df,
                    x='mentions',
                    y='keyword',
                    orientation='h',
                    title='',
                    color='mentions',
                    color_continuous_scale='Blues'
                )
                fig.update_layout(showlegend=False, height=400)
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.info("æ­£åœ¨æå–å…³é”®è¯...")

    with tab3:
        st.subheader("ğŸ“ Recent Posts")

        # æ˜¾ç¤ºæœ€è¿‘20æ¡ï¼ŒæŒ‰æ—¶é—´æ’åº
        if 'created_at' in df.columns:
            recent_df = df.sort_values('created_at', ascending=False).head(20)
        else:
            recent_df = df.head(20)

        # æ¸²æŸ“å¡ç‰‡
        for idx, row in recent_df.iterrows():
            st.markdown(render_reddit_card(row), unsafe_allow_html=True)

    # === é¡µè„š ===
    st.markdown("---")
    col1, col2, col3 = st.columns(3)

    with col1:
        st.caption(f"ğŸ”„ Last updated: {current_time}")

    with col2:
        st.caption(f"ğŸ“Š Kafka messages: {total_count:,}")

    with col3:
        st.caption("ğŸ’¾ Data source: Kafka | âš¡ Powered by Spark")

    # === å€’è®¡æ—¶å¾ªç¯ï¼ˆ60ç§’çœŸå®å€’è®¡æ—¶ï¼‰ ===
    if auto_refresh:
        for remaining in range(60, 0, -1):
            # æ ¼å¼åŒ–æ˜¾ç¤ºæ—¶é—´
            if remaining >= 60:
                time_str = f"{remaining // 60}åˆ†{remaining % 60}ç§’"
            else:
                time_str = f"{remaining}ç§’"

            with countdown_placeholder:
                st.markdown(f'<div class="countdown-container"><div class="countdown">â³ <b>ä¸‹æ¬¡åˆ·æ–°</b>: {time_str}</div></div>',
                           unsafe_allow_html=True)

            with progress_placeholder:
                progress = (60 - remaining) / 60
                st.progress(progress)

            time.sleep(1)

        # 60ç§’åè‡ªåŠ¨åˆ·æ–°
        st.session_state.refresh_count += 1
        st.rerun()


if __name__ == "__main__":
    main()
