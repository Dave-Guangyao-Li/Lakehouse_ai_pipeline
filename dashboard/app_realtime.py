"""
Streamlit Dashboard for AI Trend Monitoring - å®Œå…¨é‡æ„ç‰ˆæœ¬
- çœŸæ­£çš„5ç§’å€’è®¡æ—¶
- NLPæå–çœŸå®å…³é”®è¯
- è¯äº‘å¯è§†åŒ–
- Redditå¡ç‰‡æ ·å¼
- ç°ä»£åŒ–UIè®¾è®¡
"""
import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import time
import sys
import os
from collections import Counter
import re

# NLP imports
try:
    import spacy
    from wordcloud import WordCloud
    import matplotlib.pyplot as plt
    import nltk
    from nltk.corpus import stopwords

    NLP_AVAILABLE = True

    # åŠ è½½spacyæ¨¡å‹
    try:
        nlp = spacy.load('en_core_web_sm')
    except:
        NLP_AVAILABLE = False

    # ä¸‹è½½stopwordsï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
    try:
        STOP_WORDS = set(stopwords.words('english'))
    except LookupError:
        try:
            nltk.download('stopwords', quiet=True)
            STOP_WORDS = set(stopwords.words('english'))
        except:
            STOP_WORDS = set()

    # è‡ªå®šä¹‰æ‰©å±•åœç”¨è¯ï¼ˆè™šè¯ã€ä»£è¯ã€å¸¸è§æ— æ„ä¹‰è¯ï¼‰
    CUSTOM_STOP_WORDS = {
        'that', 'this', 'these', 'those', 'them', 'they', 'their', 'theirs',
        'people', 'anyone', 'someone', 'something', 'anything', 'everything',
        'which', 'what', 'where', 'when', 'who', 'whom', 'whose',
        'the us', 'the model', 'the future', 'the world', 'the company',
        'a lot', 'lot', 'thing', 'things', 'stuff', 'way', 'ways',
        'time', 'times', 'question', 'thanks', 'thank',
        # å¸¸è§Reddit/ç¤¾äº¤åª’ä½“è¯
        'post', 'comment', 'thread', 'subreddit', 'user', 'upvote',
        # é€šç”¨ä»£è¯å’Œé™å®šè¯
        'anyone', 'everyone', 'someone', 'nobody', 'anybody', 'somebody',
        'all', 'some', 'many', 'few', 'much', 'none',
    }
    STOP_WORDS.update(CUSTOM_STOP_WORDS)

except:
    NLP_AVAILABLE = False
    STOP_WORDS = set()

# Add parent directory to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    from kafka_reader import KafkaDataReader
    KAFKA_AVAILABLE = True
except Exception as e:
    KAFKA_AVAILABLE = False

# Page configuration
st.set_page_config(
    page_title="AI Trend Monitor - Live",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# å¢å¼ºCSS - ç°ä»£åŒ–è®¾è®¡
st.markdown("""
<style>
    /* æ•´ä½“èƒŒæ™¯ */
    .main {
        background: linear-gradient(135deg, #f5f7fa 0%, #e8eef5 100%);
    }

    /* ä¸»æ ‡é¢˜ */
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1E88E5;
        text-align: center;
        padding: 1rem 0;
        text-shadow: 2px 2px 4px rgba(0,0,0,0.1);
    }

    /* å®æ—¶çŠ¶æ€ç¯ */
    @keyframes blink {
        0%, 100% { opacity: 1; }
        50% { opacity: 0.3; }
    }

    .live-indicator {
        display: inline-block;
        width: 12px;
        height: 12px;
        background-color: #FF5252;
        border-radius: 50%;
        margin-right: 8px;
        animation: blink 1.5s infinite;
        box-shadow: 0 0 10px rgba(255, 82, 82, 0.5);
    }

    /* å€’è®¡æ—¶æ ·å¼ */
    .countdown-container {
        background: white;
        border-radius: 8px;
        padding: 12px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }

    .countdown {
        font-size: 1.1rem;
        color: #FF9800;
        font-weight: bold;
    }

    /* å®æ—¶æ—¶é’Ÿ */
    .realtime-clock {
        font-size: 1.1rem;
        font-weight: 500;
        color: #424242;
        background: white;
        border-radius: 8px;
        padding: 12px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }

    /* Metricå¡ç‰‡ç¾åŒ– */
    div[data-testid="stMetric"] {
        background: white;
        padding: 20px;
        border-radius: 12px;
        box-shadow: 0 4px 6px rgba(0,0,0,0.07);
        transition: transform 0.2s, box-shadow 0.2s;
    }

    div[data-testid="stMetric"]:hover {
        transform: translateY(-2px);
        box-shadow: 0 6px 12px rgba(0,0,0,0.12);
    }

    /* è¿›åº¦æ¡ç¾åŒ– */
    .stProgress > div > div > div > div {
        background: linear-gradient(90deg, #1E88E5, #42A5F5);
    }

    /* Redditå¡ç‰‡æ ·å¼ */
    .reddit-card {
        background: white;
        border: 1px solid #e0e0e0;
        border-radius: 12px;
        padding: 16px;
        margin-bottom: 12px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.08);
        transition: all 0.2s;
    }

    .reddit-card:hover {
        box-shadow: 0 4px 12px rgba(0,0,0,0.15);
        transform: translateY(-2px);
        border-color: #FF4500;
    }

    .card-header {
        display: flex;
        align-items: center;
        margin-bottom: 10px;
        gap: 8px;
    }

    .source-badge {
        padding: 4px 10px;
        border-radius: 12px;
        font-size: 0.75rem;
        font-weight: bold;
        color: white;
    }

    .badge-reddit {
        background: #FF4500;
    }

    .badge-twitter {
        background: #1DA1F2;
    }

    .badge-bluesky {
        background: #0085FF;
    }

    .card-meta {
        color: #787C7E;
        font-size: 0.875rem;
    }

    .card-title {
        font-size: 1.05rem;
        font-weight: 600;
        color: #1c1c1c;
        margin: 10px 0;
        line-height: 1.4;
    }

    .card-footer {
        display: flex;
        gap: 16px;
        color: #787C7E;
        font-size: 0.875rem;
        margin-top: 10px;
        padding-top: 10px;
        border-top: 1px solid #f0f0f0;
    }

    /* Tabæ ·å¼ç¾åŒ– */
    .stTabs [data-baseweb="tab-list"] {
        gap: 8px;
        background-color: white;
        border-radius: 8px;
        padding: 4px;
    }

    .stTabs [data-baseweb="tab"] {
        border-radius: 4px;
        padding: 8px 16px;
    }
</style>
""", unsafe_allow_html=True)


# åˆå§‹åŒ– session_state
if 'previous_count' not in st.session_state:
    st.session_state.previous_count = 0
if 'refresh_count' not in st.session_state:
    st.session_state.refresh_count = 0


@st.cache_data(ttl=300)  # ç¼“å­˜5åˆ†é’Ÿ
def load_real_data():
    """ä»KafkaåŠ è½½å®æ—¶æ•°æ®"""
    if not KAFKA_AVAILABLE:
        return None, 0

    try:
        reader = KafkaDataReader(
            bootstrap_servers='localhost:9092',
            topic='ai-social-raw'
        )

        # è·å–æ€»æ¶ˆæ¯æ•°
        total_count = reader.get_message_count()

        # è·å–æ‰€æœ‰æ¶ˆæ¯
        messages = reader.get_all_messages()

        if not messages:
            print(f"âš ï¸ Warning: get_all_messages() returned empty list, but total_count={total_count}")
            return None, total_count

        df = reader.parse_to_dataframe(messages)

        if df.empty:
            print(f"âš ï¸ Warning: parse_to_dataframe() returned empty DataFrame from {len(messages)} messages")
            return None, total_count

        print(f"âœ… Successfully loaded {len(df)} rows from Kafka (total_count={total_count})")
        return df, total_count

    except Exception as e:
        print(f"âŒ Error loading data: {e}")
        import traceback
        traceback.print_exc()
        st.error(f"Error loading data: {e}")
        return None, 0


def extract_real_keywords(df, top_n=20):
    """ä½¿ç”¨NLPæå–çœŸå®çš„AIæ¦‚å¿µçŸ­è¯­ï¼ˆå¤šè¯ç»„åˆï¼‰"""
    if df is None or df.empty or not NLP_AVAILABLE:
        return pd.DataFrame()

    try:
        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬
        all_text = ' '.join(df['text'].astype(str).tolist())

        # æ¸…ç†æ–‡æœ¬
        all_text = re.sub(r'http\S+|www\S+|https\S+', '', all_text)  # ç§»é™¤URL
        all_text = re.sub(r'[^\w\s]', ' ', all_text)  # ç§»é™¤æ ‡ç‚¹

        # ä½¿ç”¨spacyå¤„ç†ï¼ˆé™åˆ¶é•¿åº¦é¿å…è¶…æ—¶ï¼‰
        doc = nlp(all_text[:200000])

        # é€šç”¨è¯é»‘åå•ï¼ˆè¿‡æ»¤å•ä¸ªé€šç”¨è¯ï¼‰
        GENERIC_WORDS_BLACKLIST = {
            'data', 'image', 'model', 'tool', 'system', 'code', 'language',
            'result', 'problem', 'example', 'project', 'paper', 'test',
            'work', 'performance', 'version', 'feature', 'issue', 'user',
            'file', 'application', 'platform', 'service', 'product',
            'company', 'technology', 'solution', 'method', 'process',
            'source', 'experience', 'knowledge', 'context', 'inference',
            'search', 'noise', 'year', 'day', 'app', 'human', 'generation'
        }

        # æå–åè¯çŸ­è¯­ï¼ˆnoun chunksï¼‰
        phrases = []

        for chunk in doc.noun_chunks:
            phrase = chunk.text.lower().strip()
            words = phrase.split()
            num_words = len(words)

            # è¿‡æ»¤è§„åˆ™ï¼š
            # 1. é•¿åº¦ï¼š2-4ä¸ªè¯ï¼ˆæˆ‘ä»¬è¦çš„æ˜¯çŸ­è¯­ï¼Œä¸æ˜¯å•è¯ï¼‰
            # 2. ä¸æ˜¯çº¯åœç”¨è¯ç»„åˆ
            # 3. ä¸ä»¥é€šç”¨è¯å¼€å¤´æˆ–ç»“å°¾
            # 4. ä¸åŒ…å«æ•°å­—
            # 5. è¿‡æ»¤æ‰çº¯é€šç”¨AIè¯

            if (2 <= num_words <= 4  # å¤šè¯çŸ­è¯­
                and all(w not in STOP_WORDS for w in words)  # ä¸æ˜¯åœç”¨è¯
                and words[0] not in GENERIC_WORDS_BLACKLIST  # é¦–è¯ä¸æ˜¯é€šç”¨è¯
                and words[-1] not in GENERIC_WORDS_BLACKLIST  # å°¾è¯ä¸æ˜¯é€šç”¨è¯
                and not any(w.isdigit() for w in words)  # ä¸åŒ…å«æ•°å­—
                and phrase not in ['artificial intelligence', 'machine learning', 'deep learning', 'neural network']  # è¿‡æ»¤é€šç”¨AIè¯
                and len(phrase) >= 8  # æ€»å­—ç¬¦æ•°è‡³å°‘8ï¼ˆé¿å…å¤ªçŸ­çš„çŸ­è¯­ï¼‰
                ):
                phrases.append(phrase)

        # ç»Ÿè®¡é¢‘ç‡
        phrase_counts = Counter(phrases).most_common(top_n)

        # å¦‚æœçŸ­è¯­å¤ªå°‘ï¼Œé™ä½æ ‡å‡†ï¼Œå…è®¸å•ä¸ªæœ‰æ„ä¹‰çš„æŠ€æœ¯è¯
        if len(phrase_counts) < 5:
            keywords = []
            for token in doc:
                if token.pos_ in ['NOUN', 'PROPN']:
                    word = token.lemma_.lower()
                    if (len(word) >= 4
                        and word not in STOP_WORDS
                        and word not in GENERIC_WORDS_BLACKLIST
                        and not word.isdigit()
                        and token.is_alpha
                        and not word.startswith('ai')):
                        keywords.append(word)

            keyword_counts = Counter(keywords).most_common(top_n)
            # åˆå¹¶çŸ­è¯­å’Œå…³é”®è¯ï¼ŒçŸ­è¯­ä¼˜å…ˆ
            all_counts = phrase_counts + keyword_counts
            all_counts = dict(all_counts).items()
            phrase_counts = sorted(all_counts, key=lambda x: x[1], reverse=True)[:top_n]

        if not phrase_counts:
            return pd.DataFrame()

        return pd.DataFrame(phrase_counts, columns=['keyword', 'mentions'])

    except Exception as e:
        print(f"âŒ å…³é”®è¯æå–é”™è¯¯: {e}")
        return pd.DataFrame()


def check_bluesky_collector_status():
    """æ£€æŸ¥Blueskyé‡‡é›†å™¨è¿è¡ŒçŠ¶æ€"""
    log_file = 'logs/bluesky_collector.log'

    try:
        if not os.path.exists(log_file):
            return {
                'status': 'idle',
                'message': 'æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨ï¼Œé‡‡é›†å™¨å¯èƒ½æœªå¯åŠ¨',
                'rate_limited': False,
                'last_success': False
            }

        # è¯»å–æœ€è¿‘10è¡Œæ—¥å¿—
        with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
            lines = f.readlines()
            recent_lines = lines[-10:] if len(lines) > 10 else lines

        recent_text = ''.join(recent_lines)

        # æ£€æŸ¥æœ€åä¿®æ”¹æ—¶é—´
        last_modified = os.path.getmtime(log_file)
        time_diff = time.time() - last_modified

        # è¶…è¿‡5åˆ†é’Ÿæ²¡æœ‰æ›´æ–°
        if time_diff > 300:
            return {
                'status': 'idle',
                'message': f'é‡‡é›†å™¨å·²ç©ºé—² {int(time_diff/60)} åˆ†é’Ÿ',
                'rate_limited': False,
                'last_success': False
            }

        # æ£€æŸ¥æ˜¯å¦æœ‰é‡‡é›†æˆåŠŸçš„è®°å½•
        if 'Collected and sent' in recent_text or 'âœ…' in recent_text:
            return {
                'status': 'running',
                'message': 'æ­£åœ¨é‡‡é›† Bluesky å¸–å­',
                'rate_limited': False,
                'last_success': True
            }

        # æ£€æŸ¥æ˜¯å¦æœ‰è®¤è¯æˆ–è¿æ¥é”™è¯¯
        if 'authentication failed' in recent_text.lower() or 'connection' in recent_text.lower():
            return {
                'status': 'error',
                'message': 'APIè®¤è¯æˆ–è¿æ¥é”™è¯¯',
                'rate_limited': False,
                'last_success': False
            }

        # é»˜è®¤è¿è¡ŒçŠ¶æ€
        return {
            'status': 'running',
            'message': 'ç›‘æ§ä¸­',
            'rate_limited': False,
            'last_success': False
        }

    except Exception as e:
        return {
            'status': 'error',
            'message': f'æ— æ³•è¯»å–çŠ¶æ€: {str(e)}',
            'rate_limited': False,
            'last_success': False
        }


def check_reddit_collector_status():
    """æ£€æŸ¥Reddité‡‡é›†å™¨è¿è¡ŒçŠ¶æ€"""
    log_file = 'logs/reddit_collector.log'

    try:
        # æ£€æŸ¥æ—¥å¿—æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(log_file):
            return {
                'status': 'unknown',
                'message': 'æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨',
                'rate_limited': False,
                'last_success': False
            }

        # è¯»å–æœ€å100è¡Œæ—¥å¿—
        with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
            lines = f.readlines()[-100:]

        if not lines:
            return {'status': 'unknown', 'message': 'æ—¥å¿—ä¸ºç©º', 'rate_limited': False, 'last_success': False}

        # æ£€æŸ¥rate limitï¼ˆæœ€è¿‘çš„æ—¥å¿—ï¼‰
        recent_lines = lines[-20:]
        has_rate_limit = any(
            'rate limit' in line.lower()
            or '429' in line
            or 'too many requests' in line.lower()
            for line in recent_lines
        )

        # æ£€æŸ¥æœ€è¿‘æ˜¯å¦æœ‰æˆåŠŸé‡‡é›†
        has_success = any(
            'successfully' in line.lower()
            or 'âœ…' in line
            or 'sent to kafka' in line.lower()
            for line in recent_lines
        )

        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
        has_error = any(
            'error' in line.lower()
            or 'âŒ' in line
            or 'failed' in line.lower()
            for line in recent_lines
        )

        # åˆ¤æ–­çŠ¶æ€
        if has_rate_limit:
            status = 'limited'
            message = 'APIè°ƒç”¨å—é™ï¼Œç­‰å¾…æ¢å¤ä¸­...'
        elif has_success:
            status = 'running'
            message = 'æ­£å¸¸è¿è¡Œ'
        elif has_error:
            status = 'error'
            message = 'é‡‡é›†å‡ºç°é”™è¯¯'
        else:
            status = 'idle'
            message = 'ç©ºé—²æˆ–ç­‰å¾…ä¸­'

        return {
            'status': status,
            'message': message,
            'rate_limited': has_rate_limit,
            'last_success': has_success
        }

    except Exception as e:
        return {
            'status': 'unknown',
            'message': f'æ— æ³•è¯»å–çŠ¶æ€: {str(e)}',
            'rate_limited': False,
            'last_success': False
        }


def create_word_cloud(df):
    """ç”Ÿæˆè¯äº‘"""
    if df is None or df.empty or not NLP_AVAILABLE:
        st.info("è¯äº‘åŠŸèƒ½éœ€è¦NLPåº“æ”¯æŒ")
        return

    try:
        keywords_df = extract_real_keywords(df, top_n=100)

        if keywords_df.empty:
            st.warning("æ— æ³•ç”Ÿæˆè¯äº‘ï¼šæ²¡æœ‰æå–åˆ°å…³é”®è¯")
            return

        # æ¸…ç†å…³é”®è¯æ–‡æœ¬ï¼šç§»é™¤æ¢è¡Œç¬¦å’Œå¤šä½™ç©ºæ ¼ï¼Œç¡®ä¿æ˜¯å•è¡Œæ–‡æœ¬
        clean_keywords = {}
        for keyword, count in zip(keywords_df['keyword'], keywords_df['mentions']):
            # å°†å¤šè¡Œæ–‡æœ¬è½¬ä¸ºå•è¡Œï¼Œç§»é™¤æ¢è¡Œç¬¦
            clean_keyword = ' '.join(str(keyword).split())
            clean_keywords[clean_keyword] = count

        wordcloud = WordCloud(
            width=800,
            height=400,
            background_color='white',
            colormap='Blues',
            max_words=50,
            relative_scaling=0.5,
            min_font_size=12,
            collocations=False,
            prefer_horizontal=0.7  # ä¼˜å…ˆæ°´å¹³æ˜¾ç¤ºï¼Œé¿å…å¤šè¡Œé—®é¢˜
        ).generate_from_frequencies(clean_keywords)

        fig, ax = plt.subplots(figsize=(10, 5))
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis('off')
        st.pyplot(fig)
        plt.close(fig)  # å…³é—­å›¾å½¢é‡Šæ”¾å†…å­˜

    except Exception as e:
        st.warning(f"è¯äº‘æš‚æ—¶æ— æ³•ç”Ÿæˆ")
        st.info("ğŸ’¡ æç¤º: å½“å‰å…³é”®è¯æ•°æ®è¾ƒå°‘æˆ–æ ¼å¼ä¸å…¼å®¹ï¼Œè¯·ç¨ååˆ·æ–°æŸ¥çœ‹")


def format_time_ago(timestamp_str):
    """
    å°†æ—¶é—´æˆ³è½¬æ¢ä¸ºç›¸å¯¹æ—¶é—´æ˜¾ç¤º

    Args:
        timestamp_str: ISOæ ¼å¼æ—¶é—´æˆ³å­—ç¬¦ä¸²æˆ–Unixæ—¶é—´æˆ³

    Returns:
        ç›¸å¯¹æ—¶é—´å­—ç¬¦ä¸² (å¦‚ "2å°æ—¶å‰", "3å¤©å‰")
    """
    if not timestamp_str:
        return ''

    try:
        # å°è¯•è§£æä¸åŒçš„æ—¶é—´æ ¼å¼
        if isinstance(timestamp_str, (int, float)):
            # Unixæ—¶é—´æˆ³
            post_time = datetime.fromtimestamp(timestamp_str)
        elif 'T' in str(timestamp_str):
            # ISOæ ¼å¼ (å¦‚ "2025-01-10T15:30:00")
            post_time = datetime.fromisoformat(str(timestamp_str).replace('Z', '+00:00'))
        else:
            # å°è¯•ä½œä¸ºå­—ç¬¦ä¸²è§£æ
            post_time = datetime.fromisoformat(str(timestamp_str))

        now = datetime.now()
        delta = now - post_time

        # è®¡ç®—ç›¸å¯¹æ—¶é—´
        if delta.days > 365:
            years = delta.days // 365
            return f"{years}å¹´å‰"
        elif delta.days > 30:
            months = delta.days // 30
            return f"{months}æœˆå‰"
        elif delta.days > 0:
            return f"{delta.days}å¤©å‰"
        elif delta.seconds >= 3600:
            hours = delta.seconds // 3600
            return f"{hours}å°æ—¶å‰"
        elif delta.seconds >= 60:
            minutes = delta.seconds // 60
            return f"{minutes}åˆ†é’Ÿå‰"
        else:
            return "åˆšåˆš"

    except Exception as e:
        # è§£æå¤±è´¥,è¿”å›åŸå§‹å€¼
        return str(timestamp_str)


def render_reddit_card(row):
    """æ¸²æŸ“Reddité£æ ¼å¡ç‰‡"""
    import html
    import pandas as pd

    # å®‰å…¨è·å–å­—æ®µï¼Œå¤„ç†NaNå’ŒNone
    source = str(row.get('source', 'Unknown'))
    author = str(row.get('author', 'Unknown'))

    # è·å–subredditï¼Œå¤„ç†NaN
    subreddit = row.get('subreddit', '')
    if pd.isna(subreddit) or subreddit == 'nan':
        subreddit = ''
    else:
        subreddit = str(subreddit)

    # è·å–å¹¶æ¸…ç†æ–‡æœ¬ï¼ˆç§»é™¤HTMLæ ‡ç­¾ï¼‰
    text_raw = str(row.get('text', ''))
    # 1. è§£ç HTMLå®ä½“
    text_clean = html.unescape(text_raw)
    # 2. ç§»é™¤HTMLæ ‡ç­¾
    text_clean = re.sub(r'<[^>]+>', '', text_clean)
    # 3. ç§»é™¤å¤šä½™ç©ºç™½
    text_clean = ' '.join(text_clean.split())
    # 4. æˆªå–å‰200å­—ç¬¦
    text = text_clean[:200]

    # è·å–engagementï¼Œå¤„ç†NaN
    engagement = row.get('engagement', 0)
    if pd.isna(engagement):
        engagement = 0
    else:
        engagement = int(engagement)

    created_at = row.get('created_at', '')

    # æ ¹æ®æ¥æºè®¾ç½®badgeæ ·å¼
    if source == 'Reddit':
        badge_class = 'badge-reddit'
    elif source == 'Bluesky':
        badge_class = 'badge-bluesky'
    else:  # Twitter
        badge_class = 'badge-twitter'

    # æ ¼å¼åŒ–æ—¶é—´æˆ³
    time_display = format_time_ago(created_at)

    # æ„å»ºmetadataå­—ç¬¦ä¸²ï¼ˆå®‰å…¨æ‹¼æ¥ï¼‰
    meta_parts = []
    if subreddit:
        meta_parts.append(f'r/{subreddit}')
    meta_parts.append(f'by u/{author}')
    if time_display:
        meta_parts.append(time_display)
    meta_string = ' â€¢ '.join(meta_parts)

    card_html = f"""
    <div class="reddit-card">
        <div class="card-header">
            <span class="source-badge {badge_class}">{source}</span>
            <span class="card-meta">{meta_string}</span>
        </div>

        <div class="card-title">{text}...</div>

        <div class="card-footer">
            <span>ğŸ‘ {engagement:,}</span>
            <span>ğŸ’¬ è¯„è®º</span>
            <span>ğŸ”— åˆ†äº«</span>
        </div>
    </div>
    """

    return card_html


def main():
    """ä¸»åº”ç”¨"""

    # === é¡¶éƒ¨çŠ¶æ€æ  ===
    col_time, col_countdown, col_btn = st.columns([2, 2, 1])

    with col_time:
        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        st.markdown(f'<div class="realtime-clock">â° <b>å½“å‰æ—¶é—´</b>: {current_time}</div>',
                   unsafe_allow_html=True)

    # åˆ›å»ºå€’è®¡æ—¶å ä½ç¬¦
    countdown_placeholder = col_countdown.empty()
    progress_placeholder = col_countdown.empty()

    with col_btn:
        if st.button("ğŸ”„ ç«‹å³åˆ·æ–°", use_container_width=True):
            st.cache_data.clear()
            st.session_state.refresh_count += 1
            st.rerun()

    st.markdown("---")

    # === ä¸»æ ‡é¢˜ ===
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.markdown('<div class="main-header">ğŸ¤– AI Trend Monitor</div>', unsafe_allow_html=True)
        st.markdown("""
            <center>
                <span class="live-indicator"></span>
                <span style="font-size: 1.1rem; font-weight: 600; color: #FF5252;">LIVE DATA</span>
            </center>
        """, unsafe_allow_html=True)

    st.markdown("Real-time monitoring of AI discussions from Twitter and Reddit")

    # === ä¾§è¾¹æ  ===
    with st.sidebar:
        st.header("âš™ï¸ Settings")

        auto_refresh = st.checkbox("Auto-refresh", value=True)

        st.markdown("---")

        st.subheader("ğŸ“Š Data Sources")
        if KAFKA_AVAILABLE:
            st.success("âœ… Kafka Connected")
        else:
            st.error("âŒ Kafka Unavailable")

        st.markdown("---")

        st.subheader("ğŸ¤– Collectors Status")

        # BlueskyçŠ¶æ€ï¼ˆåŠ¨æ€æ£€æŸ¥ï¼‰
        bluesky_status = check_bluesky_collector_status()

        if bluesky_status['status'] == 'running':
            st.markdown("- ğŸ”µ **Bluesky**: âœ… æ­£å¸¸è¿è¡Œ")
            st.caption(f"   ğŸ“¡ {bluesky_status['message']}")
        elif bluesky_status['status'] == 'error':
            st.markdown("- ğŸ”µ **Bluesky**: âŒ å‡ºç°é”™è¯¯")
            st.caption(f"   {bluesky_status['message']}")
        elif bluesky_status['status'] == 'idle':
            st.markdown("- ğŸ”µ **Bluesky**: â¸ï¸ æœªå¯åŠ¨")
            st.caption(f"   {bluesky_status['message']}")
        else:
            st.markdown("- ğŸ”µ **Bluesky**: â“ çŠ¶æ€æœªçŸ¥")

        # RedditçŠ¶æ€ï¼ˆåŠ¨æ€æ£€æŸ¥ï¼‰
        reddit_status = check_reddit_collector_status()

        if reddit_status['status'] == 'limited':
            st.markdown("- ğŸ¤– **Reddit**: âš ï¸ Rate Limited")
            st.caption(f"   {reddit_status['message']}")
        elif reddit_status['status'] == 'running':
            st.markdown("- ğŸ¤– **Reddit**: âœ… æ­£å¸¸è¿è¡Œ")
            st.caption("   ğŸ“¡ é‡‡é›†é¢‘ç‡: 120ç§’/æ¬¡")
        elif reddit_status['status'] == 'error':
            st.markdown("- ğŸ¤– **Reddit**: âŒ å‡ºç°é”™è¯¯")
            st.caption(f"   {reddit_status['message']}")
        elif reddit_status['status'] == 'idle':
            st.markdown("- ğŸ¤– **Reddit**: â¸ï¸ ç©ºé—²ä¸­")
            st.caption(f"   {reddit_status['message']}")
        else:
            st.markdown("- ğŸ¤– **Reddit**: â“ çŠ¶æ€æœªçŸ¥")
            st.caption(f"   {reddit_status['message']}")

        if NLP_AVAILABLE:
            st.success("âœ… NLPåŠŸèƒ½å¯ç”¨")
        else:
            st.warning("âš ï¸ NLPåŠŸèƒ½ä¸å¯ç”¨")

    # === åŠ è½½æ•°æ® ===
    with st.spinner("ğŸ“Š Loading real-time data from Kafka..."):
        df, total_count = load_real_data()

    # === è®¡ç®—æ–°å¢æ•°æ® ===
    new_data_count = total_count - st.session_state.previous_count
    if new_data_count != 0:
        st.session_state.previous_count = total_count

    # === æ˜¾ç¤ºæ–°å¢æç¤º ===
    if new_data_count > 0:
        st.success(f"ğŸ†• æ–°å¢ +{new_data_count} æ¡æ•°æ®ï¼")

    # === æ•°æ®æ£€æŸ¥ ===
    if df is None or df.empty:
        st.error("âš ï¸ No data available")
        st.info("ğŸ’¡ æ•°æ®æ­£åœ¨é‡‡é›†ä¸­ï¼Œè¯·ç¨ç­‰...")
        st.markdown(f"**Kafkaæ€»æ¶ˆæ¯æ•°**: {total_count}")

        # å€’è®¡æ—¶å¾ªç¯ï¼ˆå³ä½¿æ²¡æ•°æ®ä¹Ÿæ˜¾ç¤ºï¼‰
        if auto_refresh:
            for remaining in range(60, 0, -1):
                # æ ¼å¼åŒ–æ˜¾ç¤ºæ—¶é—´
                if remaining >= 60:
                    time_str = f"{remaining // 60}åˆ†{remaining % 60}ç§’"
                else:
                    time_str = f"{remaining}ç§’"

                with countdown_placeholder:
                    st.markdown(f'<div class="countdown-container"><div class="countdown">â³ <b>ä¸‹æ¬¡åˆ·æ–°</b>: {time_str}</div></div>',
                               unsafe_allow_html=True)
                with progress_placeholder:
                    progress = (60 - remaining) / 60
                    st.progress(progress)
                time.sleep(1)

            st.rerun()
        return

    # === æ•°æ®ç»Ÿè®¡ ===
    total_posts = len(df)
    twitter_count = len(df[df['source'] == 'Twitter'])
    reddit_count = len(df[df['source'] == 'Reddit'])
    total_engagement = df['engagement'].sum()

    # === æ ¸å¿ƒæŒ‡æ ‡ ===
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        delta_str = f"+{new_data_count}" if new_data_count > 0 else None
        st.metric(
            label="ğŸ“Š Total Posts",
            value=f"{total_posts:,}",
            delta=delta_str
        )

    with col2:
        pct = f"{twitter_count/total_posts*100:.1f}%" if total_posts > 0 else "0%"
        st.metric(
            label="ğŸ¦ Twitter Posts",
            value=f"{twitter_count:,}",
            delta=pct
        )

    with col3:
        pct = f"{reddit_count/total_posts*100:.1f}%" if total_posts > 0 else "0%"
        st.metric(
            label="ğŸ¤– Reddit Posts",
            value=f"{reddit_count:,}",
            delta=pct
        )

    with col4:
        st.metric(
            label="ğŸ’¬ Total Engagement",
            value=f"{total_engagement:,}",
            delta="Live"
        )

    st.markdown("---")

    # === Tabsç»„ç»‡å†…å®¹ ===
    tab1, tab2, tab3 = st.tabs(["ğŸ“Š Overview", "ğŸ”¥ Trending Keywords", "ğŸ“ Recent Posts"])

    with tab1:
        st.subheader("ğŸ“Š Data Source Distribution")

        # é¥¼å›¾
        source_counts = df['source'].value_counts()

        fig = px.pie(
            values=source_counts.values,
            names=source_counts.index,
            title='Twitter vs Reddit',
            color_discrete_map={'Twitter': '#1DA1F2', 'Reddit': '#FF4500'}
        )

        fig.update_traces(textposition='inside', textinfo='percent+label')
        fig.update_layout(height=400)

        st.plotly_chart(fig, use_container_width=True)

    with tab2:
        col_cloud, col_list = st.columns([2, 1])

        with col_cloud:
            st.subheader("â˜ï¸ Topic Word Cloud")
            create_word_cloud(df)

        with col_list:
            st.subheader("ğŸ”¥ Top 10 Keywords")
            keywords_df = extract_real_keywords(df, 10)

            if not keywords_df.empty:
                # æŸ±çŠ¶å›¾
                fig = px.bar(
                    keywords_df,
                    x='mentions',
                    y='keyword',
                    orientation='h',
                    title='',
                    color='mentions',
                    color_continuous_scale='Blues'
                )
                fig.update_layout(showlegend=False, height=400)
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.info("æ­£åœ¨æå–å…³é”®è¯...")

    with tab3:
        st.subheader("ğŸ“ Recent Posts")

        # === ç­›é€‰å™¨æ§åˆ¶ ===
        col_filter1, col_filter2, col_filter3 = st.columns(3)

        with col_filter1:
            # æ—¥æœŸç­›é€‰
            date_filter = st.selectbox(
                "ğŸ“… æ—¶é—´èŒƒå›´",
                ["æ‰€æœ‰", "ä»Šå¤©", "æ˜¨å¤©", "æœ¬å‘¨", "æœ¬æœˆ"],
                index=0
            )

        with col_filter2:
            # æ¥æºç­›é€‰
            source_filter = st.selectbox(
                "ğŸ“¡ æ¥æº",
                ["æ‰€æœ‰"] + list(df['source'].unique()) if 'source' in df.columns else ["æ‰€æœ‰"],
                index=0
            )

        with col_filter3:
            # æ’åºé€‰é¡¹
            sort_by = st.selectbox(
                "ğŸ“Š æ’åº",
                ["æœ€æ–°", "æœ€çƒ­", "å‚ä¸åº¦æœ€é«˜"],
                index=0
            )

        # Subredditç­›é€‰ (ä»…å½“æœ‰Redditæ•°æ®æ—¶æ˜¾ç¤º)
        if 'subreddit' in df.columns:
            subreddits = df[df['subreddit'].notna()]['subreddit'].unique()
            if len(subreddits) > 0:
                selected_subreddits = st.multiselect(
                    "ğŸ” Subredditç­›é€‰",
                    options=subreddits,
                    default=None,
                    placeholder="é€‰æ‹©subreddit (å¯å¤šé€‰)"
                )
            else:
                selected_subreddits = []
        else:
            selected_subreddits = []

        st.markdown("---")

        # === åº”ç”¨ç­›é€‰æ¡ä»¶ ===
        filtered_df = df.copy()

        # æ—¥æœŸç­›é€‰
        if 'created_at' in df.columns and date_filter != "æ‰€æœ‰":
            # å…ˆç»Ÿä¸€è½¬æ¢æ—¶é—´æ ¼å¼ï¼Œæ”¯æŒå¤šç§æ ¼å¼ï¼ˆISO8601, Unix timestampç­‰ï¼‰
            # utc=True å¤„ç†å¸¦æ—¶åŒºçš„æ—¶é—´ï¼Œerrors='coerce' å°†æ— æ•ˆå€¼è½¬ä¸º NaT
            filtered_df['created_at_parsed'] = pd.to_datetime(
                filtered_df['created_at'],
                utc=True,
                errors='coerce'
            )

            now = datetime.now()
            if date_filter == "ä»Šå¤©":
                filtered_df = filtered_df[
                    filtered_df['created_at_parsed'].dt.tz_localize(None).dt.date == now.date()
                ]
            elif date_filter == "æ˜¨å¤©":
                yesterday = now - timedelta(days=1)
                filtered_df = filtered_df[
                    filtered_df['created_at_parsed'].dt.tz_localize(None).dt.date == yesterday.date()
                ]
            elif date_filter == "æœ¬å‘¨":
                week_ago = now - timedelta(days=7)
                filtered_df = filtered_df[
                    filtered_df['created_at_parsed'].dt.tz_localize(None) >= week_ago
                ]
            elif date_filter == "æœ¬æœˆ":
                month_ago = now - timedelta(days=30)
                filtered_df = filtered_df[
                    filtered_df['created_at_parsed'].dt.tz_localize(None) >= month_ago
                ]

            # ç§»é™¤ä¸´æ—¶åˆ—
            filtered_df = filtered_df.drop('created_at_parsed', axis=1)

        # æ¥æºç­›é€‰
        if source_filter != "æ‰€æœ‰":
            filtered_df = filtered_df[filtered_df['source'] == source_filter]

        # Subredditç­›é€‰
        if selected_subreddits:
            filtered_df = filtered_df[filtered_df['subreddit'].isin(selected_subreddits)]

        # æ’åº
        if 'created_at' in filtered_df.columns:
            if sort_by == "æœ€æ–°":
                # è½¬æ¢æ—¶é—´æ ¼å¼åæ’åºï¼Œç¡®ä¿æ­£ç¡®å¤„ç†å„ç§æ—¶é—´æ ¼å¼
                filtered_df['_sort_time'] = pd.to_datetime(
                    filtered_df['created_at'],
                    utc=True,
                    errors='coerce'
                )
                filtered_df = filtered_df.sort_values('_sort_time', ascending=False)
                filtered_df = filtered_df.drop('_sort_time', axis=1)
            elif sort_by == "æœ€çƒ­" or sort_by == "å‚ä¸åº¦æœ€é«˜":
                filtered_df = filtered_df.sort_values('engagement', ascending=False)
        else:
            if sort_by == "æœ€çƒ­" or sort_by == "å‚ä¸åº¦æœ€é«˜":
                filtered_df = filtered_df.sort_values('engagement', ascending=False)

        # é™åˆ¶æ˜¾ç¤ºæ•°é‡
        filtered_df = filtered_df.head(50)

        # === æŒ‰æ—¥æœŸåˆ†ç»„æ˜¾ç¤º ===
        if len(filtered_df) == 0:
            st.info("ğŸ” æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„å¸–å­")
        else:
            st.caption(f"ğŸ“Š æ‰¾åˆ° {len(filtered_df)} æ¡å¸–å­")

            # æŒ‰æ—¥æœŸåˆ†ç»„
            if 'created_at' in filtered_df.columns:
                # ç»Ÿä¸€è½¬æ¢æ—¶é—´æ ¼å¼ï¼Œæ”¯æŒå¸¦æ—¶åŒºçš„æ—¶é—´
                filtered_df['date'] = pd.to_datetime(
                    filtered_df['created_at'],
                    utc=True,
                    errors='coerce'
                ).dt.tz_localize(None).dt.date
                grouped = filtered_df.groupby('date')

                for date, group in grouped:
                    # è®¡ç®—ç›¸å¯¹æ—¥æœŸ
                    today = datetime.now().date()
                    if date == today:
                        date_label = "ä»Šå¤©"
                    elif date == today - timedelta(days=1):
                        date_label = "æ˜¨å¤©"
                    elif date >= today - timedelta(days=7):
                        days_ago = (today - date).days
                        date_label = f"{days_ago}å¤©å‰"
                    else:
                        date_label = date.strftime("%Y-%m-%d")

                    # æ˜¾ç¤ºæ—¥æœŸåˆ†ç»„å¤´éƒ¨
                    st.markdown(f"### ğŸ“… {date_label} ({len(group)} æ¡)")

                    # æ¸²æŸ“è¯¥æ—¥æœŸçš„æ‰€æœ‰å¡ç‰‡
                    for idx, row in group.iterrows():
                        st.markdown(render_reddit_card(row), unsafe_allow_html=True)

                    st.markdown("<br>", unsafe_allow_html=True)
            else:
                # å¦‚æœæ²¡æœ‰æ—¶é—´å­—æ®µï¼Œç›´æ¥æ˜¾ç¤º
                for idx, row in filtered_df.iterrows():
                    st.markdown(render_reddit_card(row), unsafe_allow_html=True)

    # === é¡µè„š ===
    st.markdown("---")
    col1, col2, col3 = st.columns(3)

    with col1:
        st.caption(f"ğŸ”„ Last updated: {current_time}")

    with col2:
        st.caption(f"ğŸ“Š Kafka messages: {total_count:,}")

    with col3:
        st.caption("ğŸ’¾ Data source: Kafka | âš¡ Powered by Spark")

    # === å€’è®¡æ—¶å¾ªç¯ï¼ˆ60ç§’çœŸå®å€’è®¡æ—¶ï¼‰ ===
    if auto_refresh:
        for remaining in range(60, 0, -1):
            # æ ¼å¼åŒ–æ˜¾ç¤ºæ—¶é—´
            if remaining >= 60:
                time_str = f"{remaining // 60}åˆ†{remaining % 60}ç§’"
            else:
                time_str = f"{remaining}ç§’"

            with countdown_placeholder:
                st.markdown(f'<div class="countdown-container"><div class="countdown">â³ <b>ä¸‹æ¬¡åˆ·æ–°</b>: {time_str}</div></div>',
                           unsafe_allow_html=True)

            with progress_placeholder:
                progress = (60 - remaining) / 60
                st.progress(progress)

            time.sleep(1)

        # 60ç§’åè‡ªåŠ¨åˆ·æ–°
        st.session_state.refresh_count += 1
        st.rerun()


if __name__ == "__main__":
    main()
